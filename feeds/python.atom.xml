<?xml version="1.0" encoding="utf-8"?> 
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
 <title type="text">Konrad Hinsen's Blog: Posts tagged 'python'</title>
 <link rel="self" href="http://blog.khinsen.net/feeds/python.atom.xml" />
 <link href="http://blog.khinsen.net/tags/python.html" />
 <id>urn:http-blog-khinsen-net:-tags-python-html</id>
 <updated>2017-11-22T16:56:59Z</updated>
 <entry>
  <title type="text">Stability in the SciPy ecosystem: a summary of the discussion</title>
  <link rel="alternate" href="http://blog.khinsen.net/posts/2017/11/22/stability-in-the-scipy-ecosystem-a-summary-of-the-discussion/?utm_source=python&amp;utm_medium=Atom" />
  <id>urn:http-blog-khinsen-net:-posts-2017-11-22-stability-in-the-scipy-ecosystem-a-summary-of-the-discussion</id>
  <published>2017-11-22T16:56:59Z</published>
  <updated>2017-11-22T16:56:59Z</updated>
  <author>
   <name>Konrad Hinsen</name></author>
  <content type="html">
&lt;p&gt;The &lt;a href="http://blog.khinsen.net/posts/2017/11/16/a-plea-for-stability-in-the-scipy-ecosystem/#comment-3627775108"&gt;plea for stability in the SciPy ecosystem&lt;/a&gt; that I posted last week on this blog has generated a lot of feedback, both as comments and in a lengthy &lt;a href="https://twitter.com/khinsen/status/931192953636315137"&gt;Twitter thread&lt;/a&gt;. For the benefit of people discovering it late, here is a summary of the main arguments and my reply to them.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h2 id="just-freeze-your-code-and-it-will-be-reproducible-forever"&gt;Just freeze your code and it will be reproducible forever&lt;/h2&gt;

&lt;p&gt;By far the most frequent argument against my claim that we need more stability in the SciPy ecosystem was that people can simply archive their code with all the dependencies (down to the Python language itself) in a way that lets others re-run it later for reproducibility. The most frequently proposed technical approaches were the &lt;a href="https://conda.io/docs/"&gt;conda&lt;/a&gt; package manager and &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; containers.&lt;/p&gt;

&lt;p&gt;There are three main reasons why this is not a sufficient solution:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;
  &lt;p&gt;Freezing code is fine for archival reproducibility, as I mentioned in my original post. It is not sufficient for living code bases that people work on over decades. Computational biologist Luis Pedro Coelho has &lt;a href="https://metarabbit.wordpress.com/2017/11/18/numpy-scipy-backwards-stability-debate-and-why-freezing-versions-is-not-the-solution/"&gt;explained&lt;/a&gt; this very well and I recommend everyone to read his short writeup. My situation is very much the same as his. On Twitter, astronomer Tuan Do has chimed in with a &lt;a href="https://twitter.com/quantumpenguin/status/933123060822978560"&gt;similar comment&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;The technical solutions proposed all depend on yet more infrastructure whose longevity is uncertain. For how long will a Docker container image produced in 2017 remain usable? For how long will conda and its repositories be supported, and for how long will the binaries in these repositories function on current platforms?&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;None of today&amp;rsquo;s code freezing approaches comes with easy-to-use tooling and clear documentation that make it accessible to the average computational scientist. The technologies are today in a &amp;ldquo;good for early adopters&amp;rdquo; state. This means we cannot rely on them to preserve &lt;em&gt;today&amp;rsquo;s&lt;/em&gt; research even though they may well take on this role in the future.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;To illustrate point 3, let me introduce Alice and Bob, who are real scientists I know, except that I have changed the names. Alice is a chemist with a decent knowledge of Python and basic software engineering techniques (Software Carpentry level), which she eagerly applies because she cares about the quality of her work. Alice considers herself an experimentalist. She develops and maintains a Python codebase for interpreting certain types of experimental data, but software development is not the focus of her work. The code she writes is not public, because her boss doesn&amp;rsquo;t want it to be. Worse, her code depends on a small library developed by a collaborator who doesn&amp;rsquo;t even hand out source code. What Alice gets is pre-compiled shared libraries for the three platforms that matter to herself and to her users.&lt;/p&gt;

&lt;p&gt;Bob is an experimental biologist who uses the same instruments as Alice and is happy that Alice has written nice software for interpreting the results. He gets that software, including the binary-only dependencies, by personal arrangements with the various people involved. Bob doesn&amp;rsquo;t know much about Python, nor does he care. His software installation was mostly done by Alice during a one-afternoon meeting in which they worked together to reach a state he could work with. Ideally, he would like to never touch it again, but he also wants the new features that Alice adds from time to time.&lt;/p&gt;

&lt;p&gt;To all those who replied &amp;ldquo;just use conda&amp;rdquo; or &amp;ldquo;just use Docker&amp;rdquo;, I recommend considering the situation of Alice and Bob. Do you really believe that conda or Docker are the right solution for them today? Could you point them to suitable documentation written at the right level? Both for building and for re-using frozen environments?&lt;/p&gt;

&lt;p&gt;To prevent another round of misunderstanding, I am not saying that the situation of Alice and Bob would be perfect if only they could have a stable Python infrastructure. Research code should be open, for example, for many reasons including the possibility to upload it to various repositories. Fortunately, the attitudes towards software use in science are changing in the right direction, but this will take a lot of time, like all social change.&lt;/p&gt;

&lt;p&gt;I also fully understand the point of view that the SciPy ecosystem is for advanced users who value methodological innovation, and that it cannot cater for the needs of Alice and Bob because of conflicting requirements and insufficient resources to deal with them. But then, as I said in my original post, please have the courage to say so openly and clearly. Every beginner-level tutorial for scientists should state during the first five minutes that you cannot expect stability and that you should either use Python only for throw-away code or else be sure you can assume maintenance. In other words, make sure that people like Alice have no false expectations. They can then look for other technology, or team up with like-minded people to maintain long-time-stable branches of SciPy, or try whatever else.&lt;/p&gt;

&lt;h2 id="stability-is-an-unrealistic-expectation"&gt;Stability is an unrealistic expectation&lt;/h2&gt;

&lt;p&gt;Another frequently expressed opinion was that it is unrealistic to expect the kind of stability I advocated in a modern software environment. This is a self-fulfilling prophecy: if you consider the goal impossible, you won&amp;rsquo;t even try to achieve it. As I have pointed out, long-time stability is a reality in other ecosystems, built around languages such as Fortran or Java. A few people said that Fortran or Java are unfair comparisons, because they encourage very different approaches to dependency management. This is actually my point: you can have stability, but only if it&amp;rsquo;s an explicit goal and if some effort is made to reach that goal. This includes finding suitable approaches to dependency management.&lt;/p&gt;

&lt;p&gt;David Cournape made the &lt;a href="https://twitter.com/cournape/status/849918989165842434"&gt;interesting observation&lt;/a&gt; that no technology less than 20 years old is better than Python in terms of stability. That rings true, in the sense that I cannot find a counterexample. But I see this as a statement about dominant attitudes in software engineering (way beyond scientific computing), not as a statement about technological constraints that would make stability fundamentally incompatible with other requirements. Software development today is dominated by short-lived technologies but also by short-lived applications. The application domains where stability is valued probably represent a much smaller part of the pie than 20 years ago. But then, this is just another illustration for what I wrote about recently: &lt;a href="http://blog.khinsen.net/posts/2017/11/09/there-is-no-such-thing-as-software-development/"&gt;There is no such thing as software development&lt;/a&gt; in the abstract, there is only domain-specific software development. The needs of scientific computing are clearly different from the needs of Silicon Valley startups. The conclusion is that the software development tools and practices should be different as well.&lt;/p&gt;

&lt;p&gt;Finally, even within the somewhat tumultuous SciPy ecosystem, stability is not impossible. My own &lt;a href="http://dirac.cnrs-orleans.fr/MMTK/"&gt;MMTK&lt;/a&gt; library has been around for 20 years, but in spite of continuous extensions and one API redesign (from version 1.x to version 2.x), I have never knowingly broken anyone&amp;rsquo;s application code. With the end of support Python 2, I can unfortunately no longer maintain that policy.&lt;/p&gt;

&lt;h2 id="everybody-lacks-resources-for-maintenance"&gt;Everybody lacks resources for maintenance&lt;/h2&gt;

&lt;p&gt;Many comments addressed the lack of human resources for developing and maintaining scientific software, and in particular infrastructure software like the core of the SciPy ecosystem. In combination with the fact that new developments are more attractive to most people than boring maintenance, and also more valued by the community, this leads to a culture favoring innovation over stability when most of the work is done by volunteers. This was best expressed by Peter Wang in a &lt;a href="https://twitter.com/pwang/status/931386237193211904"&gt;short sequence of tweets&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This is indeed an important factor, and one whose importance transcends scientific computing and even science itself. If you look back at the history of civilization, or even at the history of life on earth, you can&amp;rsquo;t fail to notice that all living organisms have invested the lion&amp;rsquo;s share of their efforts into maintaining the &lt;em&gt;status quo&lt;/em&gt;: staying alive, staying safe, maintaining an environment that ensures a certain quality of life, etc. In modern societies whose very survival depends on technology, infrastructure maintenance (roads, power grid, &amp;hellip;) has always been a priority of state administrations - until recently, that is. Today, we hear politicians and even intellectuals proclaim the importance of innovation and disruption, while basic infrastructure starts to rot for lack of maintenance.&lt;/p&gt;

&lt;p&gt;I can only hope that the innovation and disruption fashion will die out before the societies that have fallen victim to this fashion will do so by natural selection. In the meantime, I propose that scientists try to resist as best as possible. The fact that infrastructure software such as NumPy does get funding is a good sign in my opinion. I believe we can also get funding for stability, if only we clearly state that we need it.&lt;/p&gt;

&lt;h2 id="data-supremacy"&gt;Data supremacy&lt;/h2&gt;

&lt;p&gt;Pierre de Buyl &lt;a href="http://disq.us/p/1nveqdc"&gt;reminded me&lt;/a&gt; of an &lt;a href="http://ieeexplore.ieee.org/abstract/document/6341744/"&gt;article&lt;/a&gt; I wrote five years ago, in which I proposed that data rather than software tools should be the focus of scientific computing because data is of longer-lasting scientific importance. As I have &lt;a href="https://f1000research.com/articles/3-101/v2"&gt;pointed out two years later&lt;/a&gt;, that data includes scientific models (equations etc.), even though for technical reasons they are mostly embedded into software tools today (see &lt;a href="http://sjscience.org/article?id=527"&gt;here&lt;/a&gt; for an idea for doing things differently).&lt;/p&gt;

&lt;p&gt;In a world where all scientifically relevant information is stored in stable and well-defined open file formats, software tools can evolve much more freely without disturbing ongoing work or harming reproducibility. New versions of software tools would merely have to maintain the functionality of their predecessors, but not their implementation details. However, this is at best a promise for the future. We don&amp;rsquo;t even have the basic technology to make this happen, nor a consensus that it would be a good idea, which would open up the possibility of getting funding towards that goal. We will therefore need stable software environments for many more years to come.&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">A plea for stability in the SciPy ecosystem</title>
  <link rel="alternate" href="http://blog.khinsen.net/posts/2017/11/16/a-plea-for-stability-in-the-scipy-ecosystem/?utm_source=python&amp;utm_medium=Atom" />
  <id>urn:http-blog-khinsen-net:-posts-2017-11-16-a-plea-for-stability-in-the-scipy-ecosystem</id>
  <published>2017-11-16T15:56:49Z</published>
  <updated>2017-11-16T15:56:49Z</updated>
  <author>
   <name>Konrad Hinsen</name></author>
  <content type="html">
&lt;p&gt;Two NumPy-related news items appeared on my Twitter feed yesterday, just a few days after I had accidentally started a &lt;a href="https://twitter.com/khinsen/status/929014170749632513"&gt;somewhat heated debate&lt;/a&gt; myself concerning the poor reproducibility of Python-based computer-aided research. The first was the announcement &lt;a href="https://github.com/numpy/numpy/blob/master/doc/neps/dropping-python2.7-proposal.rst"&gt;of a plan for dropping support for Python 2&lt;/a&gt;. The second was a pointer to a &lt;a href="https://www.youtube.com/watch?v=fowHwlpGb34"&gt;recent presentation by Nathaniel Smith&lt;/a&gt; entitled &amp;ldquo;Inside NumPy&amp;rdquo; and dealing mainly with the NumPy team&amp;rsquo;s plans for the near future. Lots of material to think about&amp;hellip; and comment on.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;The end of Python 2 support for NumPy didn&amp;rsquo;t come as a surprise to anyone in the Python community. With Python 2 itself not being supported after 2020, it doesn&amp;rsquo;t make any sense for Python-dependent software to continue support beyond that date. The detailed plan for the transition of NumPy to a Python&amp;ndash;3-only package looks quite reasonable. Which doesn&amp;rsquo;t mean that everything is fine. The disappearance of Python 2 will leave much scientific software orphaned, and many published results irreproducible. Yes, the big well-known packages of the SciPy ecosystem all work with Python 3 by now, but the same cannot be said for many domain-specific libraries that have a much smaller user and developer base, and much more limited resources. As an example, my own &lt;a href="http://dirac.cnrs-orleans.fr/MMTK/"&gt;Molecular Modelling Toolkit&lt;/a&gt; (MMTK), which might well be the oldest domain-specific library of the SciPy ecosystem, will probably go away after 2020. Porting it to Python 3 is possible, of course, but an enormous effort (some details are in this &lt;a href="https://twitter.com/khinsen/status/930749714567434240"&gt;Twitter thread&lt;/a&gt;) for which resources (funding plus competent staff) are very difficult to find.&lt;/p&gt;

&lt;p&gt;Speaking purely from a computational science point of view, the Python 2-&amp;gt;3 transition was a big mistake. While Python 3 does have some interesting new features for scientists, most of them could have been implemented in Python 2 as well, without breaking backward compatibility. There are, of course, good reasons for the modernization of the language. I am not saying that Guido van Rossum is an idiot - far from it. As popular as Python may be in today&amp;rsquo;s scientific research, scientific users make up for a very small part of the total Python user base. Unfortunately, the need for long-term stability is rather specific to scientific users, and not even all of them require it (see e.g. &lt;a href="https://twitter.com/ctitusbrown/status/929044554598137856"&gt;these&lt;/a&gt; &lt;a href="https://twitter.com/ctitusbrown/status/929044751633936384"&gt;two&lt;/a&gt; tweets by Titus Brown). So while Python 3 is probably a step forward for most Python users, it&amp;rsquo;s mostly a calamity for computational science.&lt;/p&gt;

&lt;p&gt;Apart from the major earthquake caused by this change in the Python language itself, whose victims we will be able to count starting from 2020, the SciPy ecosystem has been subject to regular minor seismic activities by breaking changes in its foundational libraries, such as NumPy or matplotlib. I am not aware of any systematic study of their impact, but my personal anecdotal evidence (see e.g. this &lt;a href="http://blog.khinsen.net/posts/2017/04/06/reproducible-research-in-the-python-ecosystem-a-reality-check/"&gt;report&lt;/a&gt;) suggests that a Python script can be expected to work for two to three years, but not for five or more. Older scripts will either crash, which is a nuisance, or produce different results, which is much worse because the problem may well go unnoticed.&lt;/p&gt;

&lt;p&gt;In my corner of science, biomolecular simulation, the time scale of methodological progress is decades. This doesn&amp;rsquo;t mean that nothing exciting happens in shorter time spans. It just means that methods and techniques, including software, remain relevant for one to three decades. It isn&amp;rsquo;t even uncommon for a single research project to extend over several years. As an example, I just edited a script whose last modification date was December 2015. It&amp;rsquo;s part of collaborative project involving methodological development and application work in both experiment and theory. The back-and-forth exchanges between experimentalists and theoreticians take a lot of time. In the course of such projects, I update software and even change computers. If infrastructure updates break my code in progress, that&amp;rsquo;s a major productivity loss.&lt;/p&gt;

&lt;p&gt;Beyond personal productivity considerations, breaking changes are a threat to the reproducibility of scientific studies, an aspect that has been gaining more and more attention recently because so many published results were found to be non-reproducible or erroneous (note that these are very different things, but that&amp;rsquo;s not my topic for today), with software taking a big share of the responsibility. The two main issues are: (1) non-reproducible results cannot be trusted, because nobody really knows how they were obtained and (2) code whose results are non-reproducible is not a reliable basis for further work (Newton&amp;rsquo;s famous &amp;ldquo;standing on the shoulders of giants&amp;rdquo;). Many researchers, myself included, are advocating better practices to ensure computational reproducibility. In view of the seismic activities outlined above, I have been wondering for a while whether I should add &amp;ldquo;don&amp;rsquo;t use Python&amp;rdquo; to my list of recommendations. What&amp;rsquo;s holding me back is mainly the lack of any decent alternative to today&amp;rsquo;s SciPy ecosystem.&lt;/p&gt;

&lt;p&gt;Watching &lt;a href="https://www.youtube.com/watch?v=fowHwlpGb34"&gt;Nathaniel&amp;rsquo;s BIDS talk&lt;/a&gt;, I was rather disappointed that these issues were not treated at all. There is a general discussion of &amp;ldquo;change&amp;rdquo;, including a short reference to breaking changes and their impact on downstream projects, which suggests that there has been some debate of these questions in the NumPy community (note that I am no longer following the &lt;a href="https://mail.scipy.org/mailman/listinfo/numpy-discussion"&gt;NumPy discussion&lt;/a&gt; mailing list for lack of time). However, assuming that Nathaniel&amp;rsquo;s summary is representative of that debate, neither reproducibility nor the requirements of the different software layers in scientific computing seem to have received the attention they deserve.&lt;/p&gt;

&lt;p&gt;I have written before about &lt;a href="http://blog.khinsen.net/posts/2017/01/13/sustainable-software-and-reproducible-research-dealing-with-software-collapse/"&gt;software layers&lt;/a&gt; and the &lt;a href="http://blog.khinsen.net/posts/2015/11/09/the-lifecycle-of-digital-scientific-knowledge/"&gt;lifecycle of digital scientific knowledge&lt;/a&gt;, so I will just give a summary here. A scientific software stack looks like this:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;Layer 4: project-specific code&lt;/li&gt;
 &lt;li&gt;Layer 3: domain-specific libraries&lt;/li&gt;
 &lt;li&gt;Layer 2: scientific infrastructure&lt;/li&gt;
 &lt;li&gt;Layer 1: non-scientific infrastructure&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;In the SciPy universe, we have Python in layer 1, NumPy and friends in layer 2, lots of lesser-known libraries (including my &lt;a href="http://dirac.cnrs-orleans.fr/MMTK/"&gt;MMTK&lt;/a&gt; mentioned above) in layer 3, and application scripts and notebooks in layer 4.&lt;/p&gt;

&lt;p&gt;A breaking change in any layer affects everything in the layers above. The authors of the affected higher-level code have three options:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;adapt their code (maintenance)&lt;/li&gt;
 &lt;li&gt;freeze their code (describe the stack they actually used)&lt;/li&gt;
 &lt;li&gt;do nothing&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;The first choice is of course the ideal case but it requires serious development resources. With the second one, archival reproducibility is guaranteed, i.e. a reader knows under which conditions the code can be used and trusted, and how these conditions can be recreated. But frozen code is not a good basis for further work. Using it requires much work for re-creating an outdated environment. Worse, using two or more of such packages together is in general impossible because each one has different dependency version requirements. Finally, the third option leaves the code in a limbo state where it isn&amp;rsquo;t even clear under which conditions it can be expected to work. In a research context, this ought to be considered unacceptable.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s consider now how these three choices are applied in practice, for each layer in the software stack. Software in layers 1 and 2 must obviously be maintained, otherwise people would quickly abandon it. Fortunately these layers also suffer the least from collapse, because there is less code below them. Layer 3 code gets more or less well maintained, depending on the size of the communities supporting it, and on the development resources available. Quite often, maintenance is sub-optimal for lack of resources, with the maintainers aware of the problem but unable to do a better job. That&amp;rsquo;s my situation with MMTK.&lt;/p&gt;

&lt;p&gt;Layer 4 code is the focus of the reproducible research movement. Today, most of this code is still not published, and of the small part that does get out, a large part is neither maintained nor frozen but simply dumped to a repository. In fact, the best practices recommended for reproducible research can be summarized as &amp;ldquo;freeze and publish layer 4 code&amp;rdquo;. Maintaining layer 4 code has been proposed (see e.g. &lt;a href="https://www.biorxiv.org/content/early/2016/08/11/056473"&gt;continuous analysis&lt;/a&gt; ), but it is unclear if the idea will find acceptance. The obvious open question is who should do the maintenance. Considering that most research is done by people who spend a few years in a lab and then move on, it&amp;rsquo;s difficult to assign the responsibility for maintenance to the original authors of the code. But anyone else is less competent, less motivated, and would likely expect to be payed for doing a service job.&lt;/p&gt;

&lt;p&gt;An argument I hear frequently in the SciPy community (and elsewhere) is that scientific code that is not actively used and maintained isn&amp;rsquo;t worth bothering with (see e.g. &lt;a href="https://twitter.com/ctitusbrown/status/929045580789161984"&gt;this tweet by Titus Brown&lt;/a&gt;). The implication is that breaking changes in the infrastructure layers are OK and must be absorbed by the maintainers of layers 3 and 4. In view of what I just said about layer 4, it should be obvious that I don&amp;rsquo;t agree at all with this point of view. But even concerning layer 3, I find it a bit arrogant. The message to research communities with weaker code development traditions, and thus fewer resources, is that their work doesn&amp;rsquo;t matter.&lt;/p&gt;

&lt;p&gt;I would like to see the SciPy community define its point of view on these issues openly and clearly. We all know that development resources are scarce, that not everything that&amp;rsquo;s desirable can be done. The real world requires compromises and priorities. But these compromises and priorities need to be discussed and communicated openly. It&amp;rsquo;s OK to say that the community&amp;rsquo;s priority is developing new features and that this leaves no resources for considering stability. But then please say openly and clearly that SciPy is a community for coding-intensive research and that people who don&amp;rsquo;t have the resources to adapt to breaking changes should look elsewhere. Say openly and clearly that reproducibility beyond a two-year timescale is not the SciPy community&amp;rsquo;s business, and that those who have such needs should look elsewhere. Or else, decide that SciPy is inclusive and caters for all computer-aided research - and draw the conclusion that stability must take a larger weight in future development decisions.&lt;/p&gt;

&lt;p&gt;What is not OK is what I perceive as the dominant attitude today: sell SciPy as a great easy-to-use tool for all scientists, and then, when people get bitten by breaking changes, tell them that it&amp;rsquo;s their fault for not having a solid maintenance plan for their code.&lt;/p&gt;

&lt;p&gt;Finally, in anticipation of an argument that I expect to see, let me stress that this is not a technical issue. Computing technology moves at a fast pace, but that doesn&amp;rsquo;t mean that lack of stability is a fatality. My &lt;a href="https://github.com/khinsen/hydrolib"&gt;last Fortran code&lt;/a&gt;, published in 1994, still works without changing a single line. Banks have been running Cobol code unchanged for decades. Today&amp;rsquo;s Java implementations will run the very first Java code from 1995 without changes, and even much faster thanks to JIT technology. This last example also shows that stability is not in contradiction with progress. You can have both if that&amp;rsquo;s a design goal. It&amp;rsquo;s all a matter of policy, not technology.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note added 2017&amp;ndash;11&amp;ndash;22:&lt;/strong&gt; see also my &lt;a href="http://blog.khinsen.net/posts/2017/11/22/stability-in-the-scipy-ecosystem-a-summary-of-the-discussion/"&gt;summary of the discussion&lt;/a&gt; in reaction to this post.&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">Why Python does so well in scientific computing</title>
  <link rel="alternate" href="http://blog.khinsen.net/posts/2017/09/12/why-python-does-so-well-in-scientific-computing/?utm_source=python&amp;utm_medium=Atom" />
  <id>urn:http-blog-khinsen-net:-posts-2017-09-12-why-python-does-so-well-in-scientific-computing</id>
  <published>2017-09-12T10:11:29Z</published>
  <updated>2017-09-12T10:11:29Z</updated>
  <author>
   <name>Konrad Hinsen</name></author>
  <content type="html">
&lt;p&gt;A few days ago, I noticed this tweet in my timeline:&lt;/p&gt;

&lt;blockquote class="twitter-tweet" data-lang="en"&gt;
 &lt;p lang="en" dir="ltr"&gt;I &amp;#39;still&amp;#39; program in C. Why? Hint: it&amp;#39;s not about performance. I wrote an essay to elaborate... appearing at Onward! &lt;a href="https://t.co/pzxjfvUs5B"&gt;https://t.co/pzxjfvUs5B&lt;/a&gt;&lt;/p&gt;&amp;mdash; Stephen Kell (@stephenrkell) &lt;a href="https://twitter.com/stephenrkell/status/905126286762356736"&gt;September 5, 2017&lt;/a&gt;&lt;/blockquote&gt;

&lt;script async="async" src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

&lt;p&gt;That sounded like a good read for the weekend, which it was. The main argument the author makes is that C remains unsurpassed as a system integration language, because it permits interfacing with &amp;ldquo;alien&amp;rdquo; code, i.e. code written independently and perhaps even in different languages, down to assembly. In fact, C is one of the few programming languages that lets you deal with whatever data at the byte level. Most more &amp;ldquo;modern&amp;rdquo; languages prohibit such interfacing in the name of safety - the only memory you can access is memory allocated through your safe language&amp;rsquo;s runtime system. As a consequence, you are stuck in the closed universe of your language.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;System integration is indeed an important and often overlooked aspect of working with software. And this is particularly true for scientific computing, where application software with a fixed set of functionality is rare. Solving a scientific problem typically involves combining many pieces of software into a very problem-specific whole, which may well be run only a few times (see also my &lt;a href="http://blog.khinsen.net/posts/2017/01/13/sustainable-software-and-reproducible-research-dealing-with-software-collapse/"&gt;earlier post&lt;/a&gt; on this topic). This is exactly the task of system integration: assembling pieces into a whole using glue code where necessary. In computational science, this glue code takes the form of scripts, workflows, or more recently notebooks. This is technically quite different from the OS-level system integration that Stephen Kell refers to, but functionally it is the same.&lt;/p&gt;

&lt;p&gt;Stephen&amp;rsquo;s post reminded me of my long-standing plan to write a blog post about why Python has been so successful in scientific computing, in spite of having a reputation for bad performance. So&amp;hellip; here it is.&lt;/p&gt;

&lt;p&gt;There are of course many reasons for Python&amp;rsquo;s success, but one of them is that it does a pretty good job at system integration. There are two Python features that I consider important for this, which are not shared by many other languages. One is data types explicitly designed for interfacing, the other is &lt;a href="https://en.wikipedia.org/wiki/Duck_typing"&gt;duck typing&lt;/a&gt; in combination with a small but versatile set of standard interfaces.&lt;/p&gt;

&lt;p&gt;The first Python data type designed for interfacing in a scientific computing context is the good old &lt;a href="http://www.numpy.org/"&gt;NumPy&lt;/a&gt; array - which is in fact older than NumPy, having been introduced in 1995 by NumPy&amp;rsquo;s predecessor, Numeric. Arrays are one of the bread-and-butter data types in scientific computing, to the point of being the only one available in languages like Fortran 77 or APL. The implementation of arrays in Numeric was designed to use the same data layout as Fortran and C, in order to allow interfacing to the Fortran and C libraries that dominated scientific computing in 1995 (and still do, though to a somewhat lesser extent). The idea behind Numeric and later NumPy was always to use Python as a glue language for Fortran and C libraries, and achieve speed by delegating time-critical operations to code written in these languages.&lt;/p&gt;

&lt;p&gt;The second Python data type designed for interfacing is &lt;a href="https://docs.python.org/3/library/stdtypes.html#memoryview"&gt;memoryview&lt;/a&gt;, related to the &lt;a href="https://docs.python.org/3/c-api/buffer.html"&gt;buffer protocol&lt;/a&gt;. This is as close as Python gets to C-style memory access. The buffer protocol lets different Python data types access each other&amp;rsquo;s internal memory at the byte level. A typical use case would be an image data type (e.g. from &lt;a href="https://python-pillow.org/"&gt;Pillow&lt;/a&gt;) allowing access to the in-memory representation of an image through an array type (e.g. from NumPy), permitting the implementation of image manipulation algorithms in terms of array operations.&lt;/p&gt;

&lt;p&gt;The third and least known Python data type for interfacing is the &lt;a href="https://docs.python.org/3/c-api/capsule.html"&gt;capsule&lt;/a&gt; that replaces the earlier &lt;a href="https://docs.python.org/2/c-api/cobject.html"&gt;CObject&lt;/a&gt;. Capsules exist solely for the benefit of Python modules written in C, which can exchange opaque data with one another via glue code written in Python, even though the glue code itself cannot inspect or manipulate the data in any way. A typical use is to wrap C function pointers in a Python object such that Python glue code, e.g. a script, can pass a C function from one module to a to C code from another module.&lt;/p&gt;

&lt;p&gt;All these interfacing data types mediate between Python and C code, although quite often the Python system integrator is hardly aware of using C code at all. The other Python feature for system integration, duck typing with standard interfaces, is what facilitates glueing together independently written Python modules. By &amp;ldquo;standard interfaces&amp;rdquo;, I mean the sequence and dictionary interfaces, but also the standard method names for operator overloading.&lt;/p&gt;

&lt;p&gt;To see why this is an important feature, let us look at statically typed languages that by design do not have it. As a concrete example, consider multidimensional arrays in Java. They are not part of the language or its standard library, but they can be implemented on top of it with reasonable effort. In fact, there are several Java implementations you can choose from. And that&amp;rsquo;s the problem. Suppose you want to use an FFT library based on array implementation A together with a linear algebra library based on array implementation B. Bad luck - the arrays from A and B have different types, so you cannot use the output of an FFT as the input to a linear equation solver. It doesn&amp;rsquo;t matter that the underlying abstraction is the same, and that even the implementations have much in common. For a Java compiler, tje types don&amp;rsquo;t match, period.&lt;/p&gt;

&lt;p&gt;Python is not completely immune to this problem. It is perfectly possible to write Python code, or C code in a C module, that expects a precise type of data as input, and will raise an exception otherwise. But in Python code that would be considered bad style, and in C modules for Python as well except where required for performance or for compatibility with the C code. Wherever possible, Python programmers are expected to use the standard interfaces for working with data. Iteration and indexing work the same way for arrays as for the built-in lists, for example. For operations that are not covered by the standard interfaces, Python programmers are supposed to use Python methods, which are subject to duck typing as well. In practice, independently implemented Python types are much more interoperable than independently implemented Java types. For the specific case of n-dimensional arrays, Python has had the chance of overwhelming acceptance of a single implementation, which is due more to social and historical than to technical issues.&lt;/p&gt;

&lt;p&gt;Finally, even though Python is a pretty good choice for system integration in scientific computing, there are of course limits, which are exactly of the kind that Stephen Kell explains in his essay: combining Python code with code in other managed languages, say R or Julia, requires a lot of work and even then is fragile, because the required hacks depend on undocumented implementation details. I suspect that the only solution would be to have language-neutral garbage-collected data objects proposed as an OS-level service that maintains an option for non-managed byte-level access Ã  la C. The closest existing technology I am aware of is Microsoft&amp;rsquo;s &lt;a href="https://en.wikipedia.org/wiki/Common_Language_Runtime"&gt;CLR&lt;/a&gt;, better known by its commercial name .NET. Its implementation is now Open Source and runs on multiple platforms, but its Windows-only origins and strong ties to a huge Microsoft-y library have been an obstacle to adoption by the traditionally Unix-centric scientific computing communty.&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">Reproducible research in the Python ecosystem: a reality check</title>
  <link rel="alternate" href="http://blog.khinsen.net/posts/2017/04/06/reproducible-research-in-the-python-ecosystem-a-reality-check/?utm_source=python&amp;utm_medium=Atom" />
  <id>urn:http-blog-khinsen-net:-posts-2017-04-06-reproducible-research-in-the-python-ecosystem-a-reality-check</id>
  <published>2017-04-06T09:26:50Z</published>
  <updated>2017-04-06T09:26:50Z</updated>
  <author>
   <name>Konrad Hinsen</name></author>
  <content type="html">
&lt;p&gt;A few years ago, I decided to adopt the practices of reproducible research as far as possible within the technical and social constraints I have to live with. So how reproducible is my published code over time?&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;The example I have chosen for this reproducibility study is a 2013 paper about computing diffusion coefficients from molecular simulations. &lt;a href="https://doi.org/10.6084/m9.figshare.808594.v1"&gt;All code and data&lt;/a&gt; has been published as an &lt;a href="http://www.activepapers.org/"&gt;ActivePaper&lt;/a&gt; on &lt;a href="https://figshare.com/"&gt;figshare&lt;/a&gt;. To save space, intermediate results had been removed from the published archive. This makes my reproducibility check very straightforward: a simple &lt;code&gt;aptool update&lt;/code&gt; will recompute everything starting from these intermediate results up to the plots that went into &lt;a href="http://dx.doi.org/10.1063/1.4823996"&gt;the paper&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One nice aspect of ActivePapers is that it stores the version numbers of all dependencies, so I can quickly verify that in 2013, I had used Python 2.7.3, NumPy 1.6.2, h5py 2.1.3, and matplotlib 1.2.x (yes, the x is part of the reported version number).&lt;/p&gt;

&lt;h2 id="first-try-use-my-current-python-environment"&gt;First try: use my current Python environment&lt;/h2&gt;

&lt;p&gt;The evironment in which I do most of my current research has Python 3.5.2, NumPy 1.11.1, h5py 2.6, and Matplotlib 1.5.1. I set it up about a year ago when I got a new laptop, and haven&amp;rsquo;t had a good reason to update it since then. I had made some effort back in 2013 to make my code compatible with Python 3, so why not try now if this was a worthy investment?&lt;/p&gt;

&lt;p&gt;Outcome: running the computations works just fine, with results that are not identical at the bit level but close enough for my application. However, I get some warnings from matplotlib when generating the plots. Here is the first one, the others are similar:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;UserWarning: Legend does not support 'x' instances.
A proxy artist may be used instead.
See: http://matplotlib.org/users/legend_guide.html#using-proxy-artist
  "#using-proxy-artist".format(orig_handle)&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;A quick inspection of the plots shows that the legends have almost disappeared, all that&amp;rsquo;s left is a small white box. That makes many of the plots unintellegible.&lt;/p&gt;

&lt;p&gt;Just out of curiosity, I made a quick attempt to figure out the error message. What&amp;rsquo;s that &amp;lsquo;x&amp;rsquo; instance? The following messages also refer to &amp;lsquo;yz&amp;rsquo; instances and a few others. A look at my script reveals that &amp;lsquo;x&amp;rsquo;, &amp;lsquo;yz&amp;rsquo; etc. are in fact the strings that I supplied as legends. Sounds strange to call them &amp;lsquo;x&amp;rsquo; instances, as if &amp;lsquo;x&amp;rsquo; were a class. And what&amp;rsquo;s that cryptic reference to a proxy artist?&lt;/p&gt;

&lt;p&gt;Better stop here: my goal was to see if I can reproduce my data and figures from 2013 in a Python environment from 2016, and the answer is no. The plots are mutilated to the point of no longer being useful.&lt;/p&gt;

&lt;h2 id="second-try-use-my-current-python-27-environment"&gt;Second try: use my current Python 2.7 environment&lt;/h2&gt;

&lt;p&gt;Some of my research code still lives in the Python 2.7 universe, so I also have a Python environment based on Python 2.7.11 on my laptop, with NumPy 1.8.2, h5py 2.5, and matplotlib 1.4.3. That&amp;rsquo;s much closer to the original one, so let&amp;rsquo;s see how well it does in my reproducibility evaluation.&lt;/p&gt;

&lt;p&gt;Outcome: Much better. The computations work fine as before, and the plots generate a single warning:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;MatplotlibDeprecationWarning: The "loc" positional argument to legend is deprecated. Please use the "loc" keyword instead.&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The legends still look OK, so the warning is just a minor nuisance, as one would expect from a deprecation-related message. Interestingly, this warning is also about legends, so it looks like there was a serious backwards-incompatible change in matplotlib&amp;rsquo;s &lt;code&gt;legend&lt;/code&gt; function between 1.2 and 1.5, which was prepared by a deprecation warning in 1.4.&lt;/p&gt;

&lt;h2 id="third-try-reconstructing-the-original-environment"&gt;Third try: reconstructing the original environment&lt;/h2&gt;

&lt;p&gt;Since I have the version numbers of everything, why not try to reconstruct the original environment exactly? Let&amp;rsquo;s go for the same major and minor version numbers, which should be sufficient. That&amp;rsquo;s a job for Anaconda:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;conda create -n python2013 python=2.7 numpy=1.6 h5py=2.1 matplotlib=1.2 anaconda
source active python2013
pip install tempdir
pip install ActivePapers.Py&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Outcome: no warnings, no errors. Identical results. Reproducibility bliss at its best.&lt;/p&gt;

&lt;h2 id="conclusions"&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;In summary, my little experiment has shown that reproducibility of Python scripts requires preserving the original environment, which fortunately is not so difficult over a time span of four years, at least if everything you need is part of the Anaconda distribution. I am not sure I would have had the patience to reinstall everything from source, given &lt;a href="http://blog.khinsen.net/posts/2015/11/06/a-rant-about-software-deployment-in-2015/"&gt;an earlier bad experience&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The purely computational part of my code was even surprisingly robust under updates in its dependencies. But the plotting code wasn&amp;rsquo;t, as matplotlib has introduced backwards-incompatible changes in a widely used function. Clearly the matplotlib team prepared this carefully, introducing a deprecation warning before introducing the breaking change. For properly maintained client code, this can probably be dealt with.&lt;/p&gt;

&lt;p&gt;The problem is that I do not intend to maintain the plotting scripts for all the papers I publish. And that&amp;rsquo;s not only out of laziness, but because doing so would violate the spirit of reproducible research. The code I publish is exactly the code that I used for the original work, without any modification. If I started maintaining it, I could easily change the results by accident. I&amp;rsquo;d thus have to introduce regression tests as a safeguard against such changes. But&amp;hellip; how do I test for visual equivalence of plots? Bitwise reproducibility is about as realistic to expect for image files as for floating-point numbers: I don&amp;rsquo;t even get bitwise identical image files running the same Python code with identical matplotlib versions on different machines.&lt;/p&gt;

&lt;p&gt;For my next paper, I will look for alternatives to matplotlib. My plotting needs are rather basic, so perhaps there is some other library with a more stable API that is good enough for me. Suggestions are welcome!&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">A rant about software deployment in 2015</title>
  <link rel="alternate" href="http://blog.khinsen.net/posts/2015/11/06/a-rant-about-software-deployment-in-2015/?utm_source=python&amp;utm_medium=Atom" />
  <id>urn:http-blog-khinsen-net:-posts-2015-11-06-a-rant-about-software-deployment-in-2015</id>
  <published>2015-11-06T12:13:32Z</published>
  <updated>2015-11-06T12:13:32Z</updated>
  <author>
   <name>Konrad Hinsen</name></author>
  <content type="html">
&lt;p&gt;We all know that software deployment in a research environment can be a pain, but knowing this as a fact is not quite the same as experiencing it in reality. Over the last days, I spent way more time that I would have imagined on what sounds like a simple task: installing a scientific application written in Python on a Linux machine for use by a group of students in a training session. Here is an outline of the difficulties, in the hope that it will (1) help others who face similar problems and (2) contributes a little bit to improving the situation.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;The software that I installed is &lt;a href="http://dirac.cnrs-orleans.fr/nMOLDYN/"&gt;nMOLDYN&lt;/a&gt;, an analysis tool for Molecular Dynamics trajectories. From a software engineering point of view, this is a rather standard Python program building on &lt;a href="http://numpy.scipy.org"&gt;NumPy&lt;/a&gt; and &lt;a href="http://dirac.cnrs-orleans.fr/MMTK"&gt;MMTK&lt;/a&gt; for its computations and on &lt;a href="https://wiki.python.org/moin/TkInter"&gt;Tkinter&lt;/a&gt; and &lt;a href="http://matplotlib.org/"&gt;matplotlib&lt;/a&gt; for the graphical user interface. There is no need for anything on the bleeding edge, a decent three-year old installation of the scientific Python stack would support this perfectly well.&lt;/p&gt;

&lt;p&gt;The machine that was set up for the training session is configured much like a typical node in a compute cluster: stable and trusted software installed once and never updated. More specifically, the machine runs &lt;a href="https://www.centos.org/"&gt;CentOS&lt;/a&gt; 6.7. Another feature rather typical of compute nodes is the very restricted network connectivity: users can log in via &lt;code&gt;ssh&lt;/code&gt;, and copy data in and out using &lt;code&gt;scp&lt;/code&gt;. Everything else is blocked, in particular all outgoing network traffic. The idea is that students will work on desktop or laptop machines, from where they have full network access to search for information, and connect to the compute server only for running scientific software. For my own software installation I had to limit myself to a user account, i.e. no administrator rights, although I could ask the systems administrator to install additional RPMs from CentOS.&lt;/p&gt;

&lt;p&gt;A first exploration of the system&amp;rsquo;s Python installation showed a collection of oldies: Python 2.6.6, NumPy 1.4.1, matplotlib 0.99.1.1. That&amp;rsquo;s the state of the art five years ago. I quickly decided not to use it at all, for two reasons. First, I wasn&amp;rsquo;t sure how much of what I had to add would still work with such old versions. All the software was already around five years ago, but I would have had to track down the versions that were current back then. Second, adding modules in a user account to a Python installation at the system level can easily lead to a fragile total. Following Murphy&amp;rsquo;s law such problems would show up during the student sessions. So I decided to start with a fresh install of Python 2.7.&lt;/p&gt;

&lt;p&gt;First surprise: no C compiler. An e-mail to the administrator, and I had gcc. Trying to install Python showed that the Tcl/Tk setup was incomplete: the header files were missing. An another e-mail asking for tcl-devel and tk-devel, and that was settled as well. Python, NumPy, netCDF, ScientificPython, and MMTK were up and running half an hour later. An attempt to install nMOLDYN resulted in the information that I still needed to install Pyro and matplotlib. That can&amp;rsquo;t be so hard, right?&lt;/p&gt;

&lt;p&gt;Pyro was no problem indeed, but matplotlib kept me busy for a few more hours. All I had done in the past was &lt;code&gt;pip install matplotlib&lt;/code&gt;, but &lt;code&gt;pip&lt;/code&gt; is useless without outgoing network connections. I had to track down source tarballs for matplotlib and all its dependencies. There&amp;rsquo;s a &lt;a href="http://matplotlib.org/users/installing.html"&gt;list&lt;/a&gt; of dependencies on the matplotlib Web site, but it&amp;rsquo;s incomplete in two ways: some dependencies are missing (setuptools and six), and others are given by name but without a link. Try googling for &amp;ldquo;cycler&amp;rdquo; - you will learn a lot about celestial mechanics before you find a package with this name on &lt;a href="https://pypi.python.org/pypi"&gt;PyPI&lt;/a&gt;. Of all the matplotlib dependencides, only freetype was already available on my machine, so I had some searching and downloading to do.&lt;/p&gt;

&lt;p&gt;The installation instructions for setuptools clearly do not consider the possibility of not having a network connection. They tell me to download a Python script and execute it to download the real software. Fortunately, there is the &amp;ldquo;advanced&amp;rdquo; installation option via a tarball. Which ends rather quickly with an error message complaining about the absence of the &lt;code&gt;zlib&lt;/code&gt; module.&lt;/p&gt;

&lt;p&gt;That module is part of the Python standard library, but it is compiled only if zlib (the C library) is installed on the machine. It wasn&amp;rsquo;t on mine. This is not particularly difficult to fix, but rather annoying: I had to install zlib, and then run the Python installation once more. Not to forget: I knew &lt;code&gt;zlib&lt;/code&gt; was in the standard library, and I immediately saw why it was missing on my machine, because I have been installing Pythons in lots of environments over twenty years. Someone else might well have spent a few hours figuring out what to do about zlib.&lt;/p&gt;

&lt;p&gt;From then on everything went smoothly, so this is the end of my story. In order to provide something constructive, here is the complete list of matplotlib dependencies with links, and in the order of installation:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;&lt;a href="http://www.zlib.net/"&gt;zlib&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;&lt;a href="http://www.python.org/"&gt;Python&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;&lt;a href="http://www.numpy.org/"&gt;numpy&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;&lt;a href="http://pypi.python.org/pypi/setuptools"&gt;setuptools&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;&lt;a href="http://pypi.python.org/pypi/six"&gt;six&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;&lt;a href="http://pypi.python.org/pypi/Cycler"&gt;cycler&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;&lt;a href="http://pypi.python.org/pypi/pyparsing"&gt;pyparsing&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;&lt;a href="http://pypi.python.org/pypi/python-dateutil"&gt;python-dateutil&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;&lt;a href="http://pypi.python.org/pypi/pytz"&gt;pytz&lt;/a&gt;&lt;/li&gt;
 &lt;li&gt;&lt;a href="http://www.libpng.org/pub/png/libpng.html"&gt;libpng&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Finally, I will pass on a hint that came in this morning via Twitter:&lt;/p&gt;

&lt;blockquote class="twitter-tweet" lang="de"&gt;
 &lt;p lang="en" dir="ltr"&gt;&lt;a href="https://twitter.com/khinsen"&gt;@khinsen&lt;/a&gt; &lt;a href="https://twitter.com/MrTheodor"&gt;@MrTheodor&lt;/a&gt; pip install âdownload . âno-use-wheel &amp;lt;foobar&amp;gt;&amp;#10;&amp;#10;Wonât work if there are Linux specific dependencies though.&lt;/p&gt;&amp;mdash; Donald Stufft (@dstufft) &lt;a href="https://twitter.com/dstufft/status/662620534010740736"&gt;6. November 2015&lt;/a&gt;&lt;/blockquote&gt;

&lt;script async="async" src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

&lt;p&gt;Using &lt;code&gt;pip install --download . --no-use-wheel matplotlib&lt;/code&gt;, run of course on a machine that has a network connection, you get tarballs for matplotlib and all its dependencies that pip knows about. You still have to add setuptools (which pip doesn&amp;rsquo;t download because it depends on it itself), the C libraries libpng and zlib, and of course Python&amp;rsquo;s standard but not-always-there &lt;code&gt;zlib&lt;/code&gt; module.&lt;/p&gt;

&lt;p&gt;Looking back at my twenty years using Python, I come to the unfortunate conclusion that software installation is much more of a problem today than it was back in 1995. The main reason is of course that Python software has become more feature-rich and complex - in 1995 something like matplotlib was only a dream. But the state of Python packaging tools is also to blame, with three overlapping and partially compatible tools (distutils, setuptools, and distribute) creating a lot of confusion and various distribution formats (tarballs, eggs, wheels) adding another layer of complexity. What is also sorely missing is a straightforward way to package an application program with all its dependencies in such a way that it can be installed with reasonable effort on all common platforms.&lt;/p&gt;</content></entry></feed>