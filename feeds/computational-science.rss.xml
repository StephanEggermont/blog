<?xml version="1.0" encoding="utf-8"?> 
<rss version="2.0">
 <channel>
  <title>Konrad Hinsen's Blog: Posts tagged 'computational science'</title>
  <description>Konrad Hinsen's Blog: Posts tagged 'computational science'</description>
  <link>http://blog.khinsen.net/tags/computational-science.html</link>
  <lastBuildDate>Fri, 08 Jan 2021 13:07:26 UT</lastBuildDate>
  <pubDate>Fri, 08 Jan 2021 13:07:26 UT</pubDate>
  <ttl>1800</ttl>
  <item>
   <title>The structure and interpretation of scientific models, part 2</title>
   <link>http://blog.khinsen.net/posts/2021/01/08/the-structure-and-interpretation-of-scientific-models-part-2/?utm_source=computational-science&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2021-01-08-the-structure-and-interpretation-of-scientific-models-part-2</guid>
   <pubDate>Fri, 08 Jan 2021 13:07:26 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;In &lt;a href="https://blog.khinsen.net/posts/2020/12/10/the-structure-and-interpretation-of-scientific-models/"&gt;my last post&lt;/a&gt;, I have discussed the two main types of scientific models: empirical models, also called descriptive models, and explanatory models. I have also emphasized the crucial role of equations and specifications in the formulation of explanatory models. But my description of scientific models in that post left aside a very important aspect: on a more fundamental level, all models are stories.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;To illustrate my point, I will take up my running example from part 1: celestial mechanics. Newton&amp;rsquo;s model for our solar system is, as I said, composed of several equations, the most famous of which, &lt;em&gt;F&lt;/em&gt; = &lt;em&gt;m&lt;/em&gt; ⋅ &lt;em&gt;a&lt;/em&gt;, many readers will probably remember from a high-school physics class. But that equation means nothing on its own. It just says that there are three quantities, one of which being the product of the other two.&lt;/p&gt;

&lt;p&gt;The minimal story required to make sense of this equation provides a definition of the three quantities involved. For acceleration (the &lt;em&gt;a&lt;/em&gt;), this may look superficially simple: it&amp;rsquo;s the second derivative of an object&amp;rsquo;s position in time. The concepts of position and time are part of our everyday intuition, so that&amp;rsquo;s the easy part. Velocity is an intuitive everyday concept as well, but its precise relation to position as a time derivative is not. For acceleration, nothing short of calculus will do. In fact, Newton invented calculus along with his physical theory! Defining mass (the &lt;em&gt;m&lt;/em&gt;) and force (the &lt;em&gt;F&lt;/em&gt;) is not a trivial task either. Both concepts are rooted in our everyday intuition about the world, but their role in Newton&amp;rsquo;s law of motion requires a much more precise understanding. If you have doubts about this, try explaining the difference between &lt;em&gt;mass&lt;/em&gt; and &lt;em&gt;weight&lt;/em&gt; to someone who doesn&amp;rsquo;t have a scientific education.&lt;/p&gt;

&lt;p&gt;From this big-picture point of view, equations such as &lt;em&gt;F&lt;/em&gt; = &lt;em&gt;m&lt;/em&gt; ⋅ &lt;em&gt;a&lt;/em&gt; are tiny pieces of our scientific models. They are the tips of icebergs whose massive underwater parts are the stories defining the underlying concepts and linking them to our intuition about the world, often through multiple and increasingly abstract layers. We tend to forget about these stories, because once we have understood them well enough, what we actually work with are the equations. But this works only for the well-established models whose stories are now found in textbooks. New research continuously introduces new models, often as small variants or extensions of existing ones. Their stories are told in scientific publications.&lt;/p&gt;

&lt;p&gt;Historically, &lt;a href="https://en.wikipedia.org/wiki/History_of_mathematical_notation"&gt;mathematical notation&lt;/a&gt; was introduced as a convenient shorthand for use in plain-language stories. The lengthy phrase &amp;ldquo;force equals mass times acceleration&amp;rdquo; thus became &lt;em&gt;F&lt;/em&gt; = &lt;em&gt;m&lt;/em&gt; ⋅ &lt;em&gt;a&lt;/em&gt;. The transition to symbolic equations encouraged the development of formal methods in mathematics, starting with algebraic transformations of simple equations. This approach was so successful that equations became the main focus of interest in science. Later, other formal representations were added for the non-numerical aspects of models, graphs being the prime example. The most recent addition to the collection of formal notations for scientific models is software. Today, scientists spend most of their time working with the formalized parts of scientific models, such as equations or algorithms, to the point of neglecting the stories that give them meaning.&lt;/p&gt;

&lt;p&gt;What happens when people use the equations of scientific models without a proper understanding of their stories is nicely illustrated by the joke about the physics student who combines Einstein&amp;rsquo;s &lt;em&gt;E&lt;/em&gt; = &lt;em&gt;m&lt;/em&gt; ⋅ &lt;em&gt;c²&lt;/em&gt; with Pythagoras&amp;rsquo; &lt;em&gt;a²&lt;/em&gt; + &lt;em&gt;b²&lt;/em&gt; = &lt;em&gt;c²&lt;/em&gt; to deduce &lt;em&gt;E&lt;/em&gt; = &lt;em&gt;m&lt;/em&gt; ⋅ (&lt;em&gt;a²&lt;/em&gt; + &lt;em&gt;b²&lt;/em&gt;). It works as a joke among physicists because in their community, everybody knows the two inputs and the contexts from which they are taken. For other people, there is nothing funny about this reasoning, and it can even look convincing. Such superficial use of scientific models without understanding their context is actually quite common in today&amp;rsquo;s research: the inappropriate use of statistical inference methods is a major cause of the &lt;a href="https://en.wikipedia.org/wiki/Replication_crisis"&gt;reproducibility crisis&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Computing technology has played a big role in alienating scientists from their models. Most obviously, computers have made it possible to apply scientific models and methods as black-box tools: in an automated fashion, without understanding them. But the attitudes of the software industry, whose development tools computational science has inherited, have also contributed to this tendency. The focus of the software industry is on professional developers making tools for others that almost magically solve some of their problems. Users then get a manual, or hands-on training, for learning how to use the tool, but the inner workings of the tool are something they shouldn&amp;rsquo;t even have to think about. A good tool is one that minimizes learning requirements. Applied to science, this implies that users shouldn&amp;rsquo;t have to know the stories behind the models. Everyone with a dataset should be able to do statistical inference with a few mouse clicks and get a nice visualization. But without the stories, we can easily draw wrong conclusions from nice graphics.&lt;/p&gt;

&lt;p&gt;After a long period of separation of tools and stories, computational notebooks are now bringing some of the stories back. The enthusiastic adoption of notebooks by computational scientists is perhaps the best evidence for the importance of stories in science. But today&amp;rsquo;s notebooks capture only the surface stories of a research project. It&amp;rsquo;s tips of icebergs again. The typical notebook makes use of a large number of code libraries that are based on non-trivial scientific models, but the reader of the notebook remains completely unaware of them. Ideally, these models, with their stories, should be only a few clicks away.&lt;/p&gt;

&lt;p&gt;So what would an electronic representation of scientific models look like, ideally? It&amp;rsquo;s a collection of cross-referencing stories. In the celestial mechanics example, there&amp;rsquo;s a story about positions, velocities, and accelerations, which refers to a story about time and to a story about derivatives. There is another story that explains mass. The story of Newton&amp;rsquo;s law of motion, which also introduces the concept of force, can then refer to these more fundamental stories. If this description reminds you of Wikipedia, or in fact of any Wiki, you are right. Wikis are also collections of cross-referencing stories. What is missing in Wikis is a machine-readable version of the formalized parts of our models. Which, as I explained in &lt;a href="https://blog.khinsen.net/posts/2020/12/10/the-structure-and-interpretation-of-scientific-models/"&gt;part 1&lt;/a&gt;, needs to allow at least equations, specifications, and algorithms for its ingredients. Another feature that is missing in today&amp;rsquo;s Wikis, although some people are working on it, is the possibility to integrate computational tools in the form of code snippets. Their role would be to give access to visualizations, simulations, and other exploration tools.&lt;/p&gt;

&lt;p&gt;My own experiments in this domain are &lt;a href="https://github.com/khinsen/leibniz/"&gt;Leibniz&lt;/a&gt;, a digital scientific notation for embedding machine-readable formal models into human-readable stories, and the &lt;a href="https://github.com/activepapers/activepapers-pharo"&gt;Pharo edition of ActivePapers&lt;/a&gt;, which integrates datasets and computational tools into a Wiki-like collection of stories. Both ingredients require more work, and then need to be combined. There remains a lot of work to do.&lt;/p&gt;</description></item>
  <item>
   <title>The structure and interpretation of scientific models</title>
   <link>http://blog.khinsen.net/posts/2020/12/10/the-structure-and-interpretation-of-scientific-models/?utm_source=computational-science&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2020-12-10-the-structure-and-interpretation-of-scientific-models</guid>
   <pubDate>Thu, 10 Dec 2020 08:36:23 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;It is often said that science rests on two pillars, experiment and theory. Which has lead some to propose &lt;a href="https://physicsworld.com/a/the-third-pillar-of-science/"&gt;one&lt;/a&gt; or &lt;a href="https://www.hpcwire.com/2019/04/18/is-data-science-the-fourth-pillar-of-the-scientific-method/"&gt;two&lt;/a&gt; additional pillars for the computing age: simulation and data analysis. However, the &lt;em&gt;real&lt;/em&gt; two pillars of science are observations and models. Observations are the input to science, in the form of numerous but incomplete and imperfect views on reality. Models are the inner state of science. They represent our current understanding of reality, which is necessarily incomplete and imperfect, but understandable and applicable. Simulation and data analysis are tools for interfacing and thus comparing observations and models. They don&amp;rsquo;t add new pillars, but transforms both of them. In the following, I will look at how computing is transforming scientific models.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h2 id="empirical-models"&gt;Empirical models&lt;/h2&gt;

&lt;p&gt;The first type of scientific model that people construct when figuring out a new phenomenon is the &lt;em&gt;empirical&lt;/em&gt; or &lt;em&gt;descriptive&lt;/em&gt; model. Its role is to capture observed regularities, and to separate them from noise, the latter being small deviations from the regular behavior that are, at least provisionally, attributed to imprecisions in the observations, or to perturbations to be left for later study. Whenever you fit a straight line to a set of points, for example, you are constructing an empirical model that captures the linear relation between two observables. Empirical models almost always have parameters that must be fitted to observations. Once the parameters have been fitted, the model can be used to &lt;em&gt;predict&lt;/em&gt; future observations, which is a great way to test its generality. Usually, empirical models are constructed from generic building blocks: polynomials and sine waves for constructing mathematical functions, circles, spheres, and triangles for geometric figures, etc.&lt;/p&gt;

&lt;p&gt;The use of empirical models goes back a few thousand years. As I have described in &lt;a href="https://blog.khinsen.net/posts/2017/12/19/data-science-in-ancient-greece/"&gt;an earlier post&lt;/a&gt;, the astronomers of antiquity who constructed a model for the observed motion of the Sun and the planets used the same principles that we still use today. Their generic building blocks were circles, combined in the form of epicycles. The very latest variant of empirical models is machine learning models, where the generic building blocks are, for example, artificial neurons. Impressive success stories of machine learning models have led some enthusiasts to proclaim &lt;a href="https://www.wired.com/2008/06/pb-theory/"&gt;the end of theory&lt;/a&gt;, but I hope to be able to convince you in the following that empirical models of any kind are the beginning, not the end, of constructing scientific theories.&lt;/p&gt;

&lt;p&gt;The main problem with empirical models is that they are not that powerful. They can predict future observations from past observations, but that&amp;rsquo;s all. In particular, they cannot answer what-if questions, i.e. make predictions for systems that have never been observed in the past. The epicycles of Ptolemy&amp;rsquo;s model describing the motion celestial bodies cannot answer the question how the orbit of Mars would be changed by the impact of a huge asteroid, for example. Today&amp;rsquo;s machine learning models are no better. Their latest major success story as I am writing this is the &lt;a href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology"&gt;AlphaFold predicting protein structures from their sequences&lt;/a&gt;. This is indeed a huge step forward, as it opens the door to completely new ways of studying the folding mechanisms of proteins. It is also likely to become a powerful tool in structural biology, if it is actually made available to biologists. But it is not, as DeepMind&amp;rsquo;s blog post claims, &amp;ldquo;a solution to a 50-year-old grand challenge in biology&amp;rdquo;. We still do not know what the fundamental mechanisms of protein folding are, nor how they play together for each specific protein structure. And that means that we cannot answer what-if questions such as &amp;ldquo;How do changes in a protein&amp;rsquo;s environment influence its fold?&amp;rdquo;&lt;/p&gt;

&lt;h2 id="explanatory-models"&gt;Explanatory models&lt;/h2&gt;

&lt;p&gt;The really big success stories of science are models of a very different kind. &lt;em&gt;Explanatory&lt;/em&gt; models describe the underlying mechanisms that determine the values of observed quantities, rather than extrapolating the quantities themselves. They describe the systems being studied at a more fundamental level, allowing for a wide range of generalizations.&lt;/p&gt;

&lt;p&gt;A simple explanatory model is given by the &lt;a href="https://en.wikipedia.org/wiki/Lotka%E2%80%93Volterra_equations"&gt;Lotka-Volterra equations&lt;/a&gt;, also called predator-prey equations. This is a model for the time evolution of the populations of two species in a preditor-prey relation. An example is shown in this plot (Lamiot, CC BY-SA 4.0 &lt;a href="https://creativecommons.org/licenses/by-sa/4.0"&gt;https://creativecommons.org/licenses/by-sa/4.0&lt;/a&gt;, via Wikimedia Commons):&lt;/p&gt;

&lt;p&gt;&lt;img src="https://upload.wikimedia.org/wikipedia/commons/5/5b/Milliers_fourrures_vendues_en_environ_90_ans_odum_1953_en.jpg" alt="predator-prey" width="600" /&gt;&lt;/p&gt;

&lt;p&gt;An empirical model would capture the oscillations of the two curves and their correlations, for example by describing the populations as superpositions of sine waves. The Lotka-Volterra equations instead describe the interactions between the population numbers: predators and prey are born and die, but in addition predators eat prey, which reduces the number of prey in proportion to the number of predators, and contributes to a future increase in the number of predators because they can better feed their young. With that type of description, one can ask what-if questions: What if hunters shoot lots of predators? What if prey are hit by a famine, i.e. a decrease in their own source of food? In fact, the significant deviations from regular periodic change in the above plot suggests that such &amp;ldquo;outside&amp;rdquo; events are quite important in practice.&lt;/p&gt;

&lt;p&gt;Back to celestial mechanics. The decisive step towards an explanatory model was made by Isaac Newton, after two important preparatory steps by Copernicus and Kepler, who put the Sun at the center, removing the need for epicycles, and described the planets&amp;rsquo; orbits more accurately as ellipses. Newton&amp;rsquo;s laws of motion and gravitation fully explained these elliptical orbits and improved on them. More importantly, they showed that the fundamental laws of physics are the same on Earth and in space, a fact that may seem obvious to us today but wasn&amp;rsquo;t in the 17th century. Finally, Newton&amp;rsquo;s laws have permitted the elaboration of a rich theory, today called &amp;ldquo;classical mechanics&amp;rdquo;, that provides several alternative forms of the basic equations (in particular &lt;a href="https://en.wikipedia.org/wiki/Lagrangian_mechanics"&gt;Lagrangian&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Hamiltonian_mechanics"&gt;Hamiltonian&lt;/a&gt; mechanics), plus derived principles such as the conservation of energy. As for what-if questions, Newton&amp;rsquo;s laws have made it possible to send artefacts to the moon and to the other planets of the solar system, something which would have been unimaginable on the basis of Ptolemy&amp;rsquo;s epicycles.&lt;/p&gt;

&lt;p&gt;So far I have cited two explanatory models that take the form of differential equations, but that is not a requirement. An example from the digital age is given by &lt;a href="https://en.wikipedia.org/wiki/Agent-based_model"&gt;agent-based models&lt;/a&gt;. There is, however, a formal characteristic that is shared by all explanatory models that I know, and that distinguishes them from empirical models: they take the form of specifications.&lt;/p&gt;

&lt;h2 id="specifications-and-equations-vs-algorithms-and-functions"&gt;Specifications and equations vs. algorithms and functions&lt;/h2&gt;

&lt;p&gt;Let&amp;rsquo;s look at a simple problem for illustration: sorting a list of numbers (or anything else with a well-defined order). I have a list &lt;code&gt;L&lt;/code&gt;, with elements &lt;code&gt;L[i]&lt;/code&gt;, &lt;code&gt;i=1..N&lt;/code&gt; where &lt;code&gt;N&lt;/code&gt; is the length of the list &lt;code&gt;L&lt;/code&gt;. What I want is a sorted version which I will call &lt;code&gt;sorted(L)&lt;/code&gt;. The &lt;em&gt;specification&lt;/em&gt; for &lt;code&gt;sorted(L)&lt;/code&gt; is quite simple:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;&lt;code&gt;sorted(L)&lt;/code&gt; is a list of length &lt;code&gt;N&lt;/code&gt;.&lt;/li&gt;
 &lt;li&gt;For all elements of &lt;code&gt;L&lt;/code&gt;, their multiplicities in &lt;code&gt;L&lt;/code&gt; and &lt;code&gt;sorted(L)&lt;/code&gt; are the same.&lt;/li&gt;
 &lt;li&gt;For all &lt;code&gt;i=1..N-1&lt;/code&gt;, &lt;code&gt;sorted(L)[i] ≤ sorted(L)[i+1]&lt;/code&gt;.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;Less formally: &lt;code&gt;sorted(L)&lt;/code&gt; is a list with the same elements as &lt;code&gt;L&lt;/code&gt;, but in the right order.&lt;/p&gt;

&lt;p&gt;This specification of &lt;code&gt;sorted(L)&lt;/code&gt; is complete in that there is one unique list that satisfies it. However, it does not provide much help for actually constructing that list. That is what a sorting &lt;em&gt;algorithm&lt;/em&gt; provides. There are many known algorithms for sorting, and you can learn about them from &lt;a href="https://en.wikipedia.org/wiki/Sorting_algorithm"&gt;Wikipedia&lt;/a&gt;, for example. What matters for my point is that (1) given the specification, it is not a trivial task to construct an algorithm, (2) given a few algorithms, it is not a trivial task to write down a common specification that they satisfy (assuming of course that it exists). And that means that specifications and algorithms provide complementary pieces of knowledge about the problem.&lt;/p&gt;

&lt;p&gt;In terms of levels of abstraction, specifications are more abstract than algorithms, which in turn are more abstract than implementations. In the example of sorting, the move from specification to algorithm requires technical details to be filled in, in particular the choice of a sorting algorithm. Moving on from the algorithm to a concrete implementation involves even more technical details: the choice of a programming language, the data structures for the list and its elements, etc.&lt;/p&gt;

&lt;p&gt;In the universe of continuous mathematics, the relation between equations (e.g. differential equations) and the functions that satisfy them is exactly the same as the relation between specifications and algorithms in computation. Newton&amp;rsquo;s equations can thus be seen as a specification for the elliptical orbits that Kepler had described a bit earlier. Like in the case of sorting, it is not a trivial task to derive Kepler&amp;rsquo;s elliptical orbits from Newton&amp;rsquo;s equations, nor is it a trivial task to write down Newton&amp;rsquo;s equations as the common specification of all the (approximatively) elliptical orbits in the solar system. The two views of the problem are complementary, one being closer to the observations, the other providing more insight.&lt;/p&gt;

&lt;p&gt;One reason why specifications and equations are more powerful is that they are modular. Two specifications combined make up another, more detailed, specification. Two equations make up a system of equations. An example is given my Newton&amp;rsquo;s very general law of motion, which is extended by his law of gravitation to make a model for celestial mechanics. The same law of motion can be combined with different laws defining forces for different situations, for example the motion of an airplane. In contrast, there is no way to deduce anything about airplanes from Kepler&amp;rsquo;s elliptical planetary orbits. Functions and algorithms satisfy &lt;em&gt;complete&lt;/em&gt; specifications, and conserve little information about the &lt;em&gt;components&lt;/em&gt; from which this complete specification was constructed.&lt;/p&gt;

&lt;h2 id="a-challenge-for-computational-science"&gt;A challenge for computational science&lt;/h2&gt;

&lt;p&gt;Computational science initially used computers as a tool for applying structurally simple but laborious computational algorithms. The focus was on efficient implementations of known algorithms, later also on developing efficient algorithms for solving well-understood equations. The steps from specification to algorithm to implementation were done by hand, with little use of computational tools.&lt;/p&gt;

&lt;p&gt;That was 60 years ago. Today, we have computational models that are completely unrelated to the mathematical models that go back to the 19th century. And when we do use the foundational mathematical models of physics and chemistry, we combine them with concrete systems specifications whose size and complexity requires the use of computational tools. And yet, we still focus on implementations and to a lesser degree on algorithms, neglecting specifications almost completely. For many routinely used computational tools, the implementation is the only publicly accessible artefact. The algorithms they implement are often undocumented or not referenced, and the specifications from which the algorithms were derived are not written down at all. Given how crucial the specification level of scientific models has been in the past, we can expect to gain a lot by introducing it into computational science as well.&lt;/p&gt;

&lt;p&gt;To do so, we first need to develop a new appreciation for &lt;a href="https://f1000research.com/articles/3-101/v2"&gt;scientific models as distinct from the computational tools that implement them&lt;/a&gt;. We then need to think about how we can actually &lt;a href="https://peerj.com/articles/cs-158/"&gt;introduce specification-based models into the workflows of computational science&lt;/a&gt;. This requires designing computational tools that let us move freely between the three levels of specification, algorithm, and implementation. This is in my opinion the main challenge for computational science in the 21st century.&lt;/p&gt;

&lt;h2 id="finally"&gt;Finally&amp;hellip;&lt;/h2&gt;

&lt;p&gt;Some readers may have recognized that the title of this post is a reference to two books, &lt;a href="https://mitpress.mit.edu/sites/default/files/sicp/full-text/book/book.html"&gt;Structure and Interpretation of Computer Programs&lt;/a&gt; (with a &lt;a href="https://sarabander.github.io/sicp/html/index.xhtml"&gt;nice though inofficial online version&lt;/a&gt;) and &lt;a href="https://mitpress.mit.edu/books/structure-and-interpretation-classical-mechanics"&gt;Structure and Interpretation of Classical Mechanics&lt;/a&gt; (also &lt;a href="https://tgvaughan.github.io/sicm/toc.html"&gt;online&lt;/a&gt;). The second one is actually somewhat related to the topic of this post: it is a textbook on classical mechanics that uses computational techniques for clarity of exposition. More importantly, both books focus on inducing a deep understanding of their topics, rather than on teaching superficial technical details. This humble blog post cannot pretend to reach that level, of course, but its goal is to spark developments that will culminate in textbooks of the same quality as its two inspirations.&lt;/p&gt;</description></item>
  <item>
   <title>The rise of community-owned monopolies</title>
   <link>http://blog.khinsen.net/posts/2020/02/26/the-rise-of-community-owned-monopolies/?utm_source=computational-science&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2020-02-26-the-rise-of-community-owned-monopolies</guid>
   <pubDate>Wed, 26 Feb 2020 15:04:54 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;One question I have been thinking about in the context of reproducible research is this: Why is all stable software technology old, and all recent technology fragile? Why is it easier to run 40-year-old Fortran code than ten-year-old Python code? A hypothesis that comes to mind immediately is growing code complexity, but I&amp;rsquo;d expect this to be an amplifier rather than a cause. In this pose, I will look at another candidate: the dominance of Open Source communities in the development of scientific software.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;h2 id="from-markets-to-monopolies"&gt;From markets to monopolies&lt;/h2&gt;

&lt;p&gt;In the 1990s, when I was working on my thesis, the world of scientific computing was very different from what it is now. Innovation was driven by hardware. Processor speeds kept increasing, and new processor architectures appeared on the market in rapid succession. In the course of the 1990s, I did most of my work on Unix workstations based on variety of architectures: &lt;a href="https://en.wikipedia.org/wiki/PA-RISC"&gt;PA-RISC&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/MIPS_architecture"&gt;MIPS&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/PowerPC"&gt;PowerPC&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/DEC_Alpha"&gt;DEC Alpha&lt;/a&gt;. I also worked on mainframe computers made by IBM, Fujitsu, and Cray, all using proprietary processors. Each manufacturer sold a package of hardware, operating system, and development tools such as compilers. Compilers implemented standardized programming languages, mainly Fortran and C, with manufacturer-specific extensions that most people stayed away from because they expected to be using different machines a few years later. The computing platforms that everybody was developing for were not processors nor operating systems, but Fortran~77 and ANSI-C, each of which had developed its ecosystem of scientific libraries. For an interactive development platform, add Unix and X11. Mixing Fortran and C was somewhat platform-specific, but very doable as well. Every time I changed labs and computers during my postdoc years, I had to spend a day or two to reinstall everything I needed, but I never suffered &lt;a href="https://hal.archives-ouvertes.fr/hal-02117588"&gt;software collapse&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Today, hardware innovation in mainstream computing has almost come to a halt. All the processor architectures listed above are gone. The x86 architecture, implemented in chips from Intel and AMD, dominates scientific computing, and in fact all of computing except for mobile devices. Hardware manufacturers therefore no longer supply compilers. For everyday work, most people use the free &lt;a href="https://en.wikipedia.org/wiki/GNU_Compiler_Collection"&gt;GNU Compiler Collection&lt;/a&gt; or the equally free &lt;a href="https://en.wikipedia.org/wiki/Clang"&gt;Clang&lt;/a&gt; compiler. For performance-critical work, commercial compilers from companies such as &lt;a href="https://www.nag.com/"&gt;NAG&lt;/a&gt;, &lt;a href="https://www.pgroup.com/index.htm"&gt;PGI&lt;/a&gt;, or &lt;a href="https://software.intel.com/en-us/compilers"&gt;Intel&lt;/a&gt; offer better performance and libraries fine-tuned for high-performance computing. The standards defining Fortran and C have evolved, but have maintained strict backwards compatibility.&lt;/p&gt;

&lt;p&gt;However, in the everyday life of computational scientists, these traditional platforms have lost importance. A new breed of languages and scientific ecosystems, such as &lt;a href="https://www.python.org/"&gt;Python&lt;/a&gt;, &lt;a href="https://www.r-project.org/"&gt;R&lt;/a&gt;, and &lt;a href="https://julialang.org/"&gt;Julia&lt;/a&gt;, have become the dominant support for scientific software in many (though not all) domains of research. Their rise has gone hand in hand with software collapse becoming so common that many consider it normal or even inevitable. Scientists are starting to adopt heavy technology with large overheads in terms of complexity and invested effort to work around the problem (if you didn&amp;rsquo;t guess yet, I am referring to containers). I waste a lot more time today with configuration and setup work (including configuration debugging) than I did in the 1990s. How did we get into this sad state of affairs? Is there any hope for getting out of it again?&lt;/p&gt;

&lt;p&gt;One reason that immediately comes to mind is increasing software complexity. But that&amp;rsquo;s more of a symptom than a cause. A better explanation would be an increased &lt;em&gt;problem&lt;/em&gt; complexity that would then &lt;em&gt;require&lt;/em&gt; more complex software. Problem complexity is much harder to measure, but I don&amp;rsquo;t see much evidence supporting this hypothesis. We certainly do bigger computations, on larger datasets, but if I look at today&amp;rsquo;s commonly used models and methods in computational science, they don&amp;rsquo;t look more complex than what I saw in the 1990s. What has increased, however, is variety. Today&amp;rsquo;s science relies on &lt;em&gt;more&lt;/em&gt; computational models than it did 30 years ago, and I believe that this contributes to the fragility issue, as I will explain later.&lt;/p&gt;

&lt;p&gt;There is another reason that I haven&amp;rsquo;t heard anyone mention so far: the disappearance of technology markets in favor of monopolist players who can count on customer lock-in. This description will probably make you think of Microsoft&amp;rsquo;s grip on the Windows user base, or the &amp;ldquo;walled gardens&amp;rdquo; that Google and Apple have created around their mobile platforms. But there is another category of monopoly owner in the tech world that is hardly recognized as such: Open Source communities.&lt;/p&gt;

&lt;h2 id="open-source-monopolists"&gt;Open Source monopolists&lt;/h2&gt;

&lt;p&gt;Consider two recent events: Microsoft killing Windows 7, and the Python community killing Python 2. The story is essentially the same in both cases: the creator of a piece of infrastructure software ends support for an old but still widely used version, forcing its users to move on to a later but not fully backwards compatible version. In both cases, a significant part of the user community would have preferred to stick to the older version, as has been nicely &lt;a href="https://xkcd.com/2224/"&gt;illustrated by xkcd&lt;/a&gt;. In both cases, the end-of-support decision is a rational one for the producer because supporting old versions is costly. And in both cases, the abandoned users have no other supplier they can turn to, because the producer holds a monopoly on the technology.&lt;/p&gt;

&lt;p&gt;Compare this to the diverse market of the 1990s. Producers of infrastructure software could add new functionality and try to win new clients with such improvements, but they could not afford to cause damage to their existing user base because users would simply turn to a competitor. There are many sources for standards-conforming Fortran compilers, but there is only one source for Windows or Python.&lt;/p&gt;

&lt;p&gt;I suspect some readers will feel anger at this point. How dare you compare a monopolist business to a community of unpaid volunteers offering their work to the world for free? The crucial point is that I am comparing them as seen from the outside. There is a wide gap between the self-image that Open Source communities have of themselves and the image that they present to the outside world, and I believe that this is a big part of the problem.&lt;/p&gt;

&lt;p&gt;Open Source communities tend to see themselves as communities of like-minded people that get organized to work together towards shared objectives. They see themselves much like a sports club that organizes practice sessions for its members, or like a village community that collectively plans its road infrastructure. But this is not at all how Open Source communities present themselves to the outside world. The Web site of a sports club says something like &amp;ldquo;We are a bunch of people enthusiastic about playing football. If you are as well, come and join us.&amp;rdquo; Now look at the &lt;a href="https://www.python.org/"&gt;Python Web site&lt;/a&gt;. Its first statement, in big letters, is &amp;ldquo;Python is a programming language that lets you work quickly and integrate systems more effectively.&amp;rdquo; The site is about a product. Its goal is to convince people to use Python, not to join a community. It is more similar to &lt;a href="https://www.microsoft.com/fr-fr/windows/"&gt;Microsoft&amp;rsquo;s Windows site&lt;/a&gt; than to the site of a sports club.&lt;/p&gt;

&lt;p&gt;&amp;ldquo;But&amp;hellip;&amp;rdquo; I hear you say. Open Source. Free Software, as in &amp;ldquo;free beer&amp;rdquo; &lt;em&gt;and&lt;/em&gt; in &amp;ldquo;free speech&amp;rdquo;. And everybody can join in, the community is so welcoming! Fine, but that&amp;rsquo;s again the insiders&amp;rsquo; view, just slightly enlarged to the circle of people whose engagement with the technology is sufficiently deep that they consider joining the community. I suspect that most people who download and install Python the product will never know anything about the community, and many will even use Python without being aware of it at all. What they are aware of is an application or utility written in Python, e.g. &lt;a href="https://calibre-ebook.com/"&gt;Calibre&lt;/a&gt; for managing their e-books, or &lt;a href="https://www.offlineimap.org/"&gt;offlineimap&lt;/a&gt; for downloading e-mail. In contrast, a true community-oriented piece of software would have a splash screen saying &amp;ldquo;Welcome to the Python community! Before using this software, please become familiar wit how our community works&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Sports clubs and village communities focus on their members&amp;rsquo; needs, interacting with the outside world by necessity, but only as a side effect. Most Open Source communities are more like political parties or non-government organizations in that they &lt;em&gt;want&lt;/em&gt; to have an impact on the outside world. They care about the popularity of their products, and make efforts to increase their mind share. The reward they get in return is not money, but that&amp;rsquo;s the only difference from how a company works. Both Open Source communities and software companies have an interest in attracting new clients and keeping existing ones. Both can retain clients more efficiently by generating lock-in, and so they do.&lt;/p&gt;

&lt;p&gt;Note that I am not saying that either one creates lock-in intentionally. For Open Source communities such as Python, which I know sufficiently well, I am convinced there is no such intention. For companies such as Microsoft or Google, I can&amp;rsquo;t know for sure. But from the clients&amp;rsquo; perspective, it doesn&amp;rsquo;t matter if lock-in is intentional or a side effect.&lt;/p&gt;

&lt;p&gt;One particularity about computing technology is that lock-in happens by default. It takes a conscious effort (and thus an incentive) to &lt;em&gt;avoid&lt;/em&gt; lock-in. The reason is the fine-grained complexity of software interfaces coupled with the near-zero cost of modifying them. There are so many details that re-implementing an existing interface exactly requires a precise documentation of that interface, a perfectionist attitude, and a lot of time. The markets of the 1990s were made possible only by lengthy and costly standardization processes. Which in turn the participants accepted only because without the markets defined by those standards, none of them could continue to innovate in the field of processor architectures.&lt;/p&gt;

&lt;h2 id="lock-in-favors-software-collapse"&gt;Lock-in favors software collapse&lt;/h2&gt;

&lt;p&gt;So far for communities as monopoly holders. Back to my original question: how did software collapse become normal? I believe that this is a near-automatic consequence of infrastructure software being managed by monopoly holders. The monopoly situation prevents existing users from moving elsewhere, significantly reducing the effort that needs to be made to keep them. All effort can thus be concentrated on gaining new users, which leads to the paradoxical situation that the needs of non-users have a larger weight in strategic decisions than the needs of the user base. With backwards compatibility being costly, boring, and irrelevant to the non-users that matter for the future, why care about it? That is, in my opinion, what happened to the Scientific Python ecosystem starting in the 2010s: adoption by the explosively growing data science community drowned the existing user base. The best strategy for SciPy was then to focus on the needs of the data science people, which also became the primary source for recruiting developers and maintainers.&lt;/p&gt;

&lt;p&gt;Which brings me back to what I said earlier: the diversification of techniques in computational science is part of the problem. While the various subdomains of computational science have overlapping requirements, they also have divergent needs. The longevity of code is one aspect whose importance varies a lot, but there are others: the size of a typical computational task, the size of the datasets being processed, the nature of the algorithms being applied, the hardware platforms that matter most, and many more. While in theory Open Source is good for supporting diversity (&amp;ldquo;just fork the code and adapt it to your needs&amp;rdquo;), the reality of today&amp;rsquo;s major Open Source communities is exactly the opposite: a focus on &amp;ldquo;let&amp;rsquo;s all work together&amp;rdquo;. Combine this with the chronic lack of funding, and thus also a lack of incentives for developing the structured governance that would administrate funding and create activity reports, and you end up with large number of users depending on the work of a small number of inexperienced developers in precarious positions who cannot reasonably be expected to make an effort to even understand the needs of the user base at large. In a way, software collapse is a consequence of &lt;a href="https://en.wikipedia.org/wiki/Conway's_law"&gt;Conway&amp;rsquo;s law&lt;/a&gt; applied to Open Source communities.&lt;/p&gt;

&lt;h2 id="can-we-do-better"&gt;Can we do better?&lt;/h2&gt;

&lt;p&gt;Given that today&amp;rsquo;s tech world is dominated by software and Open Source communities, rather than by hardware-producing companies, is it possible to return to a market situation with no or weak lock-in? I don&amp;rsquo;t think so. Standards-based markets can only form when there are multiple competing producers right from the start. In contrast, Open Source communities start out small and adventurous, with a few growing big and becoming infrastructure suppliers. In the beginning, they have no competition, and when they are big, new communities cannot possibly start to compete with them in the mindshare market. Which leaves two possibilities: Open Source communities could become more user-oriented, or the maintenance of infrastructure software could be ensured by other types of organizations. Let&amp;rsquo;s start by looking at the first possibility.&lt;/p&gt;

&lt;p&gt;An important first step would be Open Source communities recognizing that they are developing and selling products to a user base that extends far beyond the circle of potential community members. A good time for that would be just now. Many Open Source communities have recently realized that the shared idealistic goal of an Open Source world is not sufficient for ensuring respectful collaboration, and have reacted by introducing codes of conduct. What I am suggesting here is a similar approach for making the relation with the user base more explicit. The absence of a legal contract between developers and users is one of the core principles of Open Source, but that doesn&amp;rsquo;t imply the absence of moral obligations. Any organization that wants to have an impact on the outside world must consider how this impact affects the life and work of other people. It should then define moral commitments, in written, even if the license prevents them from being legally enforced. A nice example are the &lt;a href="http://big-data-biology.org/software/commitments/"&gt;Big Data Biology Lab Software Tool Commitments&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Open Source communities could also more actively solicit feedback from the outside. Getting useful feedback from low-engagement users is difficult, but there are proxies, for example the people who package software for various distributions.&lt;/p&gt;

&lt;p&gt;But perhaps Open Source communities are just not the right form of organization for infrastructure software. There are other entities that create Open Source software, such as the &lt;a href="https://www.mozilla.org/"&gt;Mozilla&lt;/a&gt; and &lt;a href="https://apache.org/"&gt;Apache&lt;/a&gt; foundations, or hybrids such as the &lt;a href="https://pharo.org/community"&gt;Pharo community&lt;/a&gt; with the &lt;a href="https://consortium.pharo.org/"&gt;Pharo consortium&lt;/a&gt; and the &lt;a href="https://association.pharo.org/"&gt;Pharo User Association&lt;/a&gt; providing channels for users to influence development. It seems probable that more useful organizational forms are waiting to be discovered. In fact, a good guess is that software should best be managed much like other scientific infrastructure: by specific institutions that ensure long-term funding and provide software as a service to research communities.&lt;/p&gt;</description></item>
  <item>
   <title>Pharo year one</title>
   <link>http://blog.khinsen.net/posts/2019/12/31/pharo-year-one/?utm_source=computational-science&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2019-12-31-pharo-year-one</guid>
   <pubDate>Tue, 31 Dec 2019 14:40:33 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;It&amp;rsquo;s the season when everyone writes about the past year, or even the past decade for a year number ending in 9. I&amp;rsquo;ll make a modest contribution by summarizing my experience with Pharo after one year of using it for projects of my own.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;My first contact with Pharo happened a bit more than one year ago, when I signed up for the &lt;a href="http://mooc.pharo.org/"&gt;Pharo MOOC&lt;/a&gt; in October 2018. But following a MOOC means working on exercice problems defined by someone else. Getting a real feeling for a programming system requires moving on to problems you actually care about. That&amp;rsquo;s why I started three Pharo-based projects in 2019. The main one is the &lt;a href="https://www.activepapers.org/pharo-edition/"&gt;Pharo edition of ActivePapers&lt;/a&gt;, the other ones are &lt;a href="https://github.com/khinsen/ipfs-pharo/"&gt;an exploration&lt;/a&gt; of the &lt;a href="https://ipfs.io"&gt;Interplanetary File System (IPFS)&lt;/a&gt; and a &lt;a href="https://github.com/khinsen/leibniz-pharo/"&gt;second implementation of my digital scientific notation Leibniz&lt;/a&gt;. In all these projects, the user interface is an important aspect, because that&amp;rsquo;s one of my major motivations for using Pharo. However, instead of the standard Pharo user interface framework, which is an evolution of the original Smalltalk user interface of the 1980s, I used the &lt;a href="https://gtoolkit.com/"&gt;Glamorous Toolkit&lt;/a&gt;, a complete redesign with many interesting new ideas. Perhaps the most significant innovation in the Glamorous Toolkit from my perspective is the introduction of a computational document. It resembles the fashionable computational notebooks in many ways, but differs in being an integral part of a live programming system.&lt;/p&gt;

&lt;p&gt;As I wrote in my &lt;a href="https://blog.khinsen.net/posts/2018/12/19/exploring-pharo/"&gt;initial blog post&lt;/a&gt; on Pharo, I started out by exploring the system using the tools it provides for that purpose. In retrospect, this is clearly the strongest aspect of Pharo. The combination of code browsers, code search, object inspection, and execution inspection (via a tool misleadingly called a debugger) is an extremely powerful way to understand complex software systems. The best evidence is that I was able to write useful and non-trivial extensions to the Glamorous Toolkit, which still is rapidly evolving alpha-stage software and, judged by standard metrics such as lines of documentation per line of code, badly documented. But such metrics make no sense in a system in which searching the code base is faster than documentation lookup in standard environments. Going back to such environments after working with Pharo is a very frustrating experience.&lt;/p&gt;

&lt;p&gt;Note that I am not saying that the Pharo environment is perfect. For my taste it requires way too much mouse use. I am still much more productive in Emacs than in Pharo for tasks supported by both, mainly because I can keep my hands on the keyboard. I also find the standard code browser in Pharo too limiting in only showing one method at a time. The Glamorous Toolkit is a clear improvement in that respect. But all the criticism I can come up with is about details that can be fixed, whereas the main defects that I now see in almost every other software development environment is much more fundamental: they suffer from a barrier that separates development tools on one side from the code under development on the other side.&lt;/p&gt;

&lt;p&gt;Similar remarks apply to the Smalltalk language on which Pharo is built. It&amp;rsquo;s a minimal programming language that puts its object system in center stage and pushes as many features as possible into its libraries. That&amp;rsquo;s certainly an interesting point in design space to explore, but I&amp;rsquo;d personally prefer to have a couple of important concepts (for example immutable objects) as language features, rather than as implementation details of class hierarchies. But then, no language is perfect, and Smalltalk is certainly good enough for my needs.&lt;/p&gt;

&lt;p&gt;The most serious problem that I have with Pharo is that I don&amp;rsquo;t see how I could use it productively for my own research in computational biophysics in the near future. There is a small computational science community around Pharo (see e.g. &lt;a href="https://github.com/pharo-open-documentation/awesome-pharo#scientific-libraries"&gt;this list&lt;/a&gt; of scientific libraries), but most of the infrastructure code that I&amp;rsquo;d need is missing. Moreover, Pharo evolves too rapidly for the kind of computational research that I do (see &lt;a href="https://blog.khinsen.net/posts/2017/11/16/a-plea-for-stability-in-the-scipy-ecosystem/"&gt;my critique of the SciPy ecosystem&lt;/a&gt; for some background information). Finally, reproducible computations remain a challenge because there isn&amp;rsquo;t much of a support infrastructure for reproduciblity in Pharo so far, although the recent &lt;a href="https://github.com/guillep/PharoBootstrap"&gt;work on bootstrapping&lt;/a&gt; is an important first step.&lt;/p&gt;

&lt;p&gt;On a longer time scale, I can imagine Pharo replacing Emacs as my main user interface to computing, with the hard-core science written in different languages but interfaced to Pharo. I expect IPFS to play an important role at the cross-language interface, for various reasons that deserve an entire blog post on their own. However, it takes a lot of not-yet-written code to get there. Too much to define this as a realistic goal for myself. This means that my future use of Pharo mainly depends on the directions taken by the Pharo community over the coming years. I am pretty sure that Pharo will remain an important tool in my toolbox, I just don&amp;rsquo;t know what its exact role will be.&lt;/p&gt;</description></item>
  <item>
   <title>The computational notebook of the future (part 2)</title>
   <link>http://blog.khinsen.net/posts/2019/05/09/the-computational-notebook-of-the-future-part-2/?utm_source=computational-science&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2019-05-09-the-computational-notebook-of-the-future-part-2</guid>
   <pubDate>Thu, 09 May 2019 12:07:39 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;A while ago I &lt;a href="http://blog.khinsen.net/posts/2019/02/11/the-computational-notebook-of-the-future/"&gt;wrote about my ideas for a successor of today&amp;rsquo;s computational notebooks.&lt;/a&gt; Since then I have made some progress on a prototype implementation, which is the topic of this post. Again I have made a companion &lt;a href="https://vimeo.com/339361206"&gt;screencast&lt;/a&gt; so that you can get a better idea of how all this works in practice.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;As a reminder, the two aspects of today&amp;rsquo;s notebooks (&lt;a href="https://www.wolfram.com/mathematica/"&gt;Mathematica&lt;/a&gt;, &lt;a href="https://jupyter.org/"&gt;Jupyter&lt;/a&gt;, &lt;a href="https://rmarkdown.rstudio.com/"&gt;R markdown&lt;/a&gt;, &lt;a href="https://orgmode.org/worg/org-contrib/babel/"&gt;Emacs/OrgMode&lt;/a&gt;) that I consider harmful for scientific communication are:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;
  &lt;p&gt;The linear structure of a notebook that forces the narrative to  follow the order of the computation.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;The impossibility to refer to data and code in a notebook from the  outside, and in particular from another notebook, making reuse of  code and data impossible.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;Like the demo that I made last time, and which is best qualified as a quick hack, the computational document that I am presenting today is implemented in &lt;a href="https://pharo.org/"&gt;Pharo&lt;/a&gt; and builds on the &lt;a href="https://gtoolkit.com/"&gt;Glamorous Toolkit&lt;/a&gt;, which is an innovative development environment designed around the notion of &amp;ldquo;moldable development&amp;rdquo;, which means that developers should be able to adapt their tools to their specific needs with little effort. This is precisely what I have done. The code is &lt;a href="https://github.com/activepapers/activepapers-pharo"&gt;on GitHub&lt;/a&gt; and includes the example document from the demo.&lt;/p&gt;

&lt;p&gt;Contrary to today&amp;rsquo;s notebooks, my computational documents consist of two distinct layers, which I show for an example in the screencast. A &lt;em&gt;workflow layer&lt;/em&gt; consists of &lt;em&gt;scripts&lt;/em&gt; (short pieces of code) that compute &lt;em&gt;datasets&lt;/em&gt; keep track of the data dependencies. The workflow layer can be visualized as a graph. Scripts and datasets make up a standard Pharo object that can be used as a building block in subsequent work, unlike the code and data in today&amp;rsquo;s notebooks. For example, the Pharo expression &lt;code&gt;InfluenzaLikeIllnessInFrance data
absoluteIncidence&lt;/code&gt; yields one of the data frames from my example document and can be used in any type of Pharo code, including code in another document.&lt;/p&gt;

&lt;p&gt;On top of that workflow layer, there is a documentation layer consisting of a Wiki-style multi-page document in which each page can contain code snippets. These code snippets are intended for data presentation (plotting etc.) and for demonstrations (examples, verifications, etc.) They are not accessible from outside their pages, and they cannot change the datasets computed by the workflow. The documentation pages can refer to and include the datasets, the scripts, but also arbitrary other Pharo code. In particular, this allows including library code used by the workflow scripts in the documentation layer, as opposed to today&amp;rsquo;s notebooks for which library code is undocumentable black-box code.&lt;/p&gt;

&lt;p&gt;A third essential element is the &lt;em&gt;playground&lt;/em&gt; attached to the workflow. This is where interactive exploration takes place. Code snippets in the playground can access datasets just like scripts, but they cannot modify them. The playground is meant both for authors and for readers. Authors develop scripts incrementally in the playground, and turn them into scripts (at the click of a button) when they are satisfied. Readers can write code snippets for exploring the data in more detail.&lt;/p&gt;

&lt;p&gt;The code is currently &amp;ldquo;demo quality&amp;rdquo;, so please don&amp;rsquo;t rely on it for your own research. Even the underlying GToolkit library is still advertised as alpha level. There is a reason for calling this the future rather than the present! However, there are a few conclusions that I am already willing to draw from this work:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;
  &lt;p&gt;An authoring environment for computational documents should also  be a more general software development environment. If you have  to change tools for switching from library code to a computational  document or back, you have a technological barrier to overcome  that creates a mental separation between &amp;ldquo;inside&amp;rdquo; and &amp;ldquo;outside&amp;rdquo;,  whereas the science that you want to communicate is on both sides  of your barrier.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;The emphasis on making all code and data explorable that has been  part of Smalltalk culture from the start is highly beneficial for  computational science as well. Notebook environments such as  Jupyter or RStudio feel extremely limited compared to the standard  Pharo environment, let alone the more advanced GToolkit.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;Decomposing the computation into smaller independent scripts  with well-defined interfaces makes it more understandable.  In the traditional linear notebooks, you never know how far  further down a temporary variable will be used. You must  read the code from top to bottom to be sure not to miss  something. Likewise, separating &amp;ldquo;essential&amp;rdquo; computations  on the data from &amp;ldquo;superficial&amp;rdquo; computations such as plotting  makes the overall scientific logic stand out better.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;A good authoring environment must support the full lifecycle of  computer-aided research, starting with interactive exploration and  iterating towards a computational document optimized for the  reader rather than the author. Today&amp;rsquo;s notebooks do not provide  this support by sticking to a linear structure that is  satisfactory only in the initial stages of the lifecycle.&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</description></item>
  <item>
   <title>The computational notebook of the future</title>
   <link>http://blog.khinsen.net/posts/2019/02/11/the-computational-notebook-of-the-future/?utm_source=computational-science&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2019-02-11-the-computational-notebook-of-the-future</guid>
   <pubDate>Mon, 11 Feb 2019 13:29:24 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;Regular readers of this blog may have noticed that I am not very happy with today&amp;rsquo;s state of computational notebooks, such as they were pioneered by Mathematica and popularized by more recent free incarnations such as &lt;a href="https://jupyter.org/"&gt;Jupyter&lt;/a&gt;, &lt;a href="https://rmarkdown.rstudio.com/"&gt;R markdown&lt;/a&gt;, or &lt;a href="https://orgmode.org/worg/org-contrib/babel/"&gt;Emacs/OrgMode&lt;/a&gt;. In this post and the &lt;a href="https://peervideo.net/videos/watch/9ed70819-6271-439f-b392-54f34b73c124"&gt;accompanying screencast&lt;/a&gt; (my first one!), I will explain what I dislike about today&amp;rsquo;s notebooks, and how I think we can do better.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;There are two aspects of notebooks that I consider harmful for scientific communication:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;The linear structure of a notebook that forces the narrative to follow the order of the computation.&lt;/li&gt;
 &lt;li&gt;The impossibility to refer to data and code in a notebook from the outside, and in particular from another notebook, making reuse of code and data impossible.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;If you look at a traditional scientific article, or technical report, you will notice that its narrative is structured according to a high-level view of the work. It starts by describing the context of the work, then its goals and a very brief summary of the methods, and right after that it presents results and discusses them. Technical details are only discussed afterwards, once the reader understands why they actually matter. With today&amp;rsquo;s notebooks, the technical details come first: a typical data analysis starts with cleanup and preprocessing steps, and therefore they also come first in the narrative.&lt;/p&gt;

&lt;p&gt;An unpleasant side effect of the &amp;ldquo;narrative follows computation&amp;rdquo; principle is that some technical details actually cannot be discussed adequately. Scientific methods implemented in software libraries can be summarized in plain English, but the code is elsewhere, managed by a different toolset, and cannot be shown to the reader.&lt;/p&gt;

&lt;p&gt;This makes the transition to the second problematic aspect: there is no way to refer to or reuse any specific part of a notebook. Neither the code nor the computed results are accessible from the outside. And that also makes it impossible to build up useful libraries from notebooks.&lt;/p&gt;

&lt;p&gt;So far for the criticism - now let&amp;rsquo;s make it constructive. At this point, you should watch the &lt;a href="https://peervideo.net/videos/watch/9ed70819-6271-439f-b392-54f34b73c124"&gt;screencast&lt;/a&gt; before reading on. In the screencast, I show a simple data analysis both as a Jupyter notebook and as a demo prototype for what I consider the notebook of the future. This prototype is built using the &lt;a href="https://gtoolkit.com/"&gt;Glamorous Toolkit&lt;/a&gt;, a very innovative software development environment for &lt;a href="https://pharo.org/"&gt;Pharo&lt;/a&gt;, which is a modern descendant of &lt;a href="https://en.wikipedia.org/wiki/Smalltalk"&gt;Smalltalk&lt;/a&gt;. If you want to play with this yourself, the code is &lt;a href="https://github.com/khinsen/computational-documents-with-gtoolkit/"&gt;on GitHub&lt;/a&gt;. It&amp;rsquo;s really just a demo, because the simplistic approach to organizing the computation that I have used there would not scale to real-life computations (it does a lot of needless recomputation). My plan is to implement the &lt;a href="https://www.activepapers.org/"&gt;ActivePapers&lt;/a&gt; approach for managing the computations. GToolkit is alpha software as well. So none of this is ready for prime time, but it does show that better notebooks are possible.&lt;/p&gt;

&lt;p&gt;Unlike today&amp;rsquo;s notebooks, which are a sequence of code snippets and documentation paragraphs, the computational documents of my demo are &lt;em&gt;objects&lt;/em&gt; in the sense of object-oriented programming. Each document contains code, input data, and computed data, which can be accessed from the outside and thus reused in client code. The narrative is merely an additional view into these items, which can present and discuss them in any order that seems suitable for explaining the work. Like with scientific articles, the narrative is typically written in the final stages of the work, once the basic code skeleton is working. In the case of my demo, I started out writing the two Pharo classes, before even installing GToolkit which was a bit unstable at the time.&lt;/p&gt;

&lt;p&gt;Note that this &amp;ldquo;one job, one object, one narrative&amp;rdquo; approach has a beneficial side effect in encouraging people to do each job well, rather than just well enough for going on with the next job. My Jupyter/Python version of the data analysis only extracts the minimum information required from the input dataset, without even mentioning what else is in there. The GToolkit/Pharo version provides a complete description of the dataset, including the data that is not used at all in the second document that describes the analysis.&lt;/p&gt;

&lt;p&gt;Finally, there are other interesting aspects of GToolkit (and Pharo) for computational science, but I will leave them for future posts. I will just mention that the &amp;ldquo;inspectors&amp;rdquo; (a term familiar to every Smalltalk developer but probably unknown to anyone else) are easily extensible. Adding a pane that provides yet another view of the document is a matter of writing a couple of lines of Pharo code. It&amp;rsquo;s as if you could implement a new widget for Jupyter in a few lines of Python code right in your notebook.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: There&amp;rsquo;s a workaround for embedding figures (thanks to Tudor Gîrba for the hint!), which you can find in the current code version on GitHub.&lt;/p&gt;</description></item>
  <item>
   <title>Exploring Pharo</title>
   <link>http://blog.khinsen.net/posts/2018/12/19/exploring-pharo/?utm_source=computational-science&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2018-12-19-exploring-pharo</guid>
   <pubDate>Wed, 19 Dec 2018 05:26:08 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;One of the more interesting things I have been playing with recently is &lt;a href="http://www.pharo.org/"&gt;Pharo&lt;/a&gt;, a modern descendent of Smalltalk. This is a summary of my first impressions after using it on a &lt;a href="https://github.com/khinsen/leibniz-pharo/"&gt;small (and unfinished) project&lt;/a&gt;, for which it might actually turn out to be very helpful.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;The first time I read about Smalltalk was in the &lt;a href="https://archive.org/details/byte-magazine-1981-08"&gt;August 1981 issue of Byte magazine&lt;/a&gt;. Back then, I was a high school student and I had just invested my savings into my first home computer with characteristics typical for the time: Z80 processor, 16 KB of memory, Microsoft Basic, data storage on cassette tapes. From that perspective, Smalltalk was a utopia. The revolutionary aspect of Smalltalk was its design as an integrated computing system that combined a language, a huge standard library, a development environment, and perhaps most of all a graphical user interface (GUI), which in fact was the ancestor of all of today&amp;rsquo;s desktop-style GUIs. As a consequence, it required a high-quality graphics display, a mouse, and plenty of CPU power. None of that was available in commodity hardware.&lt;/p&gt;

&lt;p&gt;In 1995, a friend passed me a floppy disk with Smalltalk&amp;ndash;80 for the Atari ST family, and I could finally lay my hands on a working Smalltalk system. By then I had an Atari TT with the awesome big high-resolution black-and-white screen that was available for it. Just perfect for Smalltalk. I was very impressed by the system, which in many respects was superior to the Atari&amp;rsquo;s native TOS/GEM combo, and even to the Unix workstations I had in the lab. But I couldn&amp;rsquo;t actually use it for anything productive, because Smalltalk lived in a separate universe, unable to access any file on my hard disk. It wasn&amp;rsquo;t more than an impressive demo of what computing could be like.&lt;/p&gt;

&lt;p&gt;I have faint memories of playing with &lt;a href="http://www.squeak.org/"&gt;Squeak&lt;/a&gt; a couple of years later, but I found its flashy colors and toy-inspired aesthetics so unpleasant that I didn&amp;rsquo;t go very far. Pharo is actually a fork of Squeak that evolved into a different direction, with a more sober design that is much more to my liking. More importantly, some of the on-going developments in the Pharo community (in particular the &lt;a href="https://gtoolkit.com/"&gt;Glamorous Toolkit&lt;/a&gt;) are much in line with my recent interest in the &lt;a href="https://peerj.com/articles/cs-158/"&gt;human-computer interface of computational science&lt;/a&gt;. The 2018 session of the &lt;a href="https://mooc.pharo.org/"&gt;Pharo MOOC&lt;/a&gt; was thus a good occasion to take a more serious look at this up-to-date incarnation of Smalltalk. The MOOC does a pretty good job at introducing Pharo to people with various interests, and it even includes some explanations of the internal workings of Pharo (look for the &amp;ldquo;black magic&amp;rdquo; label).&lt;/p&gt;

&lt;p&gt;As a language, Smalltalk was revolutionary in the 1980s, but no longer today because many now better known languages have drawn on it for inspiration. If you know Python, for example, then Pharo won&amp;rsquo;t surprise you much beyond the obvious and important syntactical differences. On the plus side, that means it is not much effort to do a first project in Pharo when coming from a Python background. But it also means that there isn&amp;rsquo;t much to be gained from learning Pharo if you look at it as just another programming language. The really interesting part is not the language, but the user interface of Pharo the computing platform.&lt;/p&gt;

&lt;p&gt;Pharo belongs to a rare species of computing environments that I think is best described by the label &amp;ldquo;explorable&amp;rdquo;. All of Pharo is implemented in Pharo itself, and all the source code is there for you to inspect and modify. But it&amp;rsquo;s not just the code that is inspectable, it&amp;rsquo;s all the objects that exist in memory. You can, for example, evaluate &lt;code&gt;Array instanceCount&lt;/code&gt; to find out how many arrays exist at the moment (213464 when I tried). You can then obtain an arbitrarily chosen instance with &lt;code&gt;Array someInstance&lt;/code&gt; and open a graphical inspector using &lt;code&gt;Array someInstance inspect&lt;/code&gt;. You can also modify that array, without any idea of where it is used and for what, and thus wreak havoc with your system. For a more thorough approach to breaking Pharo, one of my favorites is &lt;code&gt;true become: false&lt;/code&gt;, which replaces &lt;code&gt;true&lt;/code&gt; by &lt;code&gt;false&lt;/code&gt; and vice versa everywhere in the system. Pharo reacts much like I&amp;rsquo;d expect a human logician to react: it freezes instantly.&lt;/p&gt;

&lt;p&gt;The complete state of a Pharo system, including all code and all objects, and thus even GUI elements such as open windows, can be saved with a click in what is called an image. This is obviously very convenient, but should not be used as the only strategy for storing code because images are fragile, as my example above illustrates. Consider an image your development environment rather than your code repository. In fact, Pharo supports and encourages storing code in Git repositories.&lt;/p&gt;

&lt;p&gt;It is important to understand that explorability is not an accidental feature of Pharo (and other Smalltalk derivates), but has been a design goal from the start. Those interested in the history of this idea should look at &lt;a href="https://en.wikipedia.org/wiki/Dynabook"&gt;Alan Kay&amp;rsquo;s Dynabook concept&lt;/a&gt; and then take another step back in history to &lt;a href="http://thedemo.org/"&gt;Doug Englebart&amp;rsquo;s &amp;ldquo;Mother of all Demos&amp;rdquo;&lt;/a&gt;. The motivation behind all these developments is to make computing a tool not for performing tasks, but for augmenting human intellectual abilities. That goal is, unfortunately, very rare. In fact, the only other system I know of that was designed to be explorable is &lt;a href="http://www.emacs.org/"&gt;Emacs&lt;/a&gt;, also with the goal of maximally empowering users. Once you look beyond superficialities, Pharo and Emacs are actually quite similar. Both are built around a high-level programming language with a rich library, a user-interface framework, and development tools with inspection capabilities. Emacs then comes with a text editor as the default application at startup. Pharo has no such default application, meaning that it is pretty useless before you write some code of your own. That is probably the main reason why Emacs became so much more popular - people use it as a text editor and only later, if ever, discover its empowering features.&lt;/p&gt;

&lt;p&gt;Explorability is what interests me most in Pharo, because I believe that computational science sorely needs it, and that existing interactive interfaces such as REPLs or notebooks are far from sufficient. They impose a linear thread of exploration, whereas I want to be able to go off on a tangent, dig in deeper into a model, compare two datasets side-by-side, etc. Notebooks are also rigid exploration environments which can be extended only with major effort, if at all. Pharo offers a much richer exploration environment, and makes it easy to adapt to problem-specific needs (another reference to the &lt;a href="https://gtoolkit.com/"&gt;Glamorous Toolkit&lt;/a&gt; is compulsory here). The snag is that Pharo doesn&amp;rsquo;t offer much support for working with scientific data or scientific models (though I must admit that I haven&amp;rsquo;t checked out &lt;a href="https://github.com/PolyMathOrg/PolyMath"&gt;PolyMath&lt;/a&gt; yet). There are people who use Pharo for computational science (see e.g. &lt;a href="https://github.com/UMMISCO/kendrick"&gt;this epidemiology simulation platform&lt;/a&gt;), so I suppose that there are useful tools I simply haven&amp;rsquo;t looked at yet.&lt;/p&gt;

&lt;p&gt;One power tool that I have already discovered (and explored interactively in Pharo) is the visualization library &lt;a href="http://agilevisualization.com/"&gt;Roassal&lt;/a&gt;. It may superficially resemble various visualization libraries for JavaScript, but the big difference is that it integrates with the Pharo development and exploration tools. It is very easy to add a visualization pane to Pharo&amp;rsquo;s object inspector and get a graphical view on your objects in addition to the standard browser-type interface for accessing an object&amp;rsquo;s internals. And that means that you can easily use visualization as a tool in designing, implementing, and debugging code. It also helps a lot that the visualizations are themselves interactive. You can make them react to clicks, drags, and other events, and thus turn them into a user interface to your classes. For those familiar with Jupyter notebooks, it&amp;rsquo;s as if you could implement interactive widgets in a few lines of Python code stored in your notebook.&lt;/p&gt;

&lt;p&gt;I should perhaps say something about Pharo as a software development environment, but that aspect has been covered before by others in much more depth than I would do it myself. The demos in the Pharo MOOC are a good introduction, but for an overview of the possibilities, nothing beats &lt;a href="https://www.youtube.com/watch?v=baxtyeFVn3w"&gt;Aditya Siram&amp;rsquo;s recent demo&lt;/a&gt; aimed at adepts of functional programming languages.&lt;/p&gt;

&lt;p&gt;After all that praise, I have to add some caveats. First of all, the Pharo community is tiny compared to, say, Python&amp;rsquo;s, and therefore the choice in domain-specific libraries is rather small. Next, Pharo development moves on at a rapid pace, with the main consequence that nearly all available documentation is outdated, and what&amp;rsquo;s left is often an update for insiders rather than an introduction for newcomers. No matter how explorable a system is, you need some higher-level information to use it productively, if only to know the jargon that permits you to start searching for stuff. As an example, when I tried to figure out how package dependency management works, I had to ask on the &lt;a href="http://lists.pharo.org/mailman/listinfo/pharo-users_lists.pharo.org"&gt;Pharo user mailing list&lt;/a&gt; to learn that the keyword to look for is &amp;ldquo;baseline&amp;rdquo;. The three books &lt;a href="http://books.pharo.org/updated-pharo-by-example/"&gt;Pharo by Example&lt;/a&gt;, &lt;a href="http://deepintopharo.com/"&gt;Deep into Pharo&lt;/a&gt;, and &lt;a href="http://books.pharo.org/enterprise-pharo/"&gt;Enterprise Pharo&lt;/a&gt; are probably the best place to start looking for introductory essays, but even they are two versions behind the current one.&lt;/p&gt;

&lt;p&gt;Finally, let me anticipate a reaction that I expect regular readers of this blog to have. How is it possible for someone who underlines the importance of reproducibility in every second post to say something positive about a system that relies on persistent state to the point that it cannot even be bootstrapped from its own source code? There are a couple of replies. Most importantly, reproducibility is not what I am looking for in Pharo. Every system has its good and bad sides, and I am turning to Pharo for its good sides, explorability and user interfaces. Second, the Pharo developers are working on this. And finally, decades of dealing with persistent yet fragile system images have lead the Smalltalk community to figure out ways to cope with the resulting problems (e.g. changesets) that may be worth studying for inspiration. Computational science suffers from a fundamental tension between the short-term need for interactivity and the long-term need for reproducibility. So far, no one has found a satisfying answer, so it&amp;rsquo;s worth looking for inspiration in unusual places.&lt;/p&gt;</description></item>
  <item>
   <title>Knowledge distillation in computer-aided research</title>
   <link>http://blog.khinsen.net/posts/2018/10/21/knowledge-distillation-in-computer-aided-research/?utm_source=computational-science&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2018-10-21-knowledge-distillation-in-computer-aided-research</guid>
   <pubDate>Sun, 21 Oct 2018 16:28:39 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;There is an important and ubiquitous process in scientific research that scientists never seem to talk about. There isn&amp;rsquo;t even a word for it, as far as I now, so I&amp;rsquo;ll introduce my own: I&amp;rsquo;ll call it &lt;em&gt;knowledge distillation&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In today&amp;rsquo;s scientific practice, there are two main variants of this process, one for individual research studies and one for managing the collective knowledge of a discipline. I&amp;rsquo;ll briefly present both of them, before coming to the main point of this post, which is the integration of &lt;em&gt;digital&lt;/em&gt; knowledge, and in particular software, into the knowledge distillation process.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;The first variant is performed by individual researchers or closely collaborating teams who, starting from the raw information of their lab notebooks, describing methods applied and results obtained, write a journal article summarizing all of this information into an illustrated narrative that is much easier to digest for their fellow scientists. This narrative contains what the authors consider the essence of their work, leaving out what they consider technical details. Moreover, the narrative places the work into its wider scientific context. In a second step, the authors condense the article into an even smaller abstract, supposed to tell readers at a glance if the article is of interest to them without going into any details. This process can be illustrated as a pyramid:&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="./knowledge-pyramid-1.svg" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;At the bottom we have all the gory details, one level up the distilled version for communication, and at the top the minimal summary for first contact with a potential reader. It is not uncommon to have an additional layer between the bottom two, often published as &amp;ldquo;supplementary material&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Whereas authors work from the bottom to the top of this pyramid, readers work down from the top, gaining a more detailed understanding at each step. Until not so long ago, this was a two-step process: after the abstract, they could move on to the paper, but after that they had to contact the authors for obtaining more details, and the authors might well not care to reply. The Open Science movement has made some progress in pushing for more transparency by making deeper information layers available for critical inspection, in particular raw datasets and the source code for the software used to process them. The situation is very much in flux as various scientific disciplines are working out which information can and should be shared, and how. The maximal level of openness is known as &lt;a href="https://en.wikipedia.org/wiki/Open-notebook_science"&gt;Open Notebook science&lt;/a&gt;, which basically means making the whole pyramid public. Note, however, that giving access to the base of pyramid does not make the knowledge distillation steps superfluous. Readers would succumb to information overload if exposed to all the details without a proper introduction in the form of distilled knowledge. In fact, &lt;em&gt;most&lt;/em&gt; readers don&amp;rsquo;t want to anything else than the distilled version.&lt;/p&gt;

&lt;p&gt;The second variant of knowledge distillation is performed collectively by domain experts who summarize the literature of their field into review articles and then into monographs or textbooks for students. The pyramid diagram is very similar to the first variant&amp;rsquo;s:&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="./knowledge-pyramid-2.svg" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;It&amp;rsquo;s really just the same process at another scale: knowledge transfer about a discipline, rather than about a specific study.&lt;/p&gt;

&lt;p&gt;So far for good old science - let&amp;rsquo;s move to the digital age. The base of our first pyramid now contains code and digital datasets. Some of the code was written by the authors of the study for this specific project and typically takes the form of scripts, workflows, or notebooks. This is complemented by the dependencies of this project-specific code - see my &lt;a href="http://blog.khinsen.net/posts/2017/01/13/sustainable-software-and-reproducible-research-dealing-with-software-collapse/"&gt;post on software collapse&lt;/a&gt; for an analysis of the full software stack. Full openness requires making all of this public, with computational reproducibility serving as a success indicator. If other researchers can re-run the software and get the same results, they possess all the information one could possibly ask for, from a computational point of view.&lt;/p&gt;

&lt;p&gt;But as with Open Notebook science, making all the details open is not sufficient. Readers will again succumb to information overload when exposed to a complex software stack and digital datasets whose precise role in the study is not clear. Information overload is even a much more serious problem with software because the amount of detail that software source code contains is orders of magnitude bigger than what can be written down in a lab notebook.&lt;/p&gt;

&lt;p&gt;So how do we distill the scientific knowledge embedded in software? The bad news is that we don&amp;rsquo;t yet have any good techniques. What we find in journal articles when it comes to describing computational methods is very brief summaries in plain English, closer to the abstract level than to the journal article level. As a consequence, computational methods remain impenetrable to the reader who does not have prior experience with the software that has been applied. There is no way to work down the pyramid, readers have to acquire the base level skills on their own. Worse, there is no way to stop at the middle level of the pyramid and yet have a clear understanding of what is going on.&lt;/p&gt;

&lt;p&gt;The recent years have seen a flurry of research and development concerning the publication of software and computations. One main focus has been the reproducibility of results, another the sustainability of scientific software development, and a third one the readability of computational analyses. This last focus has most notably led to the development of computational notebooks (such as Jupyter, Rmarkdown, Emacs/Org-mode and many more), which embed code and results in a narrative providing context and explanations. Notebooks are occasionally put forward as &amp;ldquo;the paper of the future&amp;rdquo;, but in view of the knowledge pyramid, that&amp;rsquo;s not what they are. They are closer to the digital age equivalent of lab notebooks, especially when combined with version control to capture the time evolution of their contents. The real paper of the future must contain a &lt;em&gt;distilled&lt;/em&gt; version of the source code.&lt;/p&gt;

&lt;p&gt;It is interesting to examine why notebooks have been so successful in some scientific domains. First of all, they are a much better human-readable presentation of source code than anything we had before, with the exception of the related idea of literate programming which I expect to see a come-back as well. Next, in domains where computational studies tend to be linear sequences of well-known standard operations, such as statistical analyses, the notebook is very similar to a distilled computational protocol, because the technical details are mostly hidden in libraries. These libraries also contain significant scientific knowledge, but because these methods are well-known, they have in a way been distilled in the form of textbooks.&lt;/p&gt;

&lt;p&gt;More generally, though, notebooks contain both too little and too much information to qualify as distilled descriptions of computational studies. Too little because much scientific knowledge is hidden in the notebook&amp;rsquo;s dependencies, which are not documented at the same level of readability (which is why I believe that literate programming has a future). Too much because they still expose technical details to the reader that is more a hindrance than a help for understanding.&lt;/p&gt;

&lt;p&gt;How, then, should the paper of the future present distilled computational knowledge? I see three main requirements:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;It must be possible to explain and discuss individual models, approximations, or algorithms without the constraints of an efficient working implementation.&lt;/li&gt;
 &lt;li&gt;These models, approximations, and algorithms must be presented in a sufficiently precise form that automatic verification procedures can ensure that the source code at the base level of the pyramid actually implements them.&lt;/li&gt;
 &lt;li&gt;Suitable user interfaces must allow a reader to explore these models, approximations, and algorithms through concrete examples.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;The first requirement says that clarity of exposition must take absolute precedence over any technical considerations of software technology. The intrinsic complexity of computational methods makes understanding hard enough, so everything possible must be done to keep accidental complexity out of the way.&lt;/p&gt;

&lt;p&gt;The second requirement ensures that the conformity between the distilled and the detailed representations of a computational protocol can be verified by computers rather than by humans. Humans aren&amp;rsquo;t very good at checking that two complex artifacts are equivalent.&lt;/p&gt;

&lt;p&gt;The third requirement is motivated by the observation that a real understanding of a computational method, which is usually too lengthy to be actually performed manually, requires both reading code and observing how it processes simple test cases. Observation is not limited to the final outcome, it may well be necessary to provide access to intermediate results.&lt;/p&gt;

&lt;p&gt;To get an idea of what &amp;ldquo;suitable user interfaces&amp;rdquo; might look like, it&amp;rsquo;s worth looking at the &lt;a href="https://explorabl.es/"&gt;explorable explanations&lt;/a&gt; and the &lt;a href="http://www.complexity-explorables.org/"&gt;Complexity Explorables&lt;/a&gt; Web sites. Note, however, that none of these exploration user interfaces provide easy access to a precise formulation of the underlying models or algorithm. They exist in the form of JavaScript source code embedded in the Web site, but that&amp;rsquo;s not exactly a reader-friendly medium of expression. Another interesting line of development is happening in the &lt;a href="https://pharo.org/"&gt;Pharo&lt;/a&gt; community (Pharo being a modern descendent of Smalltalk), e.g. the idea of &lt;a href="http://scg.unibe.ch/research/moldableinspector"&gt;moldable inspectors&lt;/a&gt;, which are user interfaces specifically designed to explore a particular kind of object, which in the O-O tradition combines code and data.&lt;/p&gt;

&lt;p&gt;Back to requirements 1 and 2: we want a precise and easily inspectable description that can be embedded into an explanatory narrative. We also want to be sure that it actually corresponds to what the user interface lets us explore, and to what the software implementation applies efficiently to real-world problems. I am not aware of any existing technology that can fulfill this role, although there many that were designed with somewhat different goals in mind that can serve as guidelines, in particular the various &lt;a href="https://en.wikipedia.org/wiki/Modeling_language"&gt;modeling&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Specification_language"&gt;specification languages&lt;/a&gt;.&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="./knowledge-pyramid-3.svg" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;My own research into this problem had led to the concept of &lt;a href="http://sjscience.org/article?id=527"&gt;digital scientific notations&lt;/a&gt;, and I am currently designing such a notation for physics and chemistry, called &lt;a href="https://github.com/khinsen/leibniz"&gt;Leibniz&lt;/a&gt;. A &lt;a href="https://peerj.com/articles/cs-158/"&gt;first report&lt;/a&gt; on this research has been published earlier this year. Leibniz is mainly inspired by traditional mathematical notation concerning the way it is embedded into a narrative, and from specification languages in terms of semantics. Some relevant features of Leibniz for expressing distilled knowledge are&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;
  &lt;p&gt;Its highly declarative nature. Leibniz code consists of short declarations that can be written down in (nearly) arbitrary order, making them easy to embed into a narrative, much like mathematical expressions and equations.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;Its foundation in term rewriting (the same foundation adopted by most computer algebra systems). Among other advantages, this allows Leibniz code to concentrate on one aspect of a model or algorithm while leaving other aspects unspecified.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;Its restriction to a single universal (but often inefficient) data structure.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;These features mainly address requirement 1. As for requirement 2, Leibniz uses XML for its syntax and has very simple semantics, making it easy to write libraries that read and execute Leibniz code which in turn make it easy to integrate Leibniz into scientific software of all kinds. Only Leibniz development environments have to deal with the more complex user-facing syntax requiring a specific parser.&lt;/p&gt;

&lt;p&gt;Leibniz does not try to address requirement 3, but since it meets requirement 2, it doesn&amp;rsquo;t get in the way of people wishing to build exploration and inspection user interfaces for Leibniz-based models and algorithms.&lt;/p&gt;

&lt;p&gt;Leibniz is still very much experimental, and I am not at all sure that it will turn out to be useful in its current form. In fact, I am almost certain that it will require modification to be of practical use. If that doesn&amp;rsquo;t scare you off, have a look at the &lt;a href="http://khinsen.net/leibniz-examples/"&gt;example collection&lt;/a&gt; to get an idea of what Leibniz can do and what it looks like. Feedback of any kind is more than welcome!&lt;/p&gt;</description></item>
  <item>
   <title>Literate computational science</title>
   <link>http://blog.khinsen.net/posts/2018/07/26/literate-computational-science/?utm_source=computational-science&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2018-07-26-literate-computational-science</guid>
   <pubDate>Thu, 26 Jul 2018 14:44:31 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;Since the dawn of computer programming, software developers have been aware of the rapidly growing complexity of code as its size increases. Keeping in mind all the details in a few hundred lines of code is not trivial, and understanding someone else&amp;rsquo;s code is even more difficult because many higher-level decisions about algorithms and data structures are not visible unless the authors have carefully documented them and keep those comments up to date.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;The main angle of attack to keep software source code manageable has been the development of ever more sophisticated programming languages and development paradigms, but it is not the only one. Another approach was initiated by Donald Knuth&amp;rsquo;s invention of &lt;a href="http://literateprogramming.com/"&gt;literate programming&lt;/a&gt;. Its basic idea is to invert the roles of code and documentation. Rather than adding doxumentation as annotations to the code, literate programming puts an explanatory narrative about the software at the center of the software author&amp;rsquo;s attention. Code snippets are embedded into this narrative, much like mathematical formulas are embedded into scientific articles and textbooks.&lt;/p&gt;

&lt;p&gt;Literate programming never gained much popularity, for reasons that, to the best of my knowledge, have never been explored systematically. Insufficient tool support is often cited as an obstacle, but I suspect that the mismatch between the structure of the narrative and the language-imposed structure of the code is equally problematic. Programmers need to name code blocks and then assemble them into valid source code by hand. My own experience is that it&amp;rsquo;s usually easier to write and test the code first and then re-create it as a literate program, but this doesn&amp;rsquo;t lead to code that naturally fits the narrative.&lt;/p&gt;

&lt;p&gt;The main argument in support of this suspicion is the much higher popularity of a variant of literate programming that both adds and removes features compared to Knuth&amp;rsquo;s original system. Computational notebooks (implemented e.g. by &lt;a href="https://jupyter.org/"&gt;Jupyter&lt;/a&gt;) document a computation rather than a piece of software. In addition to code, they embed input data and results into the narrative, but they also restrict code to a linear assembly of code cells executed in sequence. This limitation removes the need to name and assemble code blocks.&lt;/p&gt;

&lt;p&gt;An idea I have been exploring recently is to take another step towards letting the explanatory narrative take center stage, by designing a formal language specifically for embedding into such a narrative. However, my language called &lt;a href="https://github.com/khinsen/leibniz"&gt;Leibniz&lt;/a&gt; is not a programming language. I call it a digital scientific notation to emphasize its intended use in the documentation of scientific models and methods, but in terms of computer science terminology it is a &lt;a href="https://en.wikipedia.org/wiki/Specification_language"&gt;specification language&lt;/a&gt; designed for models expressed in terms of equations and algorithms. Leibniz code &lt;em&gt;must&lt;/em&gt; be embedded into a narrative, although the Leibniz authoring environment also extracts a machine-readable version as an XML file for easy processing by scientific software.&lt;/p&gt;

&lt;p&gt;For getting an overview of Leibniz, I suggest to look first at a &lt;a href="http://khinsen.net/leibniz-examples/examples/leibniz-by-example.html"&gt;simple example&lt;/a&gt;, and then read my &lt;a href="https://peerj.com/articles/cs-158/"&gt;paper&lt;/a&gt; describing Leibniz and the problems it is designed to solve, which just appeared in PeerJ CompSci (Open Access like all of PeerJ). The explanations in the paper should prepare you for a look at the currently &lt;a href="http://khinsen.net/leibniz-examples/examples/mass-on-a-spring.html"&gt;most extensive example&lt;/a&gt;, which documents, for a toy problem, the full path of assumptions and approximations that lead from a theoretical framework (Newton&amp;rsquo;s equations of motion) to a numerical algorithm, with all models along the way being machine-readable.&lt;/p&gt;

&lt;p&gt;As the paper explains, Leibniz is best described as a research prototype at the current stage. It has known limitations that make its application to complex real-world problems a bit challenging. However, I am confident that these limitations can be overcome, and that Leibniz will be suitable for a wide range of scientific models and methods, starting with mathematical equations and ending with literate workflows. As Silicon Valley startups would say, make sure you won&amp;rsquo;t be left behind by the Leibniz revolution!&lt;/p&gt;</description></item>
  <item>
   <title>Scientific software is different from lab equipment</title>
   <link>http://blog.khinsen.net/posts/2018/05/07/scientific-software-is-different-from-lab-equipment/?utm_source=computational-science&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2018-05-07-scientific-software-is-different-from-lab-equipment</guid>
   <pubDate>Mon, 07 May 2018 07:40:35 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;My most recent paper submission (&lt;a href="https://peerj.com/preprints/26633/"&gt;preprint&lt;/a&gt; available) is about improving the verifiability of computer-aided research, and contains many references to the related subject of reproducibility. A reviewer asked the same question about all these references: isn&amp;rsquo;t this the same as for experiments done with lab equipment? Is software worse? I think the answers are of general interest, so here they are.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;First of all, an inevitable remark about terminology, which is still far from standardized (see &lt;a href="https://arxiv.org/abs/1802.03311"&gt;this preprint&lt;/a&gt; and &lt;a href="https://doi.org/10.3389%2Ffninf.2017.00076"&gt;this article&lt;/a&gt; for two recent contributions to the controversy). I will use the term &amp;ldquo;computational reproducibility&amp;rdquo; in its historically first sense introduced by Claerbout in 1992, because it seems to me that this is currently the dominant usage. &lt;em&gt;Reproducing&lt;/em&gt; a computation thus means running the same software on the same data, though it&amp;rsquo;s usually done by a different person using a different computer. In contrast, &lt;em&gt;replication&lt;/em&gt; refers to solving the same problem using different software. This terminological subtlety matters for the following discussion, because experimental reproducibility is actually more similar to replicability, rather than reproducibility, in the computational case.&lt;/p&gt;

&lt;p&gt;There are two aspects in which I think scientific software differs significantly from lab equipment:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Its characteristics as a human-made artifact&lt;/li&gt;
 &lt;li&gt;Its role in the process of doing science.&lt;/li&gt;&lt;/ol&gt;

&lt;h2 id="software-is-more-complex-and-less-robust-than-lab-equipment"&gt;Software is more complex and less robust than lab equipment&lt;/h2&gt;

&lt;p&gt;The first point I raised in my paper is the epistemic opacity of automated computation. Quote:&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;The overarching issue is that performing a computation by hand, step by step, on concrete data, yields a level of understanding and awareness of potential pitfalls that cannot be achieved by reasoning more abstractly about algorithms. As one moves up the ladder of abstraction from manual computation via writing code from scratch, writing code that relies on libraries, and running code written by others, to having code run by a graduate student, more and more aspects of the computation fade from a researcher&amp;rsquo;s attention. While a certain level of epistemic opacity is inevitable if we want to delegate computations to a machine, there are also many sources of accidental epistemic opacity that can and should be eliminated in order to make scientific results as understandable as possible.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The reviewer asks: isn&amp;rsquo;t this the same as when doing experiments using lab equipment constructed by somebody else? My answer is no.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s do a little thought experiment, introducing Alice and Bob as virtual guinea pigs. Alice is an experienced microscopist, Bob is an experienced computational scientist. We give Alice a microscope she hasn&amp;rsquo;t seen before, and ask her to evaluate if it is suitable for her research. We give Bob a simulation program (with source code and documentation) that he hasn&amp;rsquo;t seen before, and ask him the same question.&lt;/p&gt;

&lt;p&gt;My expectation is that Alice will go off an do some tests with samples that she knows well, and perhaps do some measurements on the microscope. After that, she will tell us for which aspects of her work she can use this new microscope. Meanwhile, Bob will be scratching his head while trying to figure out how to deal with our question.&lt;/p&gt;

&lt;p&gt;One reason for the difference is that a microscope is a much simpler artifact than a simulation program. While it is certainly difficult to design and produce a good microscope, from a user&amp;rsquo;s perspective its characteristics can be described by a handful of parameters, and its quality can be evaluated by a series of test observations. Software, on the contrary, can do almost anything. A typical simulation program has lots of options, whose precise meaning isn&amp;rsquo;t always obvious from its documentation. More importantly, no two simulation programs have identical options. Even the most experienced user of simulation software A falls back to near-novice status when given simulation software B.&lt;/p&gt;

&lt;p&gt;A more subtle difference is that microscopes, and lab equipment in general, are designed to be robust against small production defects and small variations of environmental conditions. Such small variations cause only small changes in the generated images. With software, on the other hands, all bets are off. A one-character mistake in the source code can cause the program to crash, but also to produce arbitrarily different numbers. In fact, there is no notion of similarity and thus of small variations for software. For a more detailed discussion, see my &lt;a href="http://doi.ieeecomputersociety.org/10.1109/MCSE.2016.67"&gt;CiSE article&lt;/a&gt; on this topic. This is why you can evaluate the quality of a microscope using a few judiciously chosen samples, whereas no amount of test runs can assure you that a piece of software is free of bugs. Unless you can afford to test &lt;em&gt;all possible&lt;/em&gt; inputs, of course, but then you don&amp;rsquo;t really need the software.&lt;/p&gt;

&lt;p&gt;These two differences explain why Alice knows how to evaluate the microscope, whereas Bob doesn&amp;rsquo;t know where to start. He might look at the documentation and the test cases to see if the program is meant to be used for the kind of work he does. But the documentation almost certainly lacks some important details of the approximations that are made in the code and that matter for Bob&amp;rsquo;s work. Moreover, he would still have to check that the software has no serious bugs related to the functionality he plans to use. Without knowing the implemented algorithms in detail, he cannot even anticipate what bugs to watch out for.&lt;/p&gt;

&lt;p&gt;Bob could also choose a very different approach and judge the software by quality standards from software engineering. Is the code well structured? Does it have unit and integration tests? These are the criteria that software journal ask their reviewers to evaluate (e.g. the &lt;a href="http://dx.doi.org/10.6084/m9.figshare.795303"&gt;Journal of Open Research Software&lt;/a&gt; or the &lt;a href="http://joss.theoj.org/about#reviewer_guidelines"&gt;Journal of Open Source Software&lt;/a&gt;). Statistically, they are probably related to the risk of encountering bugs (if anyone knows about research into this question, please leave a comment!). But even the most meticulous developers make mistakes, and, more importantly, may have different applications in mind than those that Bob cares about.&lt;/p&gt;

&lt;p&gt;Finally, Bob could do what in my experience (and also according to &lt;a href="https://arxiv.org/abs/1605.02265v1"&gt;this study&lt;/a&gt; ) most scientists do in choosing research software: they use what their colleagues use. Bob would then send a few emails asking if anyone he knows uses this software and is happy with it. This is a reasonable approach if you can assume that your colleagues, or at least a sizable fraction of them, are in a better position to judge the suitability of a piece of software than yourself. But if everyone adopts this approach, it becomes a popularity contest with little intrinsic value (see &lt;a href="https://doi.org/10.1126%2Fscience.1231535"&gt;this paper&lt;/a&gt; for a detailed example). In any case, it is not a way to actually answer our question.&lt;/p&gt;

&lt;p&gt;In the end, if you really want to know if your software does what you expect it to do, you have to go through every line of the source code until you understand what it does. You are then at the minimal level of epistemic opacity that you can attain without actually doing the computations by hand. Unfortunately, in the case of complex wide-spectrum software, this is likely to be much more effort than writing your own special-purpose software.&lt;/p&gt;

&lt;p&gt;The solution I propose in my paper is to use human-readable formal specifications as a form of documentation that is rigorous and complete, and can be used as a reference to verify the software against. The idea is to have a statement of the implemented algorithms that is precise and complete but as simple as possible, without being encumbered by considerations such as performance. Note that I don&amp;rsquo;t know if this will turn out to be possible - my work is merely a first step into that direction that, to the best of my knowledge, has not been explored until now.&lt;/p&gt;

&lt;h2 id="software-is-about-models-lab-equipment-is-about-observations"&gt;Software is about models, lab equipment is about observations&lt;/h2&gt;

&lt;p&gt;A popular meme in explaining science describes it as founded on two pillars, experiment and theory. Some people propose to add computation and/or simulation as a third pillar, and data mining as a fourth, although these additions remain controversial. In my opinion, they are misguided by a bad identification of the initial pillars. They are not experiment and theory, but observations and models. We often speak of computational experiments when doing simulations, and there are good reasons for the analogy, but it is important to keep in mind that these are experiments on models, not on natural phenomena.&lt;/p&gt;

&lt;p&gt;Observations provide us with information about nature, and models allow us to organize and generalize this information. In this picture, computation has two roles: evaluating the consequences of a model, and comparing them to observations. Simulation is an example for the first role, data mining for the second. Both of these roles predate electronic computers, they simply received more modest labels such as &amp;ldquo;solving differential equations&amp;rdquo; or &amp;ldquo;fitting parameters&amp;rdquo; in the past.&lt;/p&gt;

&lt;p&gt;In the context of reproducibility and verifiability, it is important to realize that there is no symmetry between these two pillars. Nature is the big unknown that we probe through observations. To do this, we use lab equipment that can never be perfect, for two reasons: first, it is constructed on the basis of our imperfect understanding of nature, and second, our control of matter is limited, so we cannot produce equipment that behaves precisely as we imagine it. Models, on the other hand, are symbolic artifacts that are under our precise control. We can formulate and communicate them without any ambiguity, if only we are careful enough.&lt;/p&gt;

&lt;p&gt;Because of these very different roles of observations and models, computational reproducibility has no analogue in the universe of observations. It is almost exclusively a communication issue, the one exception being the non-determinism in parallel computing that we accept in exchange for getting results faster. Non-determinism aside, if Alice cannot reproduce Bob&amp;rsquo;s computations, that simply means that Bob has not been able or willing to describe his work in enough detail for Alice to re-do it identically. There is no fundamental obstacle to such a description, because models and software are symbolic artifacts. We actually know how to achieve computational reproducibility, but we still need to make it straightforward in practice.&lt;/p&gt;

&lt;p&gt;Similarly, if Alice cannot verify that Bob&amp;rsquo;s computation solves the problem he claims them to solve, this means that Bob has not succeeded in explaining his work clearly enough for Alice to understand what is going on. An unverifiable computation is thus very similar to a badly written article. The big difference in practice is that centuries of experience with writing have lead to accepted and documented standards of good writing style, whereas after a few decades of scientific computing, we still do not know how to expose complex algorithms to human readers in the most understandable way. My paper is a first small step towards developing appropriate techniques.&lt;/p&gt;

&lt;p&gt;Experimental reproducibility, on the other hand, is an ideal that can never be achieved perfectly, because no two setups are strictly the same. Verifiability is equally limited because observations can never be repeated identically, even when done with the same equipment. Reproducibility is a quality attribute much like accuracy, precision, or cost. Tradeoffs between these attributes are inevitable, and have to be made by each scientific discipline as a function of what its main obstacles to progress are.&lt;/p&gt;

&lt;p&gt;Science has been adjusting to the inevitable limits of observations since its beginnings, whereas the issue of incomplete model descriptions has come up only with the introduction of computers permitting to work with complex models. We don&amp;rsquo;t know how yet if non-verifiable models are a real problem or not. However, as a theoretician I am not comfortable with the current situation. Models can be simple or complex, good or bad, grounded in solid theory or ad-hoc, but they should not be fuzzy. In particular not for complex systems, where it is very hard to foresee the consequences of minor changes.&lt;/p&gt;</description></item>
  <item>
   <title>What can we do to check scientific computation more effectively?</title>
   <link>http://blog.khinsen.net/posts/2018/03/07/what-can-we-do-to-check-scientific-computation-more-effectively/?utm_source=computational-science&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2018-03-07-what-can-we-do-to-check-scientific-computation-more-effectively</guid>
   <pubDate>Wed, 07 Mar 2018 17:15:50 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;It is widely recognized by now that software is an important ingredient to modern scientific research. If we want to check that published results are valid, and if we want to build on our colleagues&amp;rsquo; published work, we must have access to the software and data that were used in the computations. The latest high-impact statement along these lines is a &lt;a href="https://www.nature.com/articles/d41586-018-02741-4"&gt;Nature editorial&lt;/a&gt; that argues that with any manuscript submission, authors should also submit the data &lt;em&gt;and&lt;/em&gt; the software for review. I am all for that, and I hope that more journals will follow.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;However, we must also be aware of the inherent limitations of simply including software in peer review. With the exception of small and focused software, of the kind we typically have in replications submitted to &lt;a href="http://rescience.github.io/"&gt;ReScience&lt;/a&gt; (one of the very few scientific journals that actually does code review), the task of evaluating scientific software is so enormous that asking a single person to do it within two weeks is simply unreasonable. For that reason, journals specialized in software papers, such as the &lt;a href="https://openresearchsoftware.metajnl.com/"&gt;Journal of Open Research Software&lt;/a&gt; or the &lt;a href="https://joss.theoj.org/"&gt;Journal of Open Source Software&lt;/a&gt;, limit the reviewing process to more easily verifiable formal aspects, such as the presence of documentation and the use of appropriate software engineering techniques. Which is, of course, much better than nothing, but it isn&amp;rsquo;t enough.&lt;/p&gt;

&lt;p&gt;A few months ago I wrote about the &lt;a href="http://blog.khinsen.net/posts/2017/05/04/which-mistakes-do-we-actually-make-in-scientific-code/"&gt;kinds of mistakes that we tend to make in scientific computing&lt;/a&gt;. In my experience (I&amp;rsquo;d love to see a systematic study on this), most mistakes are due to discrepancies between what a paper describes and what is actually computed. This covers simple mistakes such as a wrong sign in a computed formula (such as in the widely publicized case of &lt;a href="http://doi.org/10.1126/science.314.5807.1856"&gt;protein structure retractions&lt;/a&gt;), or a typo in the input parameter file for a simulation program, but also more complex situations such as the &lt;a href="http://doi.org/10.1073/pnas.1602413113"&gt;inflated false-positive rates in fMRI studies&lt;/a&gt; that also made it into the headlines of science news. In this case, the fundamental issue was a mismatch between the methods implemented in the software and the methods that would have been appropriate for many typical use cases of the software. Put differently, the users of the software did not fully understand what exactly the software did. They trusted the software authors blindly to do &amp;ldquo;the right thing&amp;rdquo;, whatever that was. And they were probably reinforced in their blind trust by the fact that many of their colleagues used the same software. It&amp;rsquo;s the research version of &amp;ldquo;nobody ever got fired for buying IBM equipment&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Code review is an important step to a better verification of scientific computations, but in the cases I just described its utility is very limited. Neither the wrong sign in the protein crystallography code nor the not-quite-universally-applicable statistical analysis method used by the fMRI software would be detectable by software engineering methods. In the first case, the code would have to be compared to the set of mathematical formulas on which it was based, a task requiring expert knowledge in both crystallography and programming, plus a lot of time - much more than what a reviewer can typically invest. In the second case, code review cannot do anything at all. Only the reviewers of the application papers could have spotted the inappropriateness of the methods - but why should they be expected to be more knowledgeable about the pitfalls than the authors?&lt;/p&gt;

&lt;p&gt;An important but not yet widely recognized aspect of these situations is that today&amp;rsquo;s scientific software incorporates a significant amount of scientific knowledge that is very difficult to access and verify by users and reviewers. The translation of mathematical equations in a paper into efficient computer code is almost a form of encryption from the point of view of scientific knowledge transformation. Extracting equations from software source code is not much easier than extracting source code from compiled binaries.&lt;/p&gt;

&lt;p&gt;But can we do anything about this? I believe we can, but it will require a serious rethinking of the way we use computers to do research. My first explorations in this direction are described in a paper that is now available as a &lt;a href="https://peerj.com/preprints/26633/?td=bl"&gt;PeerJ preprint&lt;/a&gt;. Please have a look, and don&amp;rsquo;t hesitate to ask a question or leave other feedback of any kind!&lt;/p&gt;</description></item>
  <item>
   <title>Composition is the root of all evil</title>
   <link>http://blog.khinsen.net/posts/2016/03/04/composition-is-the-root-of-all-evil/?utm_source=computational-science&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2016-03-04-composition-is-the-root-of-all-evil</guid>
   <pubDate>Fri, 04 Mar 2016 10:14:03 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;Think of all the things you hate about using computers in doing research. Software installation. Getting your colleagues&amp;rsquo; scripts to work on your machine. System updates that break your computational code. The multitude of file formats and the eternal need for conversion. That great library that&amp;rsquo;s unfortunately written in the wrong language for you. Dependency and provenance tracking. Irreproducible computations. They all have something in common: they are consequences of the difficulty of composing digital information. In the following, I will explain the root causes of these problem. That won&amp;rsquo;t make them go away, but understanding the issues will perhaps help you to deal with them more efficiently, and to avoid them as much as possible in the future.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;Composing information is something we all do every day, mostly without thinking of it. A shopping list is the composition of names of things you need to buy. An e-mail message is the composition of the recipients&amp;rsquo; addresses, a subject line, and the body of the message. An address book is a composition of addresses, which in turn are compositions of various pieces of information related to some person.&lt;/p&gt;

&lt;p&gt;Science has its own information items and associated compositions. Measurements are composed into tables. Mathematical equations are composed into more complex equations. Datasets are composed to make a database. Hypotheses are composed to make a model.&lt;/p&gt;

&lt;p&gt;Writing computer programs means composing expressions and statements into procedures or functions, composing procedures to make modules, and composing modules to make programs. Reading data from a file means composing your algorithms with the data they work on into a complete computation. Configuring a new computer and installing software are about composing an operating system, various libraries, and application software into a functioning whole.&lt;/p&gt;

&lt;p&gt;When you look at these examples more closely, you might notice that some of these acts of composition are so trivial that we don&amp;rsquo;t even think about them, whereas others are a real pain. In that second category, we find most of the composition work related to computers. So what is the difference?&lt;/p&gt;

&lt;h3 id="human-and-computational-information-processing"&gt;Human and computational information processing&lt;/h3&gt;

&lt;p&gt;Humans process information in terms of concepts. We all have accumulated a vast amount of conceptual knowledge over our lifetime, starting with the most basic concepts that we learned in infancy. This knowledge includes the definitions of all the concepts, but also the relations between them. Our knowledge of concepts helps us to &amp;ldquo;make sense&amp;rdquo; of information, which includes the detection of probable mistakes and sometimes even their correction. Humans are very tolerant to mistakes and variations in how some piece of information is expressed. We don&amp;rsquo;t care if the items in a shopping list are arranged vertically or horizontally, for example.&lt;/p&gt;

&lt;p&gt;When composing information, we read the individual items, translate them into concepts, and then write out the composition. I use the vocabulary of processing written language here, but the same holds for oral or visual communication. Variations in notation may be an inconvenience, but not a real problem. As long as the information refers to familiar concepts, we can deal with it.&lt;/p&gt;

&lt;p&gt;Computers process information by applying precise mechanical rules. They don&amp;rsquo;t care about concepts, nor about context. If you ask a computer to do something stupid, it will happily do so. This may look like a criticism of how computers work, but it&amp;rsquo;s also exactly why they are so useful in research: they have different strengths and weaknesses compared to humans, and are therefore complementary &amp;ldquo;partners&amp;rdquo; in solving problems.&lt;/p&gt;

&lt;h3 id="formal-languages"&gt;Formal languages&lt;/h3&gt;

&lt;p&gt;At the hardware level of a digital computer, a computation is a multi-step process that transforms an input bit sequence into an output bit sequence under the control of a program that is stored as a bit sequence as well. Information processing by computers thus requires all data to be expressed as bit sequences. Dealing with bit sequences is, however, very inconvenient for humans. We therefore use data representations that are more suitable for human brains, but still exactly convertible from and to the bit sequences that are stored in a computer&amp;rsquo;s memory. These representations are called &lt;a href="http://en.wikipedia.org/wiki/Template:Formal_languages_and_grammars"&gt;&lt;em&gt;formal languages&lt;/em&gt;&lt;/a&gt;. The definition of a formal language specifies precisely how some piece of information is encoded as sequences of bits. Many formal languages are defined in terms of sequences of text characters instead of sequences of bits, for another level of human convenience. Since the mapping from text characters to bits is straightforward, this makes little difference in practice. The term &amp;ldquo;formal language&amp;rdquo; is commonly used in computer science, but in computational science we usually speak of &amp;ldquo;data formats&amp;rdquo;, &amp;ldquo;file formats&amp;rdquo;, and &amp;ldquo;programming languages&amp;rdquo;, all of which are specific kinds of formal languages. The use of formal languages, rather the the informal languages of human communication, is the defining characteristic of digital information.&lt;/p&gt;

&lt;p&gt;The definition of a formal language consists of two parts, syntax and semantics. Syntax defines which bit patterns or text strings are valid data items in the language. Syntax rules can be verified by a suitable program called a parser. Semantics define the &lt;em&gt;meaning&lt;/em&gt; of syntactically correct data items. With one important exception, semantics are mere conventions for the interpretation of digital data. Meaning refers to conceptual knowledge that a computer neither has nor needs: all it does is process bit sequences. The exception concerns formal languages for expressing algorithms, i.e. rules for the transformation of data. The semantics of an algorithmic language defines how each operation transforms input data into output data. Writing down such transformation rules obviously requires a notation for the data is being worked on. For that reason, a formal language that can express algorithms also defines the syntax and semantics of the input and output data for these algorithms. Your favorite programming language, whichever it is, provides a good illustration.&lt;/p&gt;

&lt;p&gt;There is a huge number of formal languages today, which can be organized into a hierarchy of abstraction layers, such that languages at a higher level incorporate languages from lower levels. As a simple example, a programming language such as Fortran incorporates formal languages defining individual data elements - integers, floating-point numbers, etc. At the lowest level of this hierarchy, close to the bit level at which computing hardware operates, we have formal languages such as &lt;a href="http://unicode.org/"&gt;Unicode&lt;/a&gt; for text characters or the floating-point number formats of &lt;a href="http://dx.doi.org/10.1109%2FIEEESTD.2008.4610935"&gt;IEEE standard 754&lt;/a&gt;. One level up we find the memory layout of Fortran arrays, the layout of &lt;a href="https://en.wikipedia.org/wiki/UTF-8"&gt;UTF&amp;ndash;8&lt;/a&gt; encoded text files, and many other basic data structures and file formats. Structured file formats such as XML or HDF5 are defined on the next higher level, as they incorporate basic data structures such as arrays or text strings. Programming languages such as Python or C reside on that level as well.&lt;/p&gt;

&lt;p&gt;Different formal languages that encode the same information at the semantic level can be converted into each other. The two best-known translations of this kind in the daily life of a computational scientist are file-format conversion and the compilation of software source code into processor instructions. However, if you take into account that the in-memory data layout of any program is a formal language as well, all I/O operations can be considered conversions between two formal languages.&lt;/p&gt;

&lt;h3 id="composition-of-digital-information"&gt;Composition of digital information&lt;/h3&gt;

&lt;p&gt;Digital information is, by definition, information expressed in a formal language. Composition of digital information produces a new, more complex, digital information item, which is of course expressed in a formal language as well. And since the ingredients remain accessible as parts of the whole, everything must be expressed in one and the same formal language. And that&amp;rsquo;s where all our trouble comes from.&lt;/p&gt;

&lt;p&gt;If we start from ingredients expressed in different languages, we have basically two options: translate everything to a common language, or define a new formal superlanguage that incorporates all the languages used for expressing the various ingredients. We can of course choose a mixture of these two extreme approaches. But both of them imply a lot of overhead and add considerable complexity to the composed assembly. Translation requires either tedious and error-prone manual labor, or writing a program to do the job. Defining a superlanguage requires implementing software tools for processing this new superlanguage.&lt;/p&gt;

&lt;p&gt;As an illustration, consider a frequent situation in computational science: a data processing program that reads a specific file format, and a dataset stored in a different format. The translation option means writing a file format converter. The superlanguage option means extending the data processing program to read a second file format. In both cases, the use of multiple formal languages adds complexity to the composition that is unrelated to the real problem to be solved, which is the data analysis. In software engineering, this is known as "&lt;a href="https://en.wikipedia.org/wiki/No_Silver_Bullet"&gt;accidental complexity&lt;/a&gt;", as opposed to the &amp;ldquo;essential complexity&amp;rdquo; inherent in the problem.&lt;/p&gt;

&lt;p&gt;As a second example, consider writing a program that is supposed to call a procedure written in language A and another procedure written in language B. The translation option means writing a compiler from A to B or vice-versa. The superlanguage option means writing an interpreter or compiler that accepts both languages A and B. A mixed approach could use two compilers, one for A and one for B, that share a common target language. The latter solution seems easy at first sight, because compilers from A and B to processor instructions probably already exist. However, the target language of a compiler is not &amp;ldquo;processor instruction set&amp;rdquo; but &amp;ldquo;processor instruction set plus specific representations of data structures and conventions for memory management&amp;rdquo;. It is unlikely that two unrelated compilers for A and B are compatible at that level. Practice has shown that combining code written in different programming languages is always a source of trouble, except when using tools that were explicitly designed for implementing the superlanguage from the start.&lt;/p&gt;

&lt;p&gt;In the last paragraph, I have adopted a somewhat unusual point of view which I will continue to use in the following. We usually think of a language as something named and documented, such as C or Unicode. The point of view I adopt here is that the language in which a piece of digital information is expressed consists of all the rules and constraints that must be satisfied, &lt;em&gt;including the rules and constraints due to composition&lt;/em&gt;. To illustrate the difference, consider the &lt;a href="https://www.python.org/"&gt;Python language&lt;/a&gt; and the Python language with the &lt;a href="http://www.numpy.org/"&gt;NumPy extension&lt;/a&gt;. According to the standard point of view, Python is the language and NumPy is a library written in Python. In my point of view, Python+NumPy is a language &lt;em&gt;different&lt;/em&gt; from plain Python. To see that libraries modify their underlying languages, consider the Python statement &lt;code&gt;import numpy&lt;/code&gt;. It fails in plain Python, so it is not a valid statement in the Python language, whereas it is valid in the Python+NumPy language. Moreover, in the Python+NumPy language you are not allowed to write a module called &lt;code&gt;numpy&lt;/code&gt;. The addition of NumPy to plain Python makes some formerly invalid programs valid and vice versa, which justifies speaking of different, though certainly similar, languages.&lt;/p&gt;

&lt;h3 id="lots-of-languages-lots-of-problems"&gt;Lots of languages, lots of problems&lt;/h3&gt;

&lt;p&gt;The above discussion suggests that to keep our lives simple, we should use as few different formal languages as possible. Unfortunately, an inventory of what we have to deal with shows that we are very far from that optimum.&lt;/p&gt;

&lt;p&gt;Data formats are the easiest part. Even the number of &amp;ldquo;standard&amp;rdquo; formats is enormous, and many of them aren&amp;rsquo;t that well standardized, leading to different dialects. Worse, many scientific programs make up their own &lt;em&gt;ad-hoc&lt;/em&gt; data formats that are scarcely documented. That&amp;rsquo;s why file conversion takes up so much of our time. Moreover, we usually have different on-disk and in-memory data formats for the same data, which is why we need to write I/O routines for our software.&lt;/p&gt;

&lt;p&gt;But the complexity of formal languages used to define programs completely dwarfs the complexity of data formats. Let&amp;rsquo;s start at the bottom level: the processor&amp;rsquo;s instruction set. If you write an operating system (OS), that&amp;rsquo;s the level you work at. Otherwise, your program is a plug-in to be composed with an operating system, and the operating system defines the formal language in which you need to provide your program. The &amp;ldquo;OS language&amp;rdquo; includes the processor&amp;rsquo;s instruction set, but also adds constraints (memory use, relocatability, &amp;hellip;) and access to OS functions. The OS language can be as simple as the &lt;a href="https://en.wikipedia.org/wiki/COM_file"&gt;COM file format&lt;/a&gt; from CP/M and DOS days but also as complex as Linux&amp;rsquo; &lt;a href="https://en.wikipedia.org/wiki/Executable_and_Linkable_Format"&gt;ELF format&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The ELF format introduces the next level of composition: object files and dynamic libraries, in addition to executable files. In a modern OS, a program is composed from several ingredients immediately before execution. The motivation for introducing this last-minute composition was the possibility to share frequently used program building blocks among the hundreds of processes running in parallel, thus reducing their memory footprint. But this comes at the price of considerable accidental complexity. The OS language that your program must be written in now includes note only the processor instruction set and the ELF format specification, but also conventions about where certain shared libraries are stored in the file system. That&amp;rsquo;s why it is no longer possible to prepare a generic program for the Linux platform. Different Linux distributions have different conventions for arranging the shared libraries in the file system, and moreover these conventions change over time. They have different OS languages.&lt;/p&gt;

&lt;p&gt;Upon closer inspection, the situation is actually even worse. The OS language for a given piece of software includes &lt;em&gt;all the software packages that have been installed on the same computer before&lt;/em&gt;. Obviously, only one software package can occupy a given filename. Once you have installed a package that uses the file &lt;code&gt;/usr/lib/libm.so&lt;/code&gt;, no other package can occupy the same slot. That makes it impossible to wrap up &amp;ldquo;my program and all the files it requires&amp;rdquo; for installation on some other machine. If package A contains &lt;code&gt;/usr/lib/libm.so&lt;/code&gt; and package B another &lt;code&gt;/usr/lib/libm.so&lt;/code&gt;, even if it is only a slightly older version of the same library, the two packages could not coexist. The only solution is to distribute programs and libraries as building blocks to be added to a growing assembly, whose composition - now called &amp;ldquo;software installation - is left to the system administrator. Each block comes with a list of "required dependencies&amp;rdquo;, whose presence the system administrator must ensure. Moreover, each block occupies certain slots that must be available in the system. In the terminology of formal languages, each new block must conform to a language that its author cannot know in advance, and cannot even fully describe. I have described this error-prone approach in an &lt;a href="http://www.activepapers.org/2014/01/31/Installing-Software.html"&gt;earlier blog post&lt;/a&gt; as the Tetris model of software installation, because of its obvious similarities with the well-known video game. It&amp;rsquo;s the most widely used model in scientific computing today.&lt;/p&gt;

&lt;p&gt;The obvious problems caused by this approach have motivated the development of various tools for the management of software installations. Some are specific to some OS platform (the package managers of Debian, RedHat, BSD, etc.). Others are specific to a programming language, e.g. Python&amp;rsquo;s &lt;code&gt;distutils&lt;/code&gt; system and its derivates. The multitude of software installation managers has created a secondary composition problem: to install a Python package on a Debian system, you must negotiate a compromise between Python&amp;rsquo;s and Debian&amp;rsquo;s views on how software installation should be managed.&lt;/p&gt;

&lt;p&gt;Another approach is to give up on sharing common resources, and provide some way to package programs with all the files they need into a single unit, even if this leads to duplication of data on disk and in memory. This is the idea behind MacOS X application bundles (which go back to &lt;a href="https://en.wikipedia.org/wiki/NeXTSTEP"&gt;NextSTEP&lt;/a&gt;) and also &lt;a href="https://www.docker.com/"&gt;Docker&lt;/a&gt; containers. Tools such as Python&amp;rsquo;s &lt;a href="https://pypi.python.org/pypi/virtualenv"&gt;&lt;code&gt;virtualenv&lt;/code&gt;&lt;/a&gt; proceed in a similar way, by isolating a specific composition of building blocks from other potentially conflicting compositions of building blocks on the same computer.&lt;/p&gt;

&lt;p&gt;An ingenious construction that combines the best of both worlds is the approach taken by the &lt;a href="http://nixos.org/"&gt;Nix&lt;/a&gt; package manager and its offshoot &lt;a href="https://www.gnu.org/software/guix/"&gt;Guix&lt;/a&gt;. Instead of having building blocks refer to each other through filenames, they use a hash code computed from the actual contents of the files. This allows the composition of arbitrary building blocks, including pairs that would claim the same filenames in a standard Linux system, but also prevents multiple identical copies of any building block. This idea is known as &lt;a href="https://en.wikipedia.org/wiki/Content-addressable_storage"&gt;content-addressable storage&lt;/a&gt;, and is also used in the popular version control system &lt;a href="https://git-scm.com/"&gt;git&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Up to here, I have described the composition of specific programs with an operating system. But the program that is prepared as a plug-in to an OS is itself already a composition. How it is composed and from which constituents depends on the programming language(s) being used and on the tools that implement them. In Python, for example, a program consists in general of packages which consist of modules which consist of name-value pairs. A C program consists of source code files and header files, which each contain value and function definitions and interact via macro definitions. Like in the case of the &amp;ldquo;OS language&amp;rdquo;, the precise formal language in which each piece is written is not just Python or C. It also includes constraints and extensions coming from other building blocks &amp;mdash; libraries &amp;mdash; that the program refers to, as I have illustrated above for the example of Python plus NumPy.&lt;/p&gt;

&lt;p&gt;Comparing these two situations, we can identify the common culprit: the use of a global namespace for composing building blocks. In the &amp;ldquo;OS language&amp;rdquo; of a typical Linux system, the global namespace is the filesystem. In Python, it&amp;rsquo;s the namespace of top-level module names. In C, it&amp;rsquo;s the namespace of non-static top-level definitions. Composition requires one building block to refer to another building block through a name in that namespace. And that in turn requires each building block to occupy a specific name in that namespace, so that others can refer to it.&lt;/p&gt;

&lt;p&gt;One way to alleviate this problem is encouraging the use of very specific names. That&amp;rsquo;s the approach taken by Java, whose global namespace for packages is supposed to contain &amp;ldquo;reversed domain names&amp;rdquo; such as &lt;code&gt;org.apache.commons.lang3.math&lt;/code&gt;. While such a rule, if respected, indeed reduces the risk of name collisions between unrelated packages to almost zero, the most frequent source of name collisions remains: different versions of a package have the same name and can therefore not be used together in a composition. When composing building blocks into a program, one can argue that mixing different versions is bad practice anyway. But in the Tetris model of a single global software collection per computer, not being able to have several versions of a building block is often a serious restriction.&lt;/p&gt;

&lt;p&gt;A final kind of formal language worth mentioning in this context is languages for defining compositions. This category includes Makefiles, Dockerfiles autoconf configuration files, and of course the package specification files of the various package managers. Their multitude shows the importance of the composition problem, but it also contributes to it. It is not rare to see a specification file for one package manager refer to another package manager. Conversion from one such language to another is nearly impossible, because the precise language for defining a composition depends not only on the package manager, but also on the other existing packages. It&amp;rsquo;s exactly the same situation as with the &amp;ldquo;OS language&amp;rdquo; and programming languages extended by libraries.&lt;/p&gt;

&lt;h3 id="is-there-a-way-out"&gt;Is there a way out?&lt;/h3&gt;

&lt;p&gt;I believe that there is, and I have some ideas about this, but I will leave them for another time as this post is already quite long. I hope that the above analysis contributes to a better understanding of the problems that computational scientists are facing in their daily work, which is the prerequisite to improving the situation.&lt;/p&gt;

&lt;p&gt;As a first step, I encourage everyone to prefer &lt;em&gt;solutions&lt;/em&gt; to &lt;em&gt;workarounds&lt;/em&gt; when faced with composition-related issues. Solutions identify a cause and eliminate it, whereas workarounds merely alleviate the impact of the problem, often re-creating the same problem at another level later on. In the approaches I have discussed above, an example of a solution is content-addressable storage, as used in Nix. In contrast, the traditional Linux package managers are workarounds, because they re-create a composition issue at the package level. Linux distribution authors have done a lot of hard and useful work with these package managers, which I don&amp;rsquo;t want to play down in any way. But the fruit of that work can be carried over to better foundations. The Tetris model of software installation is not sustainable in my opinion. We have to move on.&lt;/p&gt;</description></item>
  <item>
   <title>From facts to narratives</title>
   <link>http://blog.khinsen.net/posts/2015/12/08/from-facts-to-narratives/?utm_source=computational-science&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2015-12-08-from-facts-to-narratives</guid>
   <pubDate>Tue, 08 Dec 2015 14:40:00 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;A recurrent theme in computational science (and elsewhere) is the need to combine machine-readable information (which in the following I will call &amp;ldquo;facts&amp;rdquo; for simplicity) with a narrative for the benefit of human readers. The most obvious situation is a scientific publication, which is essentially a narrative explaining the context and motivation for a study, the work that was undertaken, the results that were observed, and conclusions drawn from these results. For a scientific study that made use of computation (which is almost all of today&amp;rsquo;s research work), the narrative refers to various computational facts, in particular machine-readable input data, program code, and computed results.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;A computational notebook, as pioneered by &lt;a href="https://www.wolfram.com/technology/nb/"&gt;Mathematica&lt;/a&gt; and recently popularized by &lt;a href="https://jupyter.org/"&gt;Jupyter&lt;/a&gt; (formerly known as the IPython notebook), is another document that mixes facts and narratives. Compared to a scientific article, program code takes a much more prominent role, and the narrative is focused on the computation. In software development tools, we find the fact-narrative mixture in version control, where the commits are a stream of facts to which the commit messages attach a narrative. At a more basic level, comments in program code can be thought of as narratives embedded into the code. &lt;a href="http://www.literateprogramming.com/"&gt;Literate programming&lt;/a&gt; inverts this relation by embedding the code into a narrative.&lt;/p&gt;

&lt;p&gt;All these situations share a common problem: the tools we have today force us to choose between treating the facts first-class, accepting a low-quality narrative, or to optimize the narrative while compromising on the quality of fact management. In the following, I will argue that this is due to a poorly thought-out relation between facts and narratives, and outline possible improvements.&lt;/p&gt;

&lt;p&gt;Comments in source code are an example where priority is given to the facts, i.e. the program. The reader is supposed to read the code, the comments are there only to provide non-obvious background information, and sometimes to outline an overall structure. Reading commented code takes a lot of time and effort, because the reader has to deal with all the details of the program code. A pure narrative would explain software at a more abstract level, leaving out details or relegating them to an appendix. As an example for the opposite extreme, a scientific article is primarily a narrative, including only small pieces of the facts for illustration. A complete description of the facts would require all of the program code and input data. This is why replicability and reproducibility are currently big issues in computational science.&lt;/p&gt;

&lt;p&gt;Facts and narratives live in two different universes. Facts belong to the computational universe, in which all information is encoded in formal languages with (ideally) well-defined syntax and semantics. Computation processes input data (which includes the program code) and produces output data in a process that is perfecly well-defined and deterministic. A real-life computation depends on a lot of input data due to the many details that matter. That means a lot of facts, but computers are very good at handling a lot of facts.&lt;/p&gt;

&lt;p&gt;Narratives belong to the universe of human thought and communication. They rely on a rich context that human readers are expected to have acquired through prior study. This context contains in particular the appropriate abstractions that allow the narrative to remain at a manageable level, because humans can only keep a limited amount of details in their heads. To see the importance of this point, imagine a narrative that explains how to &amp;ldquo;open a door&amp;rdquo; in terms of the detailed eye movements and muscle contractions required to perform this task - such a narrative would be completely incomprehensible. On the other hand, narratives do not need to be very precise in many aspects because humans excel at &amp;ldquo;making sense&amp;rdquo; of information even if it contains mistakes and incongruences.&lt;/p&gt;

&lt;p&gt;Computers are good at handling facts but not narratives. Humans are good at handling narratives but not facts in the quantities that typically define a computation. Letting computers intervene in the processing of narratives leads to funny results - try &lt;a href="https://translate.google.com/"&gt;Google Translate&lt;/a&gt; on a non-trivial text for an illustration. Letting humans intervene in the execution of a computation is a major source of mistakes. That is why a key ingredient to improving replicability is the automation of all computational steps. In the ideal world, no part of a computation would be defined by a narrative providing nstructions for a human operator. Anyone who has every had to install software knows that we are still far away from that ideal world.&lt;/p&gt;

&lt;p&gt;Note that I only said that humans should not intervene in the &lt;em&gt;execution&lt;/em&gt; of a computation. They do of course intervene in its definition. Program source code, after all, is written by humans. More generally, humans intervene quite often in computational science by using interactive tools. In that case, the stream of user interactions becomes part of the definition of a computation. If it is recorded, the computation can later be executed again without human intervention. This is of course well known: replicability requires that all user interaction must be recorded.&lt;/p&gt;

&lt;p&gt;Since facts and narratives live in different universes, we should avoid mixing them carelessly. Crossing the boundary between the two universes should always be explicit. A narrative should not include copies of pieces of facts, but references to locations in a fact universe. And facts should not refer to narratives at all. The relation between the two universes is not symmetric: computers are tools made by humans for their benefit, so the computational universe is subordinate to the human universe.&lt;/p&gt;

&lt;p&gt;Now let us look at the examples cited in the beginning from this new point of view. In scientific communication, the separation of facts and narratives was actually well respected initially. The lab notebook recorded facts, and the published paper contained a narrative quoting facts from the lab notebook. No scientist would ever have contemplated writing a paper by modifying the contents of his or her lab notebook! Unfortunately, this basic wisdom was lost with the adoption of computers. Computers make it very easy to modify information, to the point that version control had to be invented to prevent massive information loss by careless editing. Moreover, the distinction between a lab notebook and a paper became blurred by both being files processed using a computer. Finally, computational scientists never adopted the habit of keeping lab notebooks until very recently, coming mostly from a theoretical rather then an experimental background.&lt;/p&gt;

&lt;p&gt;Today there is a lot of discussion about &amp;ldquo;electronic lab notebooks&amp;rdquo;, but the fundamental characteristic of a lab notebook being a record of facts is not often mentioned in this context. Very frequently, computational notebooks as implemented by Jupyter or Mathematica are claimed to be lab notebooks for computational science. It is probably clear at this point that I do not agree. Computational notebooks are designed for writing narratives that include computations and their results. They are best considered specialized word processors that encourage refining a document through many iterations of modification involving the code, its results, and the textual elements. The computational side of notebooks is limited to efficient interactive code evaluation. There is no logging of interactions, and no description of the computational infrastructure (libraries, &amp;hellip;) on which the interactive computations rely. As a consequence, computations in a notebook are in general not replicable. I believe this can be fixed, and I have made &lt;a href="https://github.com/jupyter/enhancement-proposals/pull/4"&gt;a concrete proposal&lt;/a&gt; for doing so, but unfortunately I do not have the means to actually implement this idea.&lt;/p&gt;

&lt;p&gt;In version control as it was originally designed, a repository is a fact database that contains sequences of versions of file sets. Commit messages, like comments in a program, are small narratives that provide a high-level overview and often a motivation for each change. The role of a repository is similar to the role of a lab notebook: it is a permanent record of what happened, with narratives written close in time to the recorded events. As commits and commit messages accumulate over time, following along becomes an arduous task for a human reader: the narrative contains too much irrelevant detail. This became a serious practical issue as version control was adopted as a tool for collaboration, with members of a team communicating through commit messages. Git therefore introduced the approach of &lt;a href="https://git-scm.com/book/en/v2/Git-Tools-Rewriting-History"&gt;&amp;ldquo;rewriting history&amp;rdquo;&lt;/a&gt;. The idea is to &amp;ldquo;clean up&amp;rdquo; a stream of commits by re-ordering and merging them and by writing new commit messages, with the goal of creating a better narrative. Rewriting history remains a hot topic of debate. Most people realize the utility of cleaning up the narrative, but it also feels wrong to destroy the original historical record in the process. Moreover, there is a clear risk of introducing mistakes when rewriting history. In view of what I said above, the basic mistake is the failure to separate cleanly facts from narratives. The cleaned-up narrative should be separate from the original commented stream of commits and refer to it. In git terminology, rewriting history should create a new branch, and the rebasing operations done in deriving the new branch from the initial one should be recorded. Moreover, the editing tools should ensure that the final file contents are the same in the two branches.&lt;/p&gt;

&lt;p&gt;I hope that these two examples have illustrated why it is desirable to keep facts and narratives distinct, with well-defined references from narratives to facts. Unfortunately, today&amp;rsquo;s computational technology doesn&amp;rsquo;t help much with reaching this goal when the facts are parts of a complex computation. We cannot define such a computation while remaining completely in the computational universe. And we cannot define unambiguous references to arbitrary facts inside a computational universe either. Most of the data formats and tools we use for preparing narratives do not even try to respect the separation of universes. Finally, the formal languages we use to encode computational facts (programming languages, file formats, etc.) are mostly not designed for being embedded into narratives. There&amp;rsquo;s still a lot to do.&lt;/p&gt;</description></item>
  <item>
   <title>The lifecycle of digital scientific knowledge</title>
   <link>http://blog.khinsen.net/posts/2015/11/09/the-lifecycle-of-digital-scientific-knowledge/?utm_source=computational-science&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2015-11-09-the-lifecycle-of-digital-scientific-knowledge</guid>
   <pubDate>Mon, 09 Nov 2015 07:54:57 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;Like all information with a complex structure, scientific knowledge evolves over time. New ideas turn into validated models, and are ultimately integrated into a coherent body of knowledge defined by the concensus of a scientific community. In this essay, I explore how this process is affected by the ever increasing use of computers in scientific research. More precisely, I look at &amp;ldquo;digital scientific knowledge&amp;rdquo;, by which I mean scientific knowledge that is processed using computers. This includes both software and digital datasets. For simplicity, I will concentrate on software, but much of the reasoning applies to datasets as well, if only because the precise meaning of non-trivial datasets is often defined by the software that treats them.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;Before looking at the &amp;ldquo;digital&amp;rdquo; aspects, I will summarize the traditional lifecycle of scientific knowledge from the &amp;ldquo;printed page&amp;rdquo; era. It has been going on for centuries and follows well-established procedures and habits. I will then argue that these procedures should serve as a guideline for the management of digital scientific knowledge as well, and that computing technology for science should be designed to support this lifecycle.&lt;/p&gt;

&lt;p&gt;New observations, instruments, models, methods, and ideas are first published in journal articles. Such an article explains the background and motivation for the work, summarizes the state of the art, and then exposes the new elements that the authors wish to contribute to the scientific record. Other scientists from the field read the article, and draw conclusions for their own work, which are translated to citations to the article in their own publications. After some time, if the original publication creates enough interest, it will become a subject of discussion in its research community, and it will be mentioned in review articles, which place it in the context of other recent work in the field.&lt;/p&gt;

&lt;p&gt;Being cited in review articles is typically the last step in the lifecycle of an individual contribution. Its ideas and conclusions are then merged with related ideas and conclusions and reformulated to become part of the state of the art of the field, recorded in reference works, monographs, and textbooks. These works represent a kind of community concensus. New research, in the same or in other domains, builds on such concensus knowledge, often implicitly by assuming that every reader of a journal article is familiar with the contents of reference works, monographs, and textbooks.&lt;/p&gt;

&lt;p&gt;The introduction of computers into scientfic research has lead to many changes to this process. Some of them, such as the transition from paper to computer files as a support medium for scientific article and, reference works, are relatively minor. The most profound change is that an important part of digital scientific knowledge exists only in the form of software. This is true in particular for complex scientific models, for which we have no other convenient form of representation. An example where this situation is very explicit is the &lt;a href="https://www2.cesm.ucar.edu/"&gt;Community Earth System Model&lt;/a&gt; for climate research, which takes the form of a software package. Most often, the status of computational models is more fuzzy. As an example, consider force fields for proteins such as &lt;a href="https://en.wikipedia.org/wiki/AMBER"&gt;AMBER&lt;/a&gt; or &lt;a href="https://en.wikipedia.org/wiki/CHARMM"&gt;CHARMM&lt;/a&gt;. People refer to these force fields by citing scientific articles, but these articles contain only outlines of the models. Their only complete recorded expressions are implementations as part of simulation software packages, but unlike for the Community Earth System Model, there is no software package designed to function as a reference implementation defining the model.&lt;/p&gt;

&lt;p&gt;The fundamental difference between software and other media for storing scientific knowledge is that software has two sides: a human-facing side, and a machine-facing side. As a medium for expressing scientific knowledge, software fulfills the same role as prose or mathematical formulas. But the necessity of specifying a computation so precisely that a machine can execute it imposes severe constraints (software must be expressed using formal languages), and the desire to perform computations efficiently in a world of finite resources adds a different set of priorities in software development that are often in conflict with the criteria attached to the role of a medium for expressing ideas. As an illustration, the source code of a simulation program that has been heavily optimized for parallel execution combines 10% of scientific model with 90% of resource management and bookkeeping, making the scientific model not only hard to understand but even hard to find in the source code. For a more detailed discussion, see &lt;a href="http://f1000research.com/articles/3-101/v2"&gt;my article&lt;/a&gt; in F1000Research.&lt;/p&gt;

&lt;p&gt;Many of the problems that computational science is facing today (reliability, reproducibility, black-box mentality, etc.) can be traced back to an insufficient support for the lifecycle of scientific knowledge by today&amp;rsquo;s software development tools. Practically all of them were developed by and for software development communities outside of scientific research. As a consequence, these tools (programming languages, compilers, packaging and deployment tools, version control systems, etc.) do not take into account the specificities of scientific computing. Worse, computational scientists do nothing to improve the situation. The dominant attitude today is &amp;ldquo;scientists have to adopt best practices from software engineering and acquire the skills required to apply them&amp;rdquo;. What I advocate is a somewhat different point of view: scientists should adapt these practices and the tools that implement them to their specific needs.&lt;/p&gt;

&lt;p&gt;To see where the problems are, let&amp;rsquo;s look at the lifecycle of scientific knowledge expressed as software. New models and methods are developed by a mixture of thinking, tinkering, and exploring the consequences. This requires a representation that humans can understand and manipulate easily. Executability by a computer is a condition, but other machine-related criteria hardly matter at this stage. Once some useful contribution to the field has been identified, it is communicated to the research community, in a form that is easily understandable, but also easy to deploy on other people&amp;rsquo;s computers. This step is the equivalent of publishing a scientific paper. Next, other scientists start to play with the new stuff. This includes comparisons with other models and methods, analysis of model properties, application to different scenarios, etc. The conclusions from this work should take a form similar to a review article. This would be a toolkit in which different models and methods are made available for execution, with added annotations about their relative strengths and weaknesses. Finally, a synthesis of different ideas leads to a concensus implementation supported and maintained by a wider community of scientists, both as a basis for their own future work and as an infrastructure tool for other communities. This last step corresponds to reference works, and should be accompanied by tutorials that take the role of textbooks. At this stage, usability and performance become major criteria, whereas it is acceptable that not everyone can easily understand the implementation. Those who do wish to understand the method can go back to the &amp;ldquo;review paper&amp;rdquo; stage.&lt;/p&gt;

&lt;p&gt;Most of the discussion about scientific software today is focused on the last stage. It&amp;rsquo;s about community-supported software packages, whose sustained development requires significant efforts and investments. Most of this effort is required to keep the software useful in a world of rapidly changing computational environments, and to improve its human interfaces. A smaller part is dedicated to implementing new scientific models and methods. This effort has no equally important counterpart in the traditional lifecycle of scientific knowledge, and therefore the people who work on it find it hard to get recognition for their work. It is &amp;ldquo;not science&amp;rdquo; by the standards of the generation that occupies most leadership positions in research today. Fortunately, this attitude is starting to change.&lt;/p&gt;

&lt;p&gt;This focus on the last stage is perhaps also the reason for the dominating attitude that scientists should simply adopt best practices from software engineering. In fact, the development and maintenance of community software packages implementing concensus models and methods is technically close enough to software development in business and industry that the same tools and procedures can be applied. This is not true, however, for the the earlier stages in the lifecycle of digital scientific knowledge. As we will see, they are not well supported by today&amp;rsquo;s software development tools and practices. What&amp;rsquo;s worse is that most computational scientists accept this situation as inevitable.&lt;/p&gt;

&lt;p&gt;At the first stage, a scientist&amp;rsquo;s activity is better described by &amp;ldquo;manipulating and exploring models and methods&amp;rdquo; than by &amp;ldquo;software development&amp;rdquo;. Computational models are of course algorithms, and thus software, but this is almost a technical detail. What is more important is a clear view of the hypotheses and approximations that have lead to a specific model, and a trace of the scientific validation that has been performed (comparison with experimental data and with other models). Programming languages are not at all a good match for this kind of work, nor are software engineering approaches such as testing. In terms of software technology, a computational model is much closer to a specification than to a piece of software.&lt;/p&gt;

&lt;p&gt;For the next stage, the evaluation of a new idea in a narrow community of specialists, the technical requirements are somewhere in between the two neighboring stages. The manipulation of computational models loses some importance, whereas evaluation and comparison become more relevant. Interoperability matters a lot: even if the authors of two models chose different languages (corresponding to different scientific notations in the traditional scenario), a comparative evaluation should be a straightforward task. With programming languages, it clearly isn&amp;rsquo;t. The technical difficulties of making programs written in different languages talk to each other are effectively discouraging scientists from even trying. We would need tools such as &amp;ldquo;notational adapters&amp;rdquo; and, even more importantly, some low-level conventions for code and data that everybody can agree and build on. As a guideline for developing such technology, keep the analogy with review articles in mind. What would an executable review article about similar but independently developed computational methods look like? Which authoring tools are available to support such work?&lt;/p&gt;

&lt;p&gt;Finally, the transition from the first two stages to the last one is not as smooth as it ought to be. Quite often, an implementation written for convenient manipulation by humans must be completely rewritten in order to fit into a collection of optimized subroutines. What we should have is compiler-like tools that translate code from the first two stages into standard programming languages, using annotations added by expert programmers for guidance. The idea is to have a toolchain (1) guarantee the equivalence of the initial and the optimized level, and (2) keep track of additional approximations that were made for performance reasons. Moreover, community-supported optimized software libraries should be usable as infrastructure tools in the next level of model and method development, and thus be interoperable with the tools appropriate for the first stage, which are inexistent for now.&lt;/p&gt;

&lt;p&gt;Another way to describe this specificity of scientific computing, compared to other application domains, is the absence of a clear borderline between software developers and software users. Most scientists are users of tried and trusted computational methods while working on the development or validation of methods at another level. The only clear separation we have, conceptually, is the one between scientific models and methods on one hand and computing technology (in particular resource management) on the other hand. Unfortunately, that is exactly the separation that current software technology does not allow us to make.&lt;/p&gt;</description></item></channel></rss>