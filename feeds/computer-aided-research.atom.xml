<?xml version="1.0" encoding="utf-8"?> 
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
 <title type="text">Konrad Hinsen's Blog: Posts tagged 'computer-aided research'</title>
 <link rel="self" href="http://blog.khinsen.net/feeds/computer-aided-research.atom.xml" />
 <link href="http://blog.khinsen.net/tags/computer-aided-research.html" />
 <id>urn:http-blog-khinsen-net:-tags-computer-aided-research-html</id>
 <updated>2018-10-21T16:28:39Z</updated>
 <entry>
  <title type="text">Knowledge distillation in computer-aided research</title>
  <link rel="alternate" href="http://blog.khinsen.net/posts/2018/10/21/knowledge-distillation-in-computer-aided-research/?utm_source=computer-aided-research&amp;utm_medium=Atom" />
  <id>urn:http-blog-khinsen-net:-posts-2018-10-21-knowledge-distillation-in-computer-aided-research</id>
  <published>2018-10-21T16:28:39Z</published>
  <updated>2018-10-21T16:28:39Z</updated>
  <author>
   <name>Konrad Hinsen</name></author>
  <content type="html">
&lt;p&gt;There is an important and ubiquitous process in scientific research that scientists never seem to talk about. There isn&amp;rsquo;t even a word for it, as far as I now, so I&amp;rsquo;ll introduce my own: I&amp;rsquo;ll call it &lt;em&gt;knowledge distillation&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In today&amp;rsquo;s scientific practice, there are two main variants of this process, one for individual research studies and one for managing the collective knowledge of a discipline. I&amp;rsquo;ll briefly present both of them, before coming to the main point of this post, which is the integration of &lt;em&gt;digital&lt;/em&gt; knowledge, and in particular software, into the knowledge distillation process.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;The first variant is performed by individual researchers or closely collaborating teams who, starting from the raw information of their lab notebooks, describing methods applied and results obtained, write a journal article summarizing all of this information into an illustrated narrative that is much easier to digest for their fellow scientists. This narrative contains what the authors consider the essence of their work, leaving out what they consider technical details. Moreover, the narrative places the work into its wider scientific context. In a second step, the authors condense the article into an even smaller abstract, supposed to tell readers at a glance if the article is of interest to them without going into any details. This process can be illustrated as a pyramid:&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="./knowledge-pyramid-1.svg" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;At the bottom we have all the gory details, one level up the distilled version for communication, and at the top the minimal summary for first contact with a potential reader. It is not uncommon to have an additional layer between the bottom two, often published as &amp;ldquo;supplementary material&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Whereas authors work from the bottom to the top of this pyramid, readers work down from the top, gaining a more detailed understanding at each step. Until not so long ago, this was a two-step process: after the abstract, they could move on to the paper, but after that they had to contact the authors for obtaining more details, and the authors might well not care to reply. The Open Science movement has made some progress in pushing for more transparency by making deeper information layers available for critical inspection, in particular raw datasets and the source code for the software used to process them. The situation is very much in flux as various scientific disciplines are working out which information can and should be shared, and how. The maximal level of openness is known as &lt;a href="https://en.wikipedia.org/wiki/Open-notebook_science"&gt;Open Notebook science&lt;/a&gt;, which basically means making the whole pyramid public. Note, however, that giving access to the base of pyramid does not make the knowledge distillation steps superfluous. Readers would succumb to information overload if exposed to all the details without a proper introduction in the form of distilled knowledge. In fact, &lt;em&gt;most&lt;/em&gt; readers don&amp;rsquo;t want to anything else than the distilled version.&lt;/p&gt;

&lt;p&gt;The second variant of knowledge distillation is performed collectively by domain experts who summarize the literature of their field into review articles and then into monographs or textbooks for students. The pyramid diagram is very similar to the first variant&amp;rsquo;s:&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="./knowledge-pyramid-2.svg" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;It&amp;rsquo;s really just the same process at another scale: knowledge transfer about a discipline, rather than about a specific study.&lt;/p&gt;

&lt;p&gt;So far for good old science - let&amp;rsquo;s move to the digital age. The base of our first pyramid now contains code and digital datasets. Some of the code was written by the authors of the study for this specific project and typically takes the form of scripts, workflows, or notebooks. This is complemented by the dependencies of this project-specific code - see my &lt;a href="http://blog.khinsen.net/posts/2017/01/13/sustainable-software-and-reproducible-research-dealing-with-software-collapse/"&gt;post on software collapse&lt;/a&gt; for an analysis of the full software stack. Full openness requires making all of this public, with computational reproducibility serving as a success indicator. If other researchers can re-run the software and get the same results, they possess all the information one could possibly ask for, from a computational point of view.&lt;/p&gt;

&lt;p&gt;But as with Open Notebook science, making all the details open is not sufficient. Readers will again succumb to information overload when exposed to a complex software stack and digital datasets whose precise role in the study is not clear. Information overload is even a much more serious problem with software because the amount of detail that software source code contains is orders of magnitude bigger than what can be written down in a lab notebook.&lt;/p&gt;

&lt;p&gt;So how do we distill the scientific knowledge embedded in software? The bad news is that we don&amp;rsquo;t yet have any good techniques. What we find in journal articles when it comes to describing computational methods is very brief summaries in plain English, closer to the abstract level than to the journal article level. As a consequence, computational methods remain impenetrable to the reader who does not have prior experience with the software that has been applied. There is no way to work down the pyramid, readers have to acquire the base level skills on their own. Worse, there is no way to stop at the middle level of the pyramid and yet have a clear understanding of what is going on.&lt;/p&gt;

&lt;p&gt;The recent years have seen a flurry of research and development concerning the publication of software and computations. One main focus has been the reproducibility of results, another the sustainability of scientific software development, and a third one the readability of computational analyses. This last focus has most notably led to the development of computational notebooks (such as Jupyter, Rmarkdown, Emacs/Org-mode and many more), which embed code and results in a narrative providing context and explanations. Notebooks are occasionally put forward as &amp;ldquo;the paper of the future&amp;rdquo;, but in view of the knowledge pyramid, that&amp;rsquo;s not what they are. They are closer to the digital age equivalent of lab notebooks, especially when combined with version control to capture the time evolution of their contents. The real paper of the future must contain a &lt;em&gt;distilled&lt;/em&gt; version of the source code.&lt;/p&gt;

&lt;p&gt;It is interesting to examine why notebooks have been so successful in some scientific domains. First of all, they are a much better human-readable presentation of source code than anything we had before, with the exception of the related idea of literate programming which I expect to see a come-back as well. Next, in domains where computational studies tend to be linear sequences of well-known standard operations, such as statistical analyses, the notebook is very similar to a distilled computational protocol, because the technical details are mostly hidden in libraries. These libraries also contain significant scientific knowledge, but because these methods are well-known, they have in a way been distilled in the form of textbooks.&lt;/p&gt;

&lt;p&gt;More generally, though, notebooks contain both too little and too much information to qualify as distilled descriptions of computational studies. Too little because much scientific knowledge is hidden in the notebook&amp;rsquo;s dependencies, which are not documented at the same level of readability (which is why I believe that literate programming has a future). Too much because they still expose technical details to the reader that is more a hindrance than a help for understanding.&lt;/p&gt;

&lt;p&gt;How, then, should the paper of the future present distilled computational knowledge? I see three main requirements:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;It must be possible to explain and discuss individual models, approximations, or algorithms without the constraints of an efficient working implementation.&lt;/li&gt;
 &lt;li&gt;These models, approximations, and algorithms must be presented in a sufficiently precise form that automatic verification procedures can ensure that the source code at the base level of the pyramid actually implements them.&lt;/li&gt;
 &lt;li&gt;Suitable user interfaces must allow a reader to explore these models, approximations, and algorithms through concrete examples.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;The first requirement says that clarity of exposition must take absolute precedence over any technical considerations of software technology. The intrinsic complexity of computational methods makes understanding hard enough, so everything possible must be done to keep accidental complexity out of the way.&lt;/p&gt;

&lt;p&gt;The second requirement ensures that the conformity between the distilled and the detailed representations of a computational protocol can be verified by computers rather than by humans. Humans aren&amp;rsquo;t very good at checking that two complex artifacts are equivalent.&lt;/p&gt;

&lt;p&gt;The third requirement is motivated by the observation that a real understanding of a computational method, which is usually too lengthy to be actually performed manually, requires both reading code and observing how it processes simple test cases. Observation is not limited to the final outcome, it may well be necessary to provide access to intermediate results.&lt;/p&gt;

&lt;p&gt;To get an idea of what &amp;ldquo;suitable user interfaces&amp;rdquo; might look like, it&amp;rsquo;s worth looking at the &lt;a href="https://explorabl.es/"&gt;explorable explanations&lt;/a&gt; and the &lt;a href="http://www.complexity-explorables.org/"&gt;Complexity Explorables&lt;/a&gt; Web sites. Note, however, that none of these exploration user interfaces provide easy access to a precise formulation of the underlying models or algorithm. They exist in the form of JavaScript source code embedded in the Web site, but that&amp;rsquo;s not exactly a reader-friendly medium of expression. Another interesting line of development is happening in the &lt;a href="https://pharo.org/"&gt;Pharo&lt;/a&gt; community (Pharo being a modern descendent of Smalltalk), e.g. the idea of &lt;a href="http://scg.unibe.ch/research/moldableinspector"&gt;moldable inspectors&lt;/a&gt;, which are user interfaces specifically designed to explore a particular kind of object, which in the O-O tradition combines code and data.&lt;/p&gt;

&lt;p&gt;Back to requirements 1 and 2: we want a precise and easily inspectable description that can be embedded into an explanatory narrative. We also want to be sure that it actually corresponds to what the user interface lets us explore, and to what the software implementation applies efficiently to real-world problems. I am not aware of any existing technology that can fulfill this role, although there many that were designed with somewhat different goals in mind that can serve as guidelines, in particular the various &lt;a href="https://en.wikipedia.org/wiki/Modeling_language"&gt;modeling&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Specification_language"&gt;specification languages&lt;/a&gt;.&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="./knowledge-pyramid-3.svg" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;My own research into this problem had led to the concept of &lt;a href="http://sjscience.org/article?id=527"&gt;digital scientific notations&lt;/a&gt;, and I am currently designing such a notation for physics and chemistry, called &lt;a href="https://github.com/khinsen/leibniz"&gt;Leibniz&lt;/a&gt;. A &lt;a href="https://peerj.com/articles/cs-158/"&gt;first report&lt;/a&gt; on this research has been published earlier this year. Leibniz is mainly inspired by traditional mathematical notation concerning the way it is embedded into a narrative, and from specification languages in terms of semantics. Some relevant features of Leibniz for expressing distilled knowledge are&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;
  &lt;p&gt;Its highly declarative nature. Leibniz code consists of short declarations that can be written down in (nearly) arbitrary order, making them easy to embed into a narrative, much like mathematical expressions and equations.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;Its foundation in term rewriting (the same foundation adopted by most computer algebra systems). Among other advantages, this allows Leibniz code to concentrate on one aspect of a model or algorithm while leaving other aspects unspecified.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;Its restriction to a single universal (but often inefficient) data structure.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;These features mainly address requirement 1. As for requirement 2, Leibniz uses XML for its syntax and has very simple semantics, making it easy to write libraries that read and execute Leibniz code which in turn make it easy to integrate Leibniz into scientific software of all kinds. Only Leibniz development environments have to deal with the more complex user-facing syntax requiring a specific parser.&lt;/p&gt;

&lt;p&gt;Leibniz does not try to address requirement 3, but since it meets requirement 2, it doesn&amp;rsquo;t get in the way of people wishing to build exploration and inspection user interfaces for Leibniz-based models and algorithms.&lt;/p&gt;

&lt;p&gt;Leibniz is still very much experimental, and I am not at all sure that it will turn out to be useful in its current form. In fact, I am almost certain that it will require modification to be of practical use. If that doesn&amp;rsquo;t scare you off, have a look at the &lt;a href="http://khinsen.net/leibniz-examples/"&gt;example collection&lt;/a&gt; to get an idea of what Leibniz can do and what it looks like. Feedback of any kind is more than welcome!&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">Literate computational science</title>
  <link rel="alternate" href="http://blog.khinsen.net/posts/2018/07/26/literate-computational-science/?utm_source=computer-aided-research&amp;utm_medium=Atom" />
  <id>urn:http-blog-khinsen-net:-posts-2018-07-26-literate-computational-science</id>
  <published>2018-07-26T14:44:31Z</published>
  <updated>2018-07-26T14:44:31Z</updated>
  <author>
   <name>Konrad Hinsen</name></author>
  <content type="html">
&lt;p&gt;Since the dawn of computer programming, software developers have been aware of the rapidly growing complexity of code as its size increases. Keeping in mind all the details in a few hundred lines of code is not trivial, and understanding someone else&amp;rsquo;s code is even more difficult because many higher-level decisions about algorithms and data structures are not visible unless the authors have carefully documented them and keep those comments up to date.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;The main angle of attack to keep software source code manageable has been the development of ever more sophisticated programming languages and development paradigms, but it is not the only one. Another approach was initiated by Donald Knuth&amp;rsquo;s invention of &lt;a href="http://literateprogramming.com/"&gt;literate programming&lt;/a&gt;. Its basic idea is to invert the roles of code and documentation. Rather than adding doxumentation as annotations to the code, literate programming puts an explanatory narrative about the software at the center of the software author&amp;rsquo;s attention. Code snippets are embedded into this narrative, much like mathematical formulas are embedded into scientific articles and textbooks.&lt;/p&gt;

&lt;p&gt;Literate programming never gained much popularity, for reasons that, to the best of my knowledge, have never been explored systematically. Insufficient tool support is often cited as an obstacle, but I suspect that the mismatch between the structure of the narrative and the language-imposed structure of the code is equally problematic. Programmers need to name code blocks and then assemble them into valid source code by hand. My own experience is that it&amp;rsquo;s usually easier to write and test the code first and then re-create it as a literate program, but this doesn&amp;rsquo;t lead to code that naturally fits the narrative.&lt;/p&gt;

&lt;p&gt;The main argument in support of this suspicion is the much higher popularity of a variant of literate programming that both adds and removes features compared to Knuth&amp;rsquo;s original system. Computational notebooks (implemented e.g. by &lt;a href="https://jupyter.org/"&gt;Jupyter&lt;/a&gt;) document a computation rather than a piece of software. In addition to code, they embed input data and results into the narrative, but they also restrict code to a linear assembly of code cells executed in sequence. This limitation removes the need to name and assemble code blocks.&lt;/p&gt;

&lt;p&gt;An idea I have been exploring recently is to take another step towards letting the explanatory narrative take center stage, by designing a formal language specifically for embedding into such a narrative. However, my language called &lt;a href="https://github.com/khinsen/leibniz"&gt;Leibniz&lt;/a&gt; is not a programming language. I call it a digital scientific notation to emphasize its intended use in the documentation of scientific models and methods, but in terms of computer science terminology it is a &lt;a href="https://en.wikipedia.org/wiki/Specification_language"&gt;specification language&lt;/a&gt; designed for models expressed in terms of equations and algorithms. Leibniz code &lt;em&gt;must&lt;/em&gt; be embedded into a narrative, although the Leibniz authoring environment also extracts a machine-readable version as an XML file for easy processing by scientific software.&lt;/p&gt;

&lt;p&gt;For getting an overview of Leibniz, I suggest to look first at a &lt;a href="http://khinsen.net/leibniz-examples/examples/leibniz-by-example.html"&gt;simple example&lt;/a&gt;, and then read my &lt;a href="https://peerj.com/articles/cs-158/"&gt;paper&lt;/a&gt; describing Leibniz and the problems it is designed to solve, which just appeared in PeerJ CompSci (Open Access like all of PeerJ). The explanations in the paper should prepare you for a look at the currently &lt;a href="http://khinsen.net/leibniz-examples/examples/mass-on-a-spring.html"&gt;most extensive example&lt;/a&gt;, which documents, for a toy problem, the full path of assumptions and approximations that lead from a theoretical framework (Newton&amp;rsquo;s equations of motion) to a numerical algorithm, with all models along the way being machine-readable.&lt;/p&gt;

&lt;p&gt;As the paper explains, Leibniz is best described as a research prototype at the current stage. It has known limitations that make its application to complex real-world problems a bit challenging. However, I am confident that these limitations can be overcome, and that Leibniz will be suitable for a wide range of scientific models and methods, starting with mathematical equations and ending with literate workflows. As Silicon Valley startups would say, make sure you won&amp;rsquo;t be left behind by the Leibniz revolution!&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">Scientific software is different from lab equipment</title>
  <link rel="alternate" href="http://blog.khinsen.net/posts/2018/05/07/scientific-software-is-different-from-lab-equipment/?utm_source=computer-aided-research&amp;utm_medium=Atom" />
  <id>urn:http-blog-khinsen-net:-posts-2018-05-07-scientific-software-is-different-from-lab-equipment</id>
  <published>2018-05-07T07:40:35Z</published>
  <updated>2018-05-07T07:40:35Z</updated>
  <author>
   <name>Konrad Hinsen</name></author>
  <content type="html">
&lt;p&gt;My most recent paper submission (&lt;a href="https://peerj.com/preprints/26633/"&gt;preprint&lt;/a&gt; available) is about improving the verifiability of computer-aided research, and contains many references to the related subject of reproducibility. A reviewer asked the same question about all these references: isn&amp;rsquo;t this the same as for experiments done with lab equipment? Is software worse? I think the answers are of general interest, so here they are.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;First of all, an inevitable remark about terminology, which is still far from standardized (see &lt;a href="https://arxiv.org/abs/1802.03311"&gt;this preprint&lt;/a&gt; and &lt;a href="https://doi.org/10.3389%2Ffninf.2017.00076"&gt;this article&lt;/a&gt; for two recent contributions to the controversy). I will use the term &amp;ldquo;computational reproducibility&amp;rdquo; in its historically first sense introduced by Claerbout in 1992, because it seems to me that this is currently the dominant usage. &lt;em&gt;Reproducing&lt;/em&gt; a computation thus means running the same software on the same data, though it&amp;rsquo;s usually done by a different person using a different computer. In contrast, &lt;em&gt;replication&lt;/em&gt; refers to solving the same problem using different software. This terminological subtlety matters for the following discussion, because experimental reproducibility is actually more similar to replicability, rather than reproducibility, in the computational case.&lt;/p&gt;

&lt;p&gt;There are two aspects in which I think scientific software differs significantly from lab equipment:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Its characteristics as a human-made artifact&lt;/li&gt;
 &lt;li&gt;Its role in the process of doing science.&lt;/li&gt;&lt;/ol&gt;

&lt;h2 id="software-is-more-complex-and-less-robust-than-lab-equipment"&gt;Software is more complex and less robust than lab equipment&lt;/h2&gt;

&lt;p&gt;The first point I raised in my paper is the epistemic opacity of automated computation. Quote:&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;The overarching issue is that performing a computation by hand, step by step, on concrete data, yields a level of understanding and awareness of potential pitfalls that cannot be achieved by reasoning more abstractly about algorithms. As one moves up the ladder of abstraction from manual computation via writing code from scratch, writing code that relies on libraries, and running code written by others, to having code run by a graduate student, more and more aspects of the computation fade from a researcher&amp;rsquo;s attention. While a certain level of epistemic opacity is inevitable if we want to delegate computations to a machine, there are also many sources of accidental epistemic opacity that can and should be eliminated in order to make scientific results as understandable as possible.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The reviewer asks: isn&amp;rsquo;t this the same as when doing experiments using lab equipment constructed by somebody else? My answer is no.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s do a little thought experiment, introducing Alice and Bob as virtual guinea pigs. Alice is an experienced microscopist, Bob is an experienced computational scientist. We give Alice a microscope she hasn&amp;rsquo;t seen before, and ask her to evaluate if it is suitable for her research. We give Bob a simulation program (with source code and documentation) that he hasn&amp;rsquo;t seen before, and ask him the same question.&lt;/p&gt;

&lt;p&gt;My expectation is that Alice will go off an do some tests with samples that she knows well, and perhaps do some measurements on the microscope. After that, she will tell us for which aspects of her work she can use this new microscope. Meanwhile, Bob will be scratching his head while trying to figure out how to deal with our question.&lt;/p&gt;

&lt;p&gt;One reason for the difference is that a microscope is a much simpler artifact than a simulation program. While it is certainly difficult to design and produce a good microscope, from a user&amp;rsquo;s perspective its characteristics can be described by a handful of parameters, and its quality can be evaluated by a series of test observations. Software, on the contrary, can do almost anything. A typical simulation program has lots of options, whose precise meaning isn&amp;rsquo;t always obvious from its documentation. More importantly, no two simulation programs have identical options. Even the most experienced user of simulation software A falls back to near-novice status when given simulation software B.&lt;/p&gt;

&lt;p&gt;A more subtle difference is that microscopes, and lab equipment in general, are designed to be robust against small production defects and small variations of environmental conditions. Such small variations cause only small changes in the generated images. With software, on the other hands, all bets are off. A one-character mistake in the source code can cause the program to crash, but also to produce arbitrarily different numbers. In fact, there is no notion of similarity and thus of small variations for software. For a more detailed discussion, see my &lt;a href="http://doi.ieeecomputersociety.org/10.1109/MCSE.2016.67"&gt;CiSE article&lt;/a&gt; on this topic. This is why you can evaluate the quality of a microscope using a few judiciously chosen samples, whereas no amount of test runs can assure you that a piece of software is free of bugs. Unless you can afford to test &lt;em&gt;all possible&lt;/em&gt; inputs, of course, but then you don&amp;rsquo;t really need the software.&lt;/p&gt;

&lt;p&gt;These two differences explain why Alice knows how to evaluate the microscope, whereas Bob doesn&amp;rsquo;t know where to start. He might look at the documentation and the test cases to see if the program is meant to be used for the kind of work he does. But the documentation almost certainly lacks some important details of the approximations that are made in the code and that matter for Bob&amp;rsquo;s work. Moreover, he would still have to check that the software has no serious bugs related to the functionality he plans to use. Without knowing the implemented algorithms in detail, he cannot even anticipate what bugs to watch out for.&lt;/p&gt;

&lt;p&gt;Bob could also choose a very different approach and judge the software by quality standards from software engineering. Is the code well structured? Does it have unit and integration tests? These are the criteria that software journal ask their reviewers to evaluate (e.g. the &lt;a href="http://dx.doi.org/10.6084/m9.figshare.795303"&gt;Journal of Open Research Software&lt;/a&gt; or the &lt;a href="http://joss.theoj.org/about#reviewer_guidelines"&gt;Journal of Open Source Software&lt;/a&gt;). Statistically, they are probably related to the risk of encountering bugs (if anyone knows about research into this question, please leave a comment!). But even the most meticulous developers make mistakes, and, more importantly, may have different applications in mind than those that Bob cares about.&lt;/p&gt;

&lt;p&gt;Finally, Bob could do what in my experience (and also according to &lt;a href="https://arxiv.org/abs/1605.02265v1"&gt;this study&lt;/a&gt; ) most scientists do in choosing research software: they use what their colleagues use. Bob would then send a few emails asking if anyone he knows uses this software and is happy with it. This is a reasonable approach if you can assume that your colleagues, or at least a sizable fraction of them, are in a better position to judge the suitability of a piece of software than yourself. But if everyone adopts this approach, it becomes a popularity contest with little intrinsic value (see &lt;a href="https://doi.org/10.1126%2Fscience.1231535"&gt;this paper&lt;/a&gt; for a detailed example). In any case, it is not a way to actually answer our question.&lt;/p&gt;

&lt;p&gt;In the end, if you really want to know if your software does what you expect it to do, you have to go through every line of the source code until you understand what it does. You are then at the minimal level of epistemic opacity that you can attain without actually doing the computations by hand. Unfortunately, in the case of complex wide-spectrum software, this is likely to be much more effort than writing your own special-purpose software.&lt;/p&gt;

&lt;p&gt;The solution I propose in my paper is to use human-readable formal specifications as a form of documentation that is rigorous and complete, and can be used as a reference to verify the software against. The idea is to have a statement of the implemented algorithms that is precise and complete but as simple as possible, without being encumbered by considerations such as performance. Note that I don&amp;rsquo;t know if this will turn out to be possible - my work is merely a first step into that direction that, to the best of my knowledge, has not been explored until now.&lt;/p&gt;

&lt;h2 id="software-is-about-models-lab-equipment-is-about-observations"&gt;Software is about models, lab equipment is about observations&lt;/h2&gt;

&lt;p&gt;A popular meme in explaining science describes it as founded on two pillars, experiment and theory. Some people propose to add computation and/or simulation as a third pillar, and data mining as a fourth, although these additions remain controversial. In my opinion, they are misguided by a bad identification of the initial pillars. They are not experiment and theory, but observations and models. We often speak of computational experiments when doing simulations, and there are good reasons for the analogy, but it is important to keep in mind that these are experiments on models, not on natural phenomena.&lt;/p&gt;

&lt;p&gt;Observations provide us with information about nature, and models allow us to organize and generalize this information. In this picture, computation has two roles: evaluating the consequences of a model, and comparing them to observations. Simulation is an example for the first role, data mining for the second. Both of these roles predate electronic computers, they simply received more modest labels such as &amp;ldquo;solving differential equations&amp;rdquo; or &amp;ldquo;fitting parameters&amp;rdquo; in the past.&lt;/p&gt;

&lt;p&gt;In the context of reproducibility and verifiability, it is important to realize that there is no symmetry between these two pillars. Nature is the big unknown that we probe through observations. To do this, we use lab equipment that can never be perfect, for two reasons: first, it is constructed on the basis of our imperfect understanding of nature, and second, our control of matter is limited, so we cannot produce equipment that behaves precisely as we imagine it. Models, on the other hand, are symbolic artifacts that are under our precise control. We can formulate and communicate them without any ambiguity, if only we are careful enough.&lt;/p&gt;

&lt;p&gt;Because of these very different roles of observations and models, computational reproducibility has no analogue in the universe of observations. It is almost exclusively a communication issue, the one exception being the non-determinism in parallel computing that we accept in exchange for getting results faster. Non-determinism aside, if Alice cannot reproduce Bob&amp;rsquo;s computations, that simply means that Bob has not been able or willing to describe his work in enough detail for Alice to re-do it identically. There is no fundamental obstacle to such a description, because models and software are symbolic artifacts. We actually know how to achieve computational reproducibility, but we still need to make it straightforward in practice.&lt;/p&gt;

&lt;p&gt;Similarly, if Alice cannot verify that Bob&amp;rsquo;s computation solves the problem he claims them to solve, this means that Bob has not succeeded in explaining his work clearly enough for Alice to understand what is going on. An unverifiable computation is thus very similar to a badly written article. The big difference in practice is that centuries of experience with writing have lead to accepted and documented standards of good writing style, whereas after a few decades of scientific computing, we still do not know how to expose complex algorithms to human readers in the most understandable way. My paper is a first small step towards developing appropriate techniques.&lt;/p&gt;

&lt;p&gt;Experimental reproducibility, on the other hand, is an ideal that can never be achieved perfectly, because no two setups are strictly the same. Verifiability is equally limited because observations can never be repeated identically, even when done with the same equipment. Reproducibility is a quality attribute much like accuracy, precision, or cost. Tradeoffs between these attributes are inevitable, and have to be made by each scientific discipline as a function of what its main obstacles to progress are.&lt;/p&gt;

&lt;p&gt;Science has been adjusting to the inevitable limits of observations since its beginnings, whereas the issue of incomplete model descriptions has come up only with the introduction of computers permitting to work with complex models. We don&amp;rsquo;t know how yet if non-verifiable models are a real problem or not. However, as a theoretician I am not comfortable with the current situation. Models can be simple or complex, good or bad, grounded in solid theory or ad-hoc, but they should not be fuzzy. In particular not for complex systems, where it is very hard to foresee the consequences of minor changes.&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">Scientific communication is a research problem</title>
  <link rel="alternate" href="http://blog.khinsen.net/posts/2018/04/09/scientific-communication-is-a-research-problem/?utm_source=computer-aided-research&amp;utm_medium=Atom" />
  <id>urn:http-blog-khinsen-net:-posts-2018-04-09-scientific-communication-is-a-research-problem</id>
  <published>2018-04-09T14:33:24Z</published>
  <updated>2018-04-09T14:33:24Z</updated>
  <author>
   <name>Konrad Hinsen</name></author>
  <content type="html">
&lt;p&gt;A &lt;a href="https://www.theatlantic.com/amp/article/556676/"&gt;recent article in &amp;ldquo;The Atlantic&amp;rdquo;&lt;/a&gt; has been the subject of many comments in my Twittersphere. It&amp;rsquo;s about scientific communication in the age of computer-aided research, which requires communicating computations (i.e. code, data, and results) in addition to the traditional narrative of a paper. The article focuses on computational notebooks, a technology introduced in the late 1980s by &lt;a href="https://www.wolfram.com/mathematica/"&gt;Mathematica&lt;/a&gt; but which has become accessible to most researchers only since &lt;a href="http://jupyter.org/"&gt;Project Jupyter&lt;/a&gt; (formerly known as the IPython notebook) started to offer an open-source implementation supporting a wide range of programming languages. The gist of the article is that today&amp;rsquo;s practice of publishing science in PDF files is obsolete, and that notebooks are the future.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;One &lt;a href="https://twitter.com/khinsen/status/982339472036593672"&gt;interesting follow-up thread on Twitter&lt;/a&gt; explored if any scientific papers had actually been published in the form of Jupyter notebooks. It seems that the answer is no. Notebooks are published as supplementary material to standard papers, or as informal communication outside of the official scientific record, in particular for teaching purposes, but no one could point to a paper indexed in any article database that was written as a Jupyter notebook. As to the question of &lt;em&gt;why&lt;/em&gt; his hasn&amp;rsquo;t happened, all answers remain speculative in the absence of research into the subject. Publishers&amp;rsquo; format requirements are certainly a part of the problem, but limitations of today&amp;rsquo;s notebook format also matter. In particular, notebooks lack support for bibliographies and for cross referencing.&lt;/p&gt;

&lt;p&gt;Another interesting follow-up is &lt;a href="https://metarabbit.wordpress.com/2018/04/08/the-scientific-paper-of-the-future-is-probably-a-pdf/"&gt;a blog post by Luis Pedro Coelho&lt;/a&gt; who predicts that PDFs will stay with us for many years to come, because none of the proposed successors is actually mature enough for use in real life. In particular, he points out the complexity and lack of longevity and stability of most of today&amp;rsquo;s computational tools. My personal experience is very similar to his. He also asks the very relevant question if a notebook-style presentation of results and computations is actually a good idea in the context of a scientific paper. I suspect nobody can provide an evidence-based answer at this time.&lt;/p&gt;

&lt;p&gt;As these discussions illustrate, scientific communication about computer-aided research remains a research problem. As a community, we do not know how to explain, share, or review computer-aided research in a satisfactory way. Most of us agree that PDFs are no longer sufficient, and that we need to share code and data. However, we do not yet have good enough practices for doing so, at least not for all practically relevant situations. We do not know either if sharing code and data will actually be sufficient to enable effective communication. It is well possible that we will also need to develop practices for better &lt;em&gt;explaining&lt;/em&gt; computations to each other, and have them peer reviewed in some form.&lt;/p&gt;

&lt;p&gt;From this point of view, all of today&amp;rsquo;s technology, be it Jupyter, &lt;a href="https://orgmode.org/worg/org-contrib/babel/"&gt;Org mode&lt;/a&gt;, &lt;a href="https://yihui.name/knitr/"&gt;knitr&lt;/a&gt; or similar tools, should best be seen as support tools for performing experiments in scientific communication. What is still largely missing is systematic research that evaluates these experiments with the goal of summarizing the collective experience and drawing conclusions. There are promising starts, such as &lt;a href="https://hal.archives-ouvertes.fr/hal-01676633"&gt;this study on the actual use of Jupyter notebooks&lt;/a&gt;, but their number is negligible compared to the number of articles proclaiming that this or that technology is going to revolutionize scientific communication without providing any tangible evidence.&lt;/p&gt;

&lt;p&gt;I think it is time for the scientific community to acknowledge that it doesn&amp;rsquo;t really know how to communicate computer-aided research effectively, and encourage research into the question. Experimenting with the various proposed approaches is essential, but analyzing the outcomes of these experiments is essential as well. In my opinion, we currently over-emphasize tool development, community building, and teaching, which are all directed at &lt;em&gt;implementing&lt;/em&gt; new practices, but neglect research into what these practices actually should &lt;em&gt;be&lt;/em&gt;. Future generations of scientists may well remember today&amp;rsquo;s hot developments as sources of technical debt.&lt;/p&gt;

&lt;p&gt;A personal anecdote provide and illustration of the dominating attitude. My &lt;a href="http://www.activepapers.org/"&gt;ActivePapers&lt;/a&gt; project is clearly labeled as research. Its goal is to explore how non-trivial computations (long run times, big data sets) can be performed, archived, and published reproducibly. For first results, see &lt;a href="https://f1000research.com/articles/3-289/v3"&gt;this paper&lt;/a&gt;. Whenever I present this project, I know there is one question someone in the audience will ask: What are your plans for increasing your user base? I answer that I am doing research and not product development, and that I am not recruiting users but at best collaborators. This always causes surprise and sometimes animated discussions. It almost seems that doing research on doing research is a strange idea for professional scientists. On the other hand, my other research project on scientific communication, the digital scientific notation &lt;a href="https://github.com/khinsen/leibniz"&gt;Leibniz&lt;/a&gt;, does not generate this kind of reaction, but then it hasn&amp;rsquo;t see that much exposure yet. It explores the question of how we can explain a complex computation in a way that allows readers to verify its scientific assumptions. For a first account, see &lt;a href="https://peerj.com/preprints/26633/"&gt;this preprint&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Finally, readers might be interested in two of my earlier blog posts that are related to notebooks:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;a href="https://khinsen.wordpress.com/2015/09/03/beyond-jupyter-whats-in-a-notebook/"&gt;&amp;ldquo;Beyond Jupyter: whatâ€™s in a notebook?&amp;rdquo;&lt;/a&gt; looks at notebooks as digital documents, focusing on the information content rather than on the tool for doing computations.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;a href="http://blog.khinsen.net/posts/2015/12/08/from-facts-to-narratives/"&gt;&amp;ldquo;From facts to narratives&amp;rdquo;&lt;/a&gt; explores various approaches, one of them being notebooks, to combining formal elements of a computation (code, date) with a explanatory narrative.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;</content></entry>
 <entry>
  <title type="text">What can we do to check scientific computation more effectively?</title>
  <link rel="alternate" href="http://blog.khinsen.net/posts/2018/03/07/what-can-we-do-to-check-scientific-computation-more-effectively/?utm_source=computer-aided-research&amp;utm_medium=Atom" />
  <id>urn:http-blog-khinsen-net:-posts-2018-03-07-what-can-we-do-to-check-scientific-computation-more-effectively</id>
  <published>2018-03-07T17:15:50Z</published>
  <updated>2018-03-07T17:15:50Z</updated>
  <author>
   <name>Konrad Hinsen</name></author>
  <content type="html">
&lt;p&gt;It is widely recognized by now that software is an important ingredient to modern scientific research. If we want to check that published results are valid, and if we want to build on our colleagues&amp;rsquo; published work, we must have access to the software and data that were used in the computations. The latest high-impact statement along these lines is a &lt;a href="https://www.nature.com/articles/d41586-018-02741-4"&gt;Nature editorial&lt;/a&gt; that argues that with any manuscript submission, authors should also submit the data &lt;em&gt;and&lt;/em&gt; the software for review. I am all for that, and I hope that more journals will follow.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;However, we must also be aware of the inherent limitations of simply including software in peer review. With the exception of small and focused software, of the kind we typically have in replications submitted to &lt;a href="http://rescience.github.io/"&gt;ReScience&lt;/a&gt; (one of the very few scientific journals that actually does code review), the task of evaluating scientific software is so enormous that asking a single person to do it within two weeks is simply unreasonable. For that reason, journals specialized in software papers, such as the &lt;a href="https://openresearchsoftware.metajnl.com/"&gt;Journal of Open Research Software&lt;/a&gt; or the &lt;a href="https://joss.theoj.org/"&gt;Journal of Open Source Software&lt;/a&gt;, limit the reviewing process to more easily verifiable formal aspects, such as the presence of documentation and the use of appropriate software engineering techniques. Which is, of course, much better than nothing, but it isn&amp;rsquo;t enough.&lt;/p&gt;

&lt;p&gt;A few months ago I wrote about the &lt;a href="http://blog.khinsen.net/posts/2017/05/04/which-mistakes-do-we-actually-make-in-scientific-code/"&gt;kinds of mistakes that we tend to make in scientific computing&lt;/a&gt;. In my experience (I&amp;rsquo;d love to see a systematic study on this), most mistakes are due to discrepancies between what a paper describes and what is actually computed. This covers simple mistakes such as a wrong sign in a computed formula (such as in the widely publicized case of &lt;a href="http://doi.org/10.1126/science.314.5807.1856"&gt;protein structure retractions&lt;/a&gt;), or a typo in the input parameter file for a simulation program, but also more complex situations such as the &lt;a href="http://doi.org/10.1073/pnas.1602413113"&gt;inflated false-positive rates in fMRI studies&lt;/a&gt; that also made it into the headlines of science news. In this case, the fundamental issue was a mismatch between the methods implemented in the software and the methods that would have been appropriate for many typical use cases of the software. Put differently, the users of the software did not fully understand what exactly the software did. They trusted the software authors blindly to do &amp;ldquo;the right thing&amp;rdquo;, whatever that was. And they were probably reinforced in their blind trust by the fact that many of their colleagues used the same software. It&amp;rsquo;s the research version of &amp;ldquo;nobody ever got fired for buying IBM equipment&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Code review is an important step to a better verification of scientific computations, but in the cases I just described its utility is very limited. Neither the wrong sign in the protein crystallography code nor the not-quite-universally-applicable statistical analysis method used by the fMRI software would be detectable by software engineering methods. In the first case, the code would have to be compared to the set of mathematical formulas on which it was based, a task requiring expert knowledge in both crystallography and programming, plus a lot of time - much more than what a reviewer can typically invest. In the second case, code review cannot do anything at all. Only the reviewers of the application papers could have spotted the inappropriateness of the methods - but why should they be expected to be more knowledgeable about the pitfalls than the authors?&lt;/p&gt;

&lt;p&gt;An important but not yet widely recognized aspect of these situations is that today&amp;rsquo;s scientific software incorporates a significant amount of scientific knowledge that is very difficult to access and verify by users and reviewers. The translation of mathematical equations in a paper into efficient computer code is almost a form of encryption from the point of view of scientific knowledge transformation. Extracting equations from software source code is not much easier than extracting source code from compiled binaries.&lt;/p&gt;

&lt;p&gt;But can we do anything about this? I believe we can, but it will require a serious rethinking of the way we use computers to do research. My first explorations in this direction are described in a paper that is now available as a &lt;a href="https://peerj.com/preprints/26633/?td=bl"&gt;PeerJ preprint&lt;/a&gt;. Please have a look, and don&amp;rsquo;t hesitate to ask a question or leave other feedback of any kind!&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">Which mistakes do we actually make in scientific code?</title>
  <link rel="alternate" href="http://blog.khinsen.net/posts/2017/05/04/which-mistakes-do-we-actually-make-in-scientific-code/?utm_source=computer-aided-research&amp;utm_medium=Atom" />
  <id>urn:http-blog-khinsen-net:-posts-2017-05-04-which-mistakes-do-we-actually-make-in-scientific-code</id>
  <published>2017-05-04T10:00:16Z</published>
  <updated>2017-05-04T10:00:16Z</updated>
  <author>
   <name>Konrad Hinsen</name></author>
  <content type="html">
&lt;p&gt;Over the last few years, I have repeated a little experiment: Have two scientists, or two teams of scientists, write code for the same task, described in plain English as it would appear in a paper, and then compare the results produced by the two programs. Each person/team was asked to do a maximum amount of verification and testing before comparing to the other person&amp;rsquo;s/team&amp;rsquo;s work.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;Let me state the most disturbing outcome of this experiment first: we never found complete agreement between the two programs. Not once. And when we explored to find the cause of the discrepancies, we most often found bugs in &lt;em&gt;both&lt;/em&gt; programs, plus missing details in the description written initially for human readers.&lt;/p&gt;

&lt;p&gt;The two most practically significant experiments of this kind were actual research projects that have since been published:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;a href="http://dx.doi.org/10.1063/1.4821598"&gt;A comparison of reduced coordinate sets for describing protein structure&lt;/a&gt;. For this work, Shuangwei Hu wrote Matlab code, and I wrote the Python code that was &lt;a href="https://doi.org/10.6084/m9.figshare.798825.v1"&gt;ultimately published&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;a href="http://dx.doi.org/10.1063/1.4823996"&gt;Model-free simulation approach to molecular diffusion tensors&lt;/a&gt;. In this case, Gerald Kneller wrote Mathematica code, and I wrote the &lt;a href="https://doi.org/10.6084/m9.figshare.808594.v1"&gt;Python version&lt;/a&gt; again.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Later on, I did a series of similar experiments with PhD students participating in what can be summarized as advanced Python programming courses. PhD students with limited programming experience are exactly the kind of scientists who write much of the software for research projects. But the setting was &amp;ldquo;exercises in a course&amp;rdquo;, with programming tasks being much simpler, and much better specified, than what the typical research project requires.&lt;/p&gt;

&lt;p&gt;The results of these experiments that I will summarize here are no more than anecdotal evidence. In fact, the initial goal was not to perform an experiment in scientific computing, but to perform better checks on the code for a research project. It would be interesting to do a larger-scale proper study, but that&amp;rsquo;s beyond my means and competence.&lt;/p&gt;

&lt;p&gt;As I already mentioned, there was never complete agreement between the two programs supposed to solve the same problem. In many cases the differences were small, and I suspect many would have brushed them away as caused by uncontrollable round-off, given that all problems were numerical in nature. But upon closer scrutiny, we always found different issues, and got much better agreement after fixing them. This is why I still believe that &lt;a href="https://khinsen.wordpress.com/2015/01/07/why-bitwise-reproducibility-matters/"&gt;bitwise reproducibility matters&lt;/a&gt;. When small numerical differences are inevitable, as they are with today&amp;rsquo;s scientific programming languages, it becomes much more difficult to search for and eliminate mistakes.&lt;/p&gt;

&lt;p&gt;So which are the mistakes that were uncovered by comparing two independent implementations of the same method?&lt;/p&gt;

&lt;p&gt;Number one, by far, is discrepancies between the informal description for human readers and the executable implementation. Put simply, the programs did not compute what the informal description said they should compute, or the informal description was incomplete, admitting more than one interpretation.&lt;/p&gt;

&lt;p&gt;Number two is typos in numerical constants and in variable names. Since I can almost hear proponents of static typing saying &amp;ldquo;that&amp;rsquo;s what you deserve for using Python&amp;rdquo;, let me add that most typos in variable names would &lt;em&gt;not&lt;/em&gt; have been caught by static type checking. If you have two integer loop indices &lt;code&gt;i&lt;/code&gt; and &lt;code&gt;j&lt;/code&gt;, no type checker will complain when you interchange them by mistake.&lt;/p&gt;

&lt;p&gt;Number three is off-by-one-or-two errors in loops and in array indices. If you have a complex formula involving lots of x[i], x[i+1], and x[i&amp;ndash;1], it&amp;rsquo;s hard to avoid getting an index wrong occasionally. Unfortunately, array bounds checking does not catch all of these mistakes. Another interesting observation is that this type of mistake is just as likely in the informal description as in the code. Humans are apparently not very good at handling this kind of &amp;ldquo;detail&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Is there anything we can do to reduce the risk of these types of mistakes? I&amp;rsquo;d say yes, but it&amp;rsquo;s not going to be easy.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start with what software engineering techniques could do to improve the situation. The main opportunity I see is for mistakes of the third kind. Index arithmetic could be eliminated altogether by abstracting it away. Most situations correspond to one of a handful of patterns, often called &lt;a href="https://en.wikipedia.org/wiki/Stencil_(numerical_analysis)"&gt;stencils&lt;/a&gt;, which could become functions or macros in a suitable domain-specific language. Another idea, applicable to legacy code, is to have code checking tools recognize stencils and small deviations from common stencils and point out potential mistakes - see &lt;a href="https://camfort.github.io/tvcs2017/#contrastin"&gt;this presentation&lt;/a&gt; at the recent &lt;a href="https://camfort.github.io/tvcs2017/"&gt;2nd Meeting on Testing and Verification for Computational Science&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Similar heuristic searches for potential mistakes could be applied to typos in variable names, though it is not sure that such reports would ultimately be useful. The real issue is the widespread use of short and similar variable names. A radical approach would be to ban them as part of a programming style guide, and have source code checkers flag violations of such a rule.&lt;/p&gt;

&lt;p&gt;For the main source of mistakes, discrepancies between informal specification and implementation, software engineering approaches are totally hopeless in my opinion. After all, the programs are perfectly reasonable and consistent, they merely solve a problem that is different from the one they were written to solve. Given the current state of technology, the comparison between the two problem decriptions can only be done by human proofreading, as long as at least one problem description is informal. I suspect the best approach we have today is exactly what I described above - develop two independent implementations and compare.&lt;/p&gt;

&lt;p&gt;In the long run, we can work on reducing the gap between informal descriptions (papers, software documentations) and executable implementations. I vaguely remember hearing about people exploring the possibility of turning informal descriptions into formal specifications by natural language processing - if anyone has a reference, please leave a comment! But I am rather skeptical of this approach, and therefore I prefer to let humans make the move towards formal specifications. The human-computer interface for such specifications is what I call &lt;a href="http://sjscience.org/article?id=527"&gt;digital scientific notations&lt;/a&gt;, and I am currently working on &lt;a href="https://github.com/khinsen/leibniz"&gt;developing such a notation&lt;/a&gt; for my corner of science, which is computational physics and chemistry.&lt;/p&gt;

&lt;p&gt;Finally, let me point out that my experiments and their conclusion apply only to research code in the strict sense, i.e. code that was written to compute a result that is &lt;em&gt;a priori&lt;/em&gt; unknown. Referring to my &lt;a href="http://blog.khinsen.net/posts/2017/01/13/sustainable-software-and-reproducible-research-dealing-with-software-collapse/"&gt;earlier post on software collapse&lt;/a&gt;, this is the fourth and project-specific layer of scientific software. When writing libraries and software tools that implement established methods for wider use, the situation is different because testing can be used much more effectively.&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">Reproducibility does not imply reproduction</title>
  <link rel="alternate" href="http://blog.khinsen.net/posts/2017/01/24/reproducibility-does-not-imply-reproduction/?utm_source=computer-aided-research&amp;utm_medium=Atom" />
  <id>urn:http-blog-khinsen-net:-posts-2017-01-24-reproducibility-does-not-imply-reproduction</id>
  <published>2017-01-24T08:42:16Z</published>
  <updated>2017-01-24T08:42:16Z</updated>
  <author>
   <name>Konrad Hinsen</name></author>
  <content type="html">
&lt;p&gt;In discussions about computational reproducibility (or replicability, or repeatability, according to the preference of each author), I often see the argument that reproducing computations may not be worth the investment in terms of human effort and computational resources. I think this argument misses the point of computational reproducibility.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;Obviously, there is no point in repeating a computation identically. The results will be the same. So the only reason to re-run a computation is when there are doubts about the exact software or data that were used in the original work, or doubts about the reliability of the hardware.&lt;/p&gt;

&lt;p&gt;The point of computational reproducibility is to dispel those doubts. The holy grail of computational reproducibility is not a world in which every computation is run five times, but a world in which a straightforward and cheap analysis of the published material verifies that it is reproducible, so that there is no need to run it again. Actual reproduction attempts would be rare and reserved for situations such as suspicion of hardware failure or suspicion of fraud.&lt;/p&gt;

&lt;p&gt;So how can we make reproducibility credible without actually doing reproduction? By using toolchains that have been proven in practice to make computations reproducible. Of course we do need to attempt &lt;em&gt;some&lt;/em&gt; reproductions in order to validate these toolchains, but it&amp;rsquo;s sufficient to do this for short computations. And if the toolchain is any good, the human effort should be close to zero as well.&lt;/p&gt;

&lt;p&gt;The mere fact that we discuss computational reproducibility at all shows that we do have doubts. Most of us doing computational science have at some point had doubts about our own work. How did I make this figure? Was it made with the latest version of this script, or an earlier one? Did I run that simulations before or after installing the recent important bug fix? And when it comes to examining work by others described in a journal article, our ignorance usually reaches a level that the word &amp;ldquo;doubt&amp;rdquo; cannot convey - we don&amp;rsquo;t really know anything. All we have is someone else&amp;rsquo;s incomplete story. If we have doubts about our own work whose full story we know, why should we trust someone else&amp;rsquo;s story blindly?&lt;/p&gt;

&lt;p&gt;So the question about &amp;ldquo;how much&amp;rdquo; reproducibility we need comes down to a more basic question: What would it take to make you trust a computational result beyond a reasonable doubt? Here is my personal list of acceptable evidence as of today:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;I can repeat the computation on my computer and get close enough results.&lt;/li&gt;
 &lt;li&gt;The results are published as an &lt;a href="http://www.activepapers.org/"&gt;ActivePaper&lt;/a&gt;.&lt;/li&gt;
 &lt;li&gt;The results come with a &lt;a href="https://nixos.org/"&gt;Nix&lt;/a&gt; or &lt;a href="http://guixsd.org/"&gt;Guix&lt;/a&gt; recipe for reproducing them.&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;The last two cases point to toolchains that I personally consider trustworthy, given the experience I have with them. Both toolchains generate a detailed trace of what happened, with references to all the software and data. And both toolchains make mistakes improbable enough that the remaining risk is acceptable for me. Neither toolchain provides protection from fraud, so if I had a reason to suspect fraud, I&amp;rsquo;d still attempt a reproduction.&lt;/p&gt;

&lt;p&gt;Note that I am not saying that everybody should use one of those toolchains. In their current state, they are neither universal nor sufficiently easy to use. But they do show the toolchain approach to reproducibility is viable.&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">Sustainable software and reproducible research: dealing with software collapse</title>
  <link rel="alternate" href="http://blog.khinsen.net/posts/2017/01/13/sustainable-software-and-reproducible-research-dealing-with-software-collapse/?utm_source=computer-aided-research&amp;utm_medium=Atom" />
  <id>urn:http-blog-khinsen-net:-posts-2017-01-13-sustainable-software-and-reproducible-research-dealing-with-software-collapse</id>
  <published>2017-01-13T12:40:52Z</published>
  <updated>2017-01-13T12:40:52Z</updated>
  <author>
   <name>Konrad Hinsen</name></author>
  <content type="html">
&lt;p&gt;Two currently much discussed issues in scientific computing are the sustainability of research software and the reproducibility of computer-aided research. I believe that the communities behind these two ideals should work together on taming their common enemy: software collapse. As a starting point, I propose an analysis of how the risk of collapse affects sustainability and reproducibility.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;What I call software &lt;em&gt;collapse&lt;/em&gt; is what is more commonly referred to as software &lt;em&gt;rot&lt;/em&gt;: the fact that software stops working eventually if is not actively maintained. The rot/maintenance metaphor is not appropriate in my opinion because it blames the phenomenon on the wrong part. Software does not disintegrate with time. It stops working because the foundations on which it was built start to move. This is more like an earthquake destroying a house than like fungi or bacteria transforming food, which is why I am trying out the term &lt;em&gt;collapse&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The software stacks used in computational science have a multi-layer structure that seems to be nearly universal. At the bottom, there is non-scientific infrastructure, such as operating systems, compilers, and support code for I/O, user interfaces, etc. All of this software is used by scientists in the same way as by other computer users. The predominant view is that this software is external to scientific computing, much like computer hardware. One exception is infrastructure software for high-performance computing, which like the hardware it runs on is often designed specifically for use in science and engineering.&lt;/p&gt;

&lt;p&gt;The second layer is scientific infrastructure. Here we find libraries and utilities used for research in many different disciplines, such as LAPACK, NumPy, or Gnuplot. The people developing this software tend to be researchers or research software engineers, i.e. people with a scientific background. The methods (algorithms, data structures) implemented in these packages are typically well-known and stable. This does not exclude ongoing research on improving the implementations, but from the users&amp;rsquo; point of view, the job done by the software remains the same, often for several decades.&lt;/p&gt;

&lt;p&gt;The third layer contains discipline-specific research software. These are tools and libraries that implement models and methods which are developed and used by research communities. Often the developers are simply a subset of the user community, but even if they aren&amp;rsquo;t, they work in very close contact with their users, who provide essential feedback not only on the quality of the software, but also on the directions that future development should take.&lt;/p&gt;

&lt;p&gt;The fourth and final layer is project-specific software, which is whatever it takes to do a computation using software building blocks from the lower three levels: scripts, workflows, computational notebooks, small special-purpose libraries and utilities. At the end of a project, such software may become the starting point for software specific to another project, but it is rarely reused without modification, and rarely used by anyone except the members of the project that developed it.&lt;/p&gt;

&lt;p&gt;Computational models and methods often move down the stack in the course of time. They are developed initially within a specific project, then the more widely useful ones become part of discipline-specific software, and some of them may find adoption in other fields of research and become a part of the scientific infrastructure layer.&lt;/p&gt;

&lt;p&gt;Software in each layer builds on and depends on software in all layers below it, meaning that changes in any lower layer can cause it to collapse.&lt;/p&gt;

&lt;p&gt;The reproducible research community focuses on the fourth layer, the project-specific software. Traditionally, the main obstacle to reproducibility was that this layer was not published, and sometimes even deleted by its authors at the end of a project. This layer also contains algorithms executed by a human user, e.g. by entering commands one by one into the computer. This ephemeral software is typically not even recorded. Fixing these problems is mainly a matter of creating an awareness of their importance, and much progress has been made in this respect. But the problem of layer&amp;ndash;4 software collapsing due to changes in the lower levels remains largely unsolved. Project-specific software is particularly vulnerable to collapse because it is almost never maintained, since its active days are over.&lt;/p&gt;

&lt;p&gt;The sustainable software community is mainly interested in layer 3, the discipline-specific community software. Its development is fragile because the importance of this software is not yet recognized by institutions and funders, unlike the scientific infrastructure software one layer below. Moreover, this software is often developed by scientists with insufficient training in software engineering techniques. There are essentially two tasks that need to be organized and financed: preventing collapse due to changes in layers 1 and 2, and implementing new models and methods as the scientific state of the art advances. These two tasks go on in parallel and are often executed by the same people, but in principle they are separate and one could concentrate on just one or the other.&lt;/p&gt;

&lt;p&gt;The common problem of both communities is collapse, and the common enemy is changes in the foundations that scientists and developers build on. The options they have for dealing with this are about the same as for house owners facing the risk of earthquakes:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Accept that your house or software is short-lived. In case of collapse, start from scratch.&lt;/li&gt;
 &lt;li&gt;Whenever shaking foundations cause damage, do repair work before more serious collapse happens.&lt;/li&gt;
 &lt;li&gt;Make your house or software robust against perturbations from below.&lt;/li&gt;
 &lt;li&gt;Choose stable foundations.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;House owners generally opt for strategies 3 or 4, or a mixture of them. Strategies 1 and 2 are unattractive because house owners might well be injured or killed during a collapse.&lt;/p&gt;

&lt;p&gt;Most software developers, in science or elsewhere, prefer strategies 1 or 2. In many business settings, this makes sense because software is short-lived or rapidly evolving anyway, due to changing requirements and newly appearing possibilities. In science, these motivations exist as well, but must be weighed against the need for preservation of the scientific knowledge embodied by scientific software. You may not care about losing the Web browser you used long ago, given that there&amp;rsquo;s a better one now. But if ten years from now, doubts come up about the analysis of &lt;a href="http://ligo.org/"&gt;LIGO&lt;/a&gt; data, you want to be able to go back to the analysis code and check what exactly was done at the time.&lt;/p&gt;

&lt;p&gt;A difference between the sustainable software and the reproducible research communities is that the former privileges strategy 2, continuous repair, whereas the latter dreams of strategy 4, stable foundations. Strategy 2 is in fact easier to adopt, given that most of the software industry is applying it. Strategy 4 is seen as unrealistic by many, because stable foundations are hard to find, and the few we have impose unpleasant restrictions. But if developers in layer 3 adopt the continuous-repair strategy, this leaves only one option for the code in layer 4 - accept that it is short-lived. This is more or less what we see happening at the moment. For a recent discussion, see &lt;a href="http://ivory.idyll.org/blog/2017-pof-software-archivability.html"&gt;this blog post&lt;/a&gt; by C. Titus Brown and the discussion following it.&lt;/p&gt;

&lt;p&gt;In one of the comments there, Daniel S. Katz proposes a cost-benefit analysis, which to the best of my knowledge has not been attempted until now. However, I think it should be done globally, rather than for an individual research project. A move towards stable foundations (strategy 4) is likely to require a large up-front investment, but lower development costs later on, for scientific code in all layers. It might well be interesting for nothing else but reducing global development costs, not even counting the hard to evaluate benefit of long-term reproducibility.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s also worth looking at &lt;em&gt;why&lt;/em&gt; software foundations are shaking all the time. Why can&amp;rsquo;t we just keep on using the same software forever, if we are happy with the way it works?&lt;/p&gt;

&lt;p&gt;One reason is the bottom layer of our software stack, which we share with non-scientific software. There are market incentives for shaking up the foundations of commercial software, which then cause collateral damage elsewhere, such as in science. For example, some markets rely on planned obsolescence and never-ending change to create continuous customer demand. Smartphones are a good example. Also, a company controlling a software platform might benefit from changing it a bit all the time in order to retain control and customer attention. Finally, security problems in systems software are discovered regularly, and their fixes can send ripples up the software stack. All this makes it difficult to find stable foundations to build on. However, it is clearly not impossible. After all, banks have been keeping their COBOL software alive for decades. At worst, we could build our own bottom layer instead of sharing it with other application domains. One advantage of scientific software in that respect is that it has few if any security concerns to deal with.&lt;/p&gt;

&lt;p&gt;Unfortunately, we also have home-made quakes in our software stack, due to changes in layers 2 and 3. In the fast-paced development of layer 3, collateral damage sometimes leads to collapse in layer 4. I suspect much of this could be avoided with some more attention on stability, plus extensive testing. What&amp;rsquo;s worse is a widespread attitude that considers stability impossible anyway and concludes that one more breaking change is not such a big problem after all. This is particularly harmful for the scientific infrastructure of layer 2. I&amp;rsquo;ll just mention my two-year-old &lt;a href="https://khinsen.wordpress.com/2014/09/12/the-state-of-numpy/"&gt;rant about NumPy&lt;/a&gt; as an example. In view of the systematic non-maintenance of layer&amp;ndash;4 software, this is an inappropriate attitude in the world of scientific computing in my opinion.&lt;/p&gt;

&lt;p&gt;As a final remark, strategy 3 does not seem to exist in the software world. There are no proven techniques for making a program robust against changes in its foundations. Software interfaces are much too rigid for that. I vaguely remember Alan Kay speaking about more lenient interface mechanisms - if anyone has a reference to share, please leave a comment! A recent &lt;a href="https://www.youtube.com/watch?v=oyLBGkS5ICk"&gt;presentation by Rich Hickey&lt;/a&gt;, the creator of the Clojure language, also contains useful ideas for dealing with change in interfaces (executive summary: add new features, but don&amp;rsquo;t remove or change existing ones), but it&amp;rsquo;s more of a move towards strategy 4 than strategy 3. More generally, I would like to see more research and development along these lines. Robustness is a major design principle in other engineering domains, and software would benefit from a larger dose as well.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note added 2019&amp;ndash;09&amp;ndash;04:&lt;/strong&gt; I have written a more detailed article about &lt;a href="https://doi.org/10.1109/MCSE.2019.2900945"&gt;Dealing with Software Collapse&lt;/a&gt; for the May 2019 issue of &lt;em&gt;Computing in Science and Engineering&lt;/em&gt; magazine. A &lt;a href="https://hal.archives-ouvertes.fr/hal-02117588"&gt;preprint&lt;/a&gt; is available as well.&lt;/p&gt;</content></entry>
 <entry>
  <title type="text">From reproducible to verifiable computer-aided research</title>
  <link rel="alternate" href="http://blog.khinsen.net/posts/2016/05/11/from-reproducible-to-verifiable-computer-aided-research/?utm_source=computer-aided-research&amp;utm_medium=Atom" />
  <id>urn:http-blog-khinsen-net:-posts-2016-05-11-from-reproducible-to-verifiable-computer-aided-research</id>
  <published>2016-05-11T12:40:40Z</published>
  <updated>2016-05-11T12:40:40Z</updated>
  <author>
   <name>Konrad Hinsen</name></author>
  <content type="html">
&lt;p&gt;The importance of reproducibility in computer-aided research (and elsewhere) is by now widely recognized in the scientific community. Of course, a lot of work remains to be done before reproducibility can be considered the default. Doing computational research reproducibly must become easier, which requires in particular better support in computational tools. Incentives for working and publishing reproducibly must also be improved. But I believe that the Reproducible Research movement has made enough progress that it&amp;rsquo;s worth considering the next step towards doing trustworthy research with the help of computers: verifiable research.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;Verifiable research is research that you can verify for yourself. Not in the sense of verifying the scientific conclusions, which often can only be done many years later. The more modest goal is to verify that a publication contains no mistakes of the kind that every human being tends to make: mistakes in manual computations, mistakes in transcribing observations from a lab notebook, etc.&lt;/p&gt;

&lt;p&gt;Ideally, all research should be verifiable. A paper is supposed to provide sufficient details about the work that was done to enable competent peers to verify the reasoning and repeat any experiments. Peer review is supposed to certify that a paper is verifiable, and reviewers are even encouraged to do the verification if that is possible with reasonable effort.&lt;/p&gt;

&lt;p&gt;In the pre-computing era, much published research was indeed verifiable. Given the high cost of verifying experimental work, it is safe to assume that actual verification was the exeception. But theoretical work of any importance was commonly verified by many readers who repeated the (manual) computations.&lt;/p&gt;

&lt;p&gt;With the increasing use of computers, papers slowly turned into mere summaries of research work. Providing all the details was simply impossible - software was too complex to be fully described in a journal article. It also became common to use software written by other people, and even commercial software whose detailed workings are secret. This development was nicely summarized &lt;a href="http://statweb.stanford.edu/~wavelab/Wavelab_850/wavelab.pdf"&gt;by Buckheit and Donoho&lt;/a&gt; in 1995 in what became a famous quote in the Reproducible Research movement:&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;An article about computational science in a scientific publication is not the scholarship itself, it is merely advertising of the scholarship. The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Today this statement applies not only to computational science, but to all of computer-aided research, as many experimental and theoretical studies involve computers and software as well. The publication of all software and all input datasets in a form that other scientists can actually process on their own computers has become the main objective for making computer-aided research reproducible.&lt;/p&gt;

&lt;p&gt;Unfortunately, having all the software and input data that go with a journal article is still not sufficient to make the work verifiable. With the exception of particularly simple computations, it is practically impossible to figure out what the software really computes, and in particular to verify that it computes what the paper claims it computes. Assuming, of course, that the paper actually &lt;em&gt;does&lt;/em&gt; provide a detailed description of its claims, which is often not the case. Much computer-aided research is thus &lt;a href="https://en.wikipedia.org/wiki/Not_even_wrong"&gt;&amp;ldquo;not even wrong&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It is the complexity of much modern scientific software that makes verification practically impossible, and for that reason software is rarely subjected to peer review. After all, who would accept the Herculean task to verify the correct functioning of a piece of software? Even &amp;ldquo;software papers&amp;rdquo;, i.e. papers that merely exist to provide a citable reference for some software, are reviewed without any serious validation of the software itself. At best, reviewers check that best practices of software engineering have been respected, for example by writing a test suite with good code coverage. But no amount of testing can verify that the software computes what it is supposed to compute. If some numerical constant in the source code is off by 10% due to a typo, there&amp;rsquo;s a good chance that nobody will ever notice. Such mistakes have happened (see &lt;a href="http://dx.doi.org/10.1038/467775a"&gt;this article&lt;/a&gt; for a few stories), and there are good reasons to believe they are actually frequent (see &lt;a href="http://f1000research.com/articles/3-303/v1"&gt;this article&lt;/a&gt; for arguments). The most convincing argument should be our daily experience with computers that crash or ask us to install &amp;ldquo;critical updates&amp;rdquo;. If systems software is so clearly full of mistakes, is it reasonable to assume that scientific software has none at all?&lt;/p&gt;

&lt;p&gt;The difficulty of verifying computational results in combination with the obvious importance of computational techniques in science has lead to a change of attitude that in my opinion is detrimental to science in the long run. Most importantly, the burden of proof has been shifted from the proponents of a new hypothesis to its opponents. If you cannot show that a computational study is wrong, then it is silently assumed correct. If you want to publish results that are contradictory to work published earlier, it&amp;rsquo;s your obligation to explain why, even though you cannot possibly verify the earlier work. This is why protein structures in contradiction with the &lt;a href="http://www.the-scientist.com/news/home/39805/"&gt;later retracted ones from Geoffrey Chang&amp;rsquo;s group&lt;/a&gt; were rejected for publication for a long time. Contradictory results should be handled by a critical inspection of all of them, but this is possible only for verifiable research.&lt;/p&gt;

&lt;p&gt;Another detrimental change of attitude is that &amp;ldquo;correct&amp;rdquo; has been replaced by &amp;ldquo;community-accepted&amp;rdquo; as a quality criterion in many fields. Recently, I have started to ask a simple question after seminars on computational work: &amp;ldquo;Why should I believe your results? What did you do to verify them?&amp;rdquo; Most often, the answer is &amp;ldquo;We used software and protocols that are widely applied in our community&amp;rdquo;. Unfortunately, popularity can be taken as an indicator of correctness only if it is safe to assume that many users have actually verified those tools and methods. Which again assumes verifiability as a minimum criterion.&lt;/p&gt;

&lt;h2 id="so-what-can-we-do"&gt;So&amp;hellip; what can we do?&lt;/h2&gt;

&lt;p&gt;Verifiable computer-aided research is a tiny subset of today&amp;rsquo;s published research. It&amp;rsquo;s even a small subset of today&amp;rsquo;s reproducible research. Can we do something about this? I believe we can, and I will summarize some possible approaches.&lt;/p&gt;

&lt;p&gt;The most obvious approach to make a computation verifiable is to document all code and data well enough that a competent reader is convinced of its correctness. Literate programming (for algorithms) and computational notebooks (for computations) are good techniques for this. As with any scientific proofreading, verification by inspection requires much care and a critical attitude. People are easily fooled into believing something because it is well presented, for example. But the most important obstacle to this approach is the modularity of much of today&amp;rsquo;s scientific software. If you reuse existing libraries - and there are of course good reasons to do so - then you probably won&amp;rsquo;t rewrite them in literate programming style for explaining their algorithms to your critical reader. A computation is only as verifiable as its least verifiable ingredient.&lt;/p&gt;

&lt;p&gt;Another way to make computer-aided research verifiable is to make the computations reimplementable. This means that the published journal article, or some supplementary material to that article, contains a precise enough human-readable description of the algorithms that a scientist competent in the field can write a new implementation from scratch, and verify that it produces the same (or close enough) results. This is not a fool-proof approach, of course, and again modularity is a major risk factor. If the computation uses some complex library and the reimplementor chooses to use the same library, then the library code is not verified by the reimplementation. The more the reimplementation differs from the original authors&amp;rsquo; code, the better it is as a verification aid. This is by the way also a strong argument for diversity in scientific software. In terms of development efficiency, a single community-supported software package per field is great, but for verifiability, it is better to have multiple packages that can do the same job.&lt;/p&gt;

&lt;p&gt;Both approaches I have outlined fail for complex software. A million-line simulation code developed over many years by an entire research group can neither be studied nor reimplemented by a single person wishing to verify it. Even a small team working in close collaboration wouldn&amp;rsquo;t be up to the task. The solution I propose for this situation is to introduce an intermediate layer between the software and the human-readable documents (papers, software documentation) that describe what it computes. A layer that contains all the science but none of the technicalities of the software, such as parallelism, platform-dependence, or resource management. The idea is to &amp;ldquo;factor out&amp;rdquo; the &lt;a href="https://en.wikipedia.org/wiki/No_Silver_Bullet"&gt;accidental complexity&lt;/a&gt; and retain only the essential complexity, the one due to the complexity of the models and methods that the software implements. This idea is very similar to the use of &lt;a href="https://en.wikipedia.org/wiki/Formal_specification"&gt;formal specifications&lt;/a&gt; in software development. The specification would be verified by human scientists, whereas the conformity of the software to the specification would be checked by automated methods, of which &lt;a href="https://en.wikipedia.org/wiki/QuickCheck"&gt;randomized unit testing&lt;/a&gt; is probably the most immediately useful one.&lt;/p&gt;

&lt;p&gt;An intermediate layer that factors out accidental complexity is also of interest for other uses in scientific research. That new layer would be the closest we can get to a digital representation of a model or a method. Rather than use it just in the specification of a single piece of software, we can use it for all kinds of analyses and comparisons, and cite it as the main scientific reference in work based on it, in addition to the citation to the software as the technical tool for doing the computations. For this reason, I call this layer &amp;ldquo;digital scientific knowledge&amp;rdquo; and the languages for expressing it &amp;ldquo;digital scientific notation&amp;rdquo;. None of this exists today, but many developments in computer science can be used as a basis for its development. For the details, see &lt;a href="http://sjscience.org/article?id=527"&gt;this article&lt;/a&gt;.&lt;/p&gt;</content></entry></feed>