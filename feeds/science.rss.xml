<?xml version="1.0" encoding="utf-8"?> 
<rss version="2.0">
 <channel>
  <title>Konrad Hinsen's Blog: Posts tagged 'science'</title>
  <description>Konrad Hinsen's Blog: Posts tagged 'science'</description>
  <link>http://blog.khinsen.net/tags/science.html</link>
  <lastBuildDate>Wed, 02 Dec 2020 20:27:36 UT</lastBuildDate>
  <pubDate>Wed, 02 Dec 2020 20:27:36 UT</pubDate>
  <ttl>1800</ttl>
  <item>
   <title>Some comments on AlphaFold</title>
   <link>http://blog.khinsen.net/posts/2020/12/02/some-comments-on-alphafold/?utm_source=science&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2020-12-02-some-comments-on-alphafold</guid>
   <pubDate>Wed, 02 Dec 2020 20:27:36 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;Many people are asking for my opinion on the recent &lt;a href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology"&gt;impressive success of AlphaFold at CASP14&lt;/a&gt;, perhaps incorrectly assuming that I am an expert on protein folding. I have actually never done any research in that field, but it&amp;rsquo;s close enough to my research interests that I have closely followed the progress that has been made over the years. Rather than reply to everyone individually, here is a public version of my comments. They are based on the limited information on AlphaFold that is available today. I may come back to this post later and expand it.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;First of all, the GDT scores obtained by AlphaFold are impressive, which is of course the reason for all the buzz at the moment. The GDT score measures how close a predicted structure is to the experimentally determined one. It is defined on a scale from 0 to 100 and can roughly be interpreted as the percentage of amino acid residues that were placed correctly. For about 2/3 of the proteins in this year&amp;rsquo;s competition, AlphaFold achieved a GDT score in the 90s, whereas in the not so distant past, a score in the 70s was already considered very good. Which exact techniques were used to obtain the predicted structures is not something I can comment on: as far as I know, no technical details have been made public so far. Nor is AlphaFold a publicly available program or service that scientists could explore or apply to their own work. So all we know for now is that DeepMind, the company behind AlphaFold, has figured out a way to obtain good scores at CASP14. In the following I will assume that this is not just good luck, and that the method is applicable to a much larger class of proteins than the CASP candidates.&lt;/p&gt;

&lt;p&gt;The scores obtained by AlphaFold are clearly a sign of significant progress. But does it mean that we have &amp;ldquo;a solution to a 50-year-old grand challenge in biology&amp;rdquo;, as the press release claims? That depends on what exactly one considers that challenge to be.&lt;/p&gt;

&lt;p&gt;If the challenge of protein folding is taken to be a purely pragmatic one, i.e. being able to predict structure from sequence, then AlphaFold is a candidate for a solution. How much of a solution will depend on further evaluations that remain to be done, on a larger range of proteins. CASP is limited to proteins for which experimental structures are (just) available. But some proteins resist experimental structure determination, for example because they have no well-defined structure at all. A robust structure prediction tool would have to identify such cases, rather than predict bogus structures. Allosteric proteins, which are proteins that can take more than one stable structure, provide another set of interesting test cases. A third case of interest is protein pairs that differ minimally in their sequence but importantly in structure. The goal of evaluating the robustness of a tool is to understand how it behaves at best, at worst, and for important edge cases, such that its users can judge the trustworthiness of its results.&lt;/p&gt;

&lt;p&gt;For many scientists, including myself, having a black-box structure prediction tool is not sufficient to declare the protein folding problem solved. A solution requires an in-depth understanding of the mechanisms that determine protein structure. Whether or not AlphaFold can contribute to identifying these mechanisms is a question that scientists can only start to examine, and only if AlphaFold becomes sufficiently accessible and inspectable for critical examination by outside experts. I hope this will happen, and in fact I am optimistic that it will happen: the problem is important enough to deserve a serious effort by everyone involved. AlphaFold is not the end of the quest for a solution of the protein folding problem, but it could well turn out to be the beginning of a new chapter in the story.&lt;/p&gt;</description></item>
  <item>
   <title>The landscapes of digital scientific knowledge</title>
   <link>http://blog.khinsen.net/posts/2020/07/08/the-landscapes-of-digital-scientific-knowledge/?utm_source=science&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2020-07-08-the-landscapes-of-digital-scientific-knowledge</guid>
   <pubDate>Wed, 08 Jul 2020 14:09:46 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;Over the last years, an interesting metaphor for information and knowledge curation is beginning to take root. It compares knowledge to a landscape in which it identifies in particular two key elements: streams and gardens. The first use of this metaphor that I am aware of is &lt;a href="https://hapgood.us/2015/10/17/the-garden-and-the-stream-a-technopastoral/"&gt;this essay by Mike Caulfield&lt;/a&gt;, which I strongly recommend you to read first. In the following, I will apply this metaphor specifically to scientific knowledge and its possible evolution in the digital era.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;In the landscape metaphor, streams are timelines of information parcels. News, RSS feeds, Twitter, Facebook, but also scientific journals, are stream media. Gardens are continuously evolving information assemblies that are actively curated by their authors. Encyclopedias and dictionaries are perhaps the oldest examples. In the printed paper era, updating an information collection was expensive because everything had to be reprinted and redistributed. As a consequence, garden-type resources were rare. Digital gardens have no such overhead, and almost no cost other than the work of their curators. More and more people are setting up their own digital gardens as an alternative or complement to the personal stream, better known as a blog. Click &lt;a href="https://joelhooks.com/digital-garden"&gt;here&lt;/a&gt;, &lt;a href="https://tomcritchlow.com/blogchains/digital-gardens/"&gt;here&lt;/a&gt;, and &lt;a href="https://www.christopherbiscardi.com/what-is-a-digital-garden"&gt;here&lt;/a&gt; to see a few examples of personal digital gardens. Like blogs, digital gardens can also be collective efforts, run by a company, a research group, or a larger community. The most widespread tool for digital gardening is the Wiki, but there are also more recent developments in this space, such as &lt;a href="https://www.notion.so/"&gt;Notion&lt;/a&gt; or &lt;a href="https://roamresearch.com/"&gt;Roam&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One distinction that I haven&amp;rsquo;t seen mentioned yet in this context is the one between a garden and a park. Both are curated and thus continuously evolving. But whereas gardens are set up and maintained for the benefit and enjoyment of their owners, parks are created and maintained for the benefit and enjoyment of the public. The difference can be subtle, as digital gardens are often visible to the public as well. But they are more like the unwalled garden on the roadside that you can admire passing by than like the park in which you can take a walk and sit down reading a book. A good example of a digital park is &lt;a href="https://www.wikipedia.org/"&gt;Wikipedia&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Science is all about acquiring information about our world and distilling it into knowledge, and therefore requires a fair bit of gardening. In its early days, it was managed as a garden by and for a small community of people who were motivated by curiosity and relied on personal wealth or on sponsors for doing their work. Universities employed scientists more for teaching than for doing research. Research was done by individuals or small teams, and presented at conferences or in journal articles, much like today. Unlike today, most scientists were up to date on everything that was happening in their field, and had personal exchanges with almost everyone else, in face-to-face meetings or by correspondence. Conferences were events in which conflicting results and different points of views were actively debated, enabling the formation of consensus. The streams of papers and conference contributions thus watered the garden of scientific knowledge.&lt;/p&gt;

&lt;p&gt;All that changed after World War II, when science underwent rapid growth as states injected a lot of money while at the same time expecting the scientific community to cultivate a park rather than a garden, contributing to the common good. Keeping up to date with everybody else&amp;rsquo;s work became more and more difficult, slowly eroding the possibility of consensus formation through live debate at conferences. Productivity metrics focusing on what is easiest to quantify ended up rewarding scientists for contributing to the stream of journal articles, but not for contributing to the cultivation of the park of scientific knowledge. Today, the streams of journal articles have become torrents whose distillation into knowledge is becoming ever more difficult. A good illustration is the (serious) &lt;a href="https://science.sciencemag.org/content/368/6494/924.full"&gt;proposal to use machine learning tools&lt;/a&gt; to make sense of the &amp;ldquo;tsunami&amp;rdquo; of articles resulting from the intense research on the Covid&amp;ndash;19 pandemic.&lt;/p&gt;

&lt;p&gt;The design and implementation of new mechanisms for knowledge distillation and consensus formation is thus a major challenge for science today, and even though machine learning techniques may prove to be helpful, I expect this to remain a fundamentally human task for a long time to come. These new mechanisms must combine technological aspects (good tools for working towards these goals) and social aspects (incentives for scientists to participate in this work). As always, the social aspects are the harder problem. As a first step and as a source for inspiration, let&amp;rsquo;s look at similar existing mechanisms in science and elsewhere. Which digital parks exist? How do they work? Can their mechanisms be adapted to other applications?&lt;/p&gt;

&lt;p&gt;I have already cited Wikipedia as a prime example of a digital park. I had expected to see Wikis more widely used as a platform for collective information curation in science, be it as gardens or parks, but when I searched for examples I found surprisingly few, e.g. &lt;a href="http://www.tricki.org/"&gt;Tricki&lt;/a&gt; (for mathematical problem-solving techniques) or the &lt;a href="https://complexityzoo.uwaterloo.ca/Complexity_Zoo"&gt;Complexity Zoo&lt;/a&gt; (on classes of computational complexity). One problematic aspects of Wikis is that they present only a single view to the outside world. They are better suited for presenting an established consensus than for supporting the process of consensus formation in rapidly evolving fields. One of the rare cases of a Wiki used for coordinating collaborative research, rather than for summarizing the state of the art, is the &lt;a href="https://asone.ai/polymath/index.php"&gt;Polymath project&lt;/a&gt;. It is probably not a coincidence that this has happened in mathematics, a domain whose working habits remain close to those of the early scientific community, with individuals having more agency than in disciplines that are more dependent on material resources.&lt;/p&gt;

&lt;p&gt;&lt;a href="http://fed.wiki.org/"&gt;Federated Wiki&lt;/a&gt; is an interesting evolution of the Wiki concept (initiated by the original inventor of the Wiki, &lt;a href="https://twitter.com/WardCunningham"&gt;Ward Cunningham&lt;/a&gt;) that allows individual contributors to maintain and publish their own view while at the same time encouraging reciprocal borrowing of content. &lt;a href="https://www.youtube.com/watch?time_continue=111&amp;amp;v=2Gi9SRsRrE4"&gt;This video&lt;/a&gt; illustrates the process nicely. Whereas federated Wiki looks like a promising approach to consensus formation, the technical obstacles to setting up a federated Wiki are significant (contributors must manage personal Web servers and domains) and make it difficult to evaluate it in practice.&lt;/p&gt;

&lt;p&gt;Perhaps the most frequent kind of digital park in science today is the collaborative software development project, hosted on platforms such as &lt;a href="https://github.com/"&gt;GitHub&lt;/a&gt;, &lt;a href="https://gitlab.com/"&gt;GitLab&lt;/a&gt;, or similar platforms operated by research institutions. Ignoring the differences resulting from the focus on code rather than prose, the main differences between platforms and Wikis are (1) a stronger emphasis on discussion (&amp;ldquo;issues&amp;rdquo;) and (2) the co-existence of multiple branches representing different public or private views of a common project, with one branch (conventionally named &amp;ldquo;master&amp;rdquo; or &amp;ldquo;main&amp;rdquo;) representing the current consensus.&lt;/p&gt;

&lt;p&gt;Collaborative software projects are an interesting case study also for the question of incentives. The lack of recognition of software development as a research activity has been deplored for a long time. It is usually attributed to the relative novelty of software as a form of research output. But I suspect that the park nature of software, as opposed to the stream nature of journals, is also an important factor, because it makes it more difficult to evaluate an individual&amp;rsquo;s contributions based on purely formal (and thus easily measurable) criteria. On the other hand, today&amp;rsquo;s collaborative platforms make such an evaluation technically feasible, by counting for example the number of commits made by an individual, or the number of lines changed by those commits. Everybody involved in software development will probably agree that this is a stupid metric, but it&amp;rsquo;s no more stupid than counting publications weighted by journal impact factor.&lt;/p&gt;

&lt;p&gt;Another social aspect that is well illustrated by software is the difficulty of the transition from gardens to parks. Projects usually start out as gardens, with a small team developing software for its own use. Then early users start to join, who by necessity have to figure out for themselves how to adapt the software to their needs, and are thus likely to become contributors. With an increasing user base, developers have an interest to work on more robust code and better documentation, in order to reduce the effort of technical support. At that stage, the software becomes attractive to less technically minded users who see no need to ever get in touch with the development community. These users consider the software a park, even if its developers still consider it a garden, leading to contradictory tacit expectations on both sides about the priorities for future maintenance, which I have described &lt;a href="https://blog.khinsen.net/posts/2020/02/26/the-rise-of-community-owned-monopolies/"&gt;in an earlier post&lt;/a&gt;. Developers tend to contribute to this confusion by advertising their project as a park while maintaining it as a garden.&lt;/p&gt;

&lt;p&gt;The above examples illustrate that the technical challenges of digital gardens and parks are somewhat understood and partially solved. Collaborative software development platforms in particular have proven very effective. Adapting their concepts to different use cases and different users looks definitely possible, although the effort required should not be underestimated, in particular for developing appropriate user interfaces. But the real challenge is creating incentives for collaboration, in a universe currently dominated by competition for limited resources.&lt;/p&gt;</description></item>
  <item>
   <title>Industrialization of scientific software: a case study</title>
   <link>http://blog.khinsen.net/posts/2019/11/12/industrialization-of-scientific-software-a-case-study/?utm_source=science&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2019-11-12-industrialization-of-scientific-software-a-case-study</guid>
   <pubDate>Tue, 12 Nov 2019 09:46:08 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;A coffee break conversion at a scientific conference last week provided an excellent illustration for the industrialization of scientific research that I wrote about in a &lt;a href="https://blog.khinsen.net/posts/2019/10/29/the-industrialization-of-scientific-research/"&gt;recent blog post&lt;/a&gt;. It has provoked some discussion &lt;a href="https://twitter.com/khinsen/status/1191673813626499072"&gt;on Twitter&lt;/a&gt; that deserves being recorded and commented on a more permanent medium. Which is here.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;I was chatting with a colleague who I have been meeting at such occasions for about 15 years. He asked me if I was still developing my &lt;a href="http://dirac.cnrs-orleans.fr/MMTK"&gt;Molecular Modelling Toolkit&lt;/a&gt;. I replied that I had stopped working on it because the &lt;a href="https://www.python.org/doc/sunset-python-2/"&gt;end of support for Python 2 in 2020&lt;/a&gt; would quickly make it too hard to use for most of its intended audience, and that I didn&amp;rsquo;t have the means nor the motivation to port it to Python 3. He was quite surprised by my explanations, since he had never heard of the end of support for Python 2, though he did know that there was also a version 3 that was a bit different. His own data analysis scripts were still Python 2 because he had never seen a good reason to even look at Python 3 - never break a working system! But he was alarmed by my prediction that Python 2 would soon disappear from Linux distributions, as he relied on Ubuntu (regularly updated by his lab&amp;rsquo;s systems administrator) to provide him with Python 2 and the few libraries he used.&lt;/p&gt;

&lt;p&gt;I was not surprised, as I have had similar conversations with various colleagues over the last years. In particular when someone contacts me with a Python question, which happens quite frequently as I have the reputation of being a Python expert in my little corner of science. The typical profile of these people is experimentalists who write and use small data analysis scripts, but for whom computation is not the central part of their research. They picked up Python from a colleague or a student, or perhaps through attending a short introductory course (such as a &lt;a href="https://software-carpentry.org"&gt;Software Carpentry&lt;/a&gt; workshop). They have a Python installation on their machine, which is managed by someone else. For them, Python is &amp;ldquo;just there&amp;rdquo;, exactly like other Unix basics such as &lt;code&gt;sh&lt;/code&gt;, or &lt;code&gt;grep&lt;/code&gt;. Moreover, Python has been part of their computing life for many years, often for their entire scientific career, and it has never caused them any trouble.&lt;/p&gt;

&lt;p&gt;When I mentioned my coffee break conversation &lt;a href="https://twitter.com/khinsen/status/1191673813626499072"&gt;on Twitter&lt;/a&gt;, Greg Landrum &lt;a href="https://twitter.com/dr_greg_landrum/status/1192446793612705792"&gt;commented&lt;/a&gt; that he would expect every Python user to make an effort to stay informed about important Python news, so everyone should by now have heard of the end-of-life decision for Python 2. This reminded me of an earlier &lt;a href="https://twitter.com/zacchiro/status/1123168548929536000"&gt;Twitter conversation with Stefano Zacchiroli&lt;/a&gt;, who expressed similar views. As did other actors of the FOSS universe in various real-life discussions. There seems to be a widely shared expectation among FOSS developers that users should follow news about the software they use and take the required steps to adapt to &amp;ldquo;mandatory changes&amp;rdquo;, as Stefano put it. My story illustrates that this is not happening. There is a category of users who (1) don&amp;rsquo;t follow development news and (2) expect the software they use to stay around forever without major breaking changes.&lt;/p&gt;

&lt;p&gt;This is exactly the phenomenon that I call the industrialization of scientific software. Some software packages, such as the core of the Scientific Python ecosystem, become so popular beyond their core community that for an important part of their users they are industrial products, something they obtain once and then use without thinking much about its origins or possible evolution. One sign of a piece software becoming an industrial product is its inclusion in standard Linux distributions, where it is just one package out of many that users can choose from. Linux distributions take the role that department stores have for material goods, providing a platform for window-shopping and acquisition via a standardized procedure. For users who get their software from a Linux distribution, all software looks a bit alike. They have no reason to be more careful about Python than about &lt;code&gt;sh&lt;/code&gt; or &lt;code&gt;grep&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Just like material goods industries, the developers of industrial software, FOSS or not, have no easy way to communicate with their clients. If such communication becomes inevitable, as for example in the case of a product recall for safety reasons, an enormous effort must be deployed to ensure that the message reaches most of its audience. &lt;a href="https://twitter.com/pdebuyl/status/1192410647784574976"&gt;Pierre de Buyl made a suggestion along these lines&lt;/a&gt;, proposing to put up posters with an explanation of the Python 2-&amp;gt;3 transition in every research lab. Asking research funders to support such an action would be an interesting experiment.&lt;/p&gt;

&lt;p&gt;Is there anything that FOSS communities can do to prevent such miscommunication in the future? A look at industrial material goods may provide inspiration. Every non-trivial technical product comes with a user manual, which typically starts with pointing out safety precautions that users are expected to be aware of. Do this, don&amp;rsquo;t do that, watch out for exceptional situations. The documentation of software packages could do the same, and tutorials could then emphasize the message when explaining the product to potential future customers. Here is what such a warning could look like:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;This software package is developed for cutting-edge scientific
research. Our priority in development is to improve the software
and to adapt it for the needs of future applications. As a consequence,
we cannot maintain client code compatibility indefinitely.
Users of this package are expected to check the release notes
(available at http://...) at least once per year, and to adapt
their code to changes in the interfaces explained there.&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I would expect such a notice in the &lt;a href="https://scipy-lectures.org/intro/intro.html"&gt;introduction to the SciPy Lecture Notes&lt;/a&gt;, for example. It describes the SciPy ecosystem, comparing it to alternative choices, but says no word about what users need to do to safely use this ecosystem in their research work. As I said in my &lt;a href="https://blog.khinsen.net/posts/2019/10/29/the-industrialization-of-scientific-research/"&gt;previous post&lt;/a&gt;, the FOSS community has largely been blind to the consequences of software industrialization, maintaining the outdated view that developers and users form a single community. It&amp;rsquo;s time for an upgrade.&lt;/p&gt;

&lt;p&gt;Note added after the initial publication: Dan Katz commented &lt;a href="https://twitter.com/danielskatz/status/1194203819271491586"&gt;on Twitter&lt;/a&gt; with a reference to this very clear &lt;a href="https://collegeville.github.io/CW3S19/WorkshopResources/WhitePapers/quillenCW3S19.pdf"&gt;statement on the development priorities for Matlab&lt;/a&gt;. It would be very helpful if FOSS communities published similar statements about their products.&lt;/p&gt;</description></item>
  <item>
   <title>The industrialization of scientific research</title>
   <link>http://blog.khinsen.net/posts/2019/10/29/the-industrialization-of-scientific-research/?utm_source=science&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2019-10-29-the-industrialization-of-scientific-research</guid>
   <pubDate>Tue, 29 Oct 2019 08:52:19 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;Over the last few years, I have spent a lot of time thinking, speaking, and discussing about the reproducibility crisis in scientific research. An obvious but hard to answer question is: Why has reproducibility become such a major problem, in so many disciplines? And why now? In this post, I will make an attempt at formulating an hypothesis: the underlying cause for the reproducibility crisis is the ongoing industrialization of scientific research.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;First of all, let me explain what I mean by industrialization. In the production of material goods, this term stands for a transition to high-volume production in large sites (factories), profiting from economies of scale. This doesn&amp;rsquo;t directly carry over to immaterial goods such as information and knowledge, which can be copied at near-zero cost. There are, however, aspects of industrialization that do make sense for immaterial goods. The main one is a clear separation of producers, who design and make products for an anonymous group of potential clients, and consumers who choose from pre-existing products on the market. This stands in contrast to 1) producing for one&amp;rsquo;s own consumption, and 2) commissioning someone else (e.g. a craftsman) to make a personalized product. Both of these approaches lead to products optimized for a specific consumer&amp;rsquo;s need, whereas industrial products are made for a large and anonymous market.&lt;/p&gt;

&lt;p&gt;In scientific research, immaterial industrial products are a recent phenomenon. The ones that I will concentrate on are software and datasets that are publicly available and used by scientists outside of any collaboration with their authors. Twenty years ago, this would have been a rare event. Most software was written for in-lab use, and not even made available to others. Only a small number of basic, standardized, and widely used tools, such as compilers, were already industrial products. Most data were likewise not shared outside the research group that collected them. The resulting non-verifiability of scientific findings was an obvious problem, and led ultimately to today&amp;rsquo;s growing Open Science movement. However, the Open Science movement goes well beyond asking for the transparency that is fundamentally required by the scientific method. It wants software and data to be &lt;em&gt;reusable&lt;/em&gt; by other scientists and for different purposes. This is stated most explicitly by the &lt;a href="https://www.go-fair.org/fair-principles/"&gt;FAIR data&lt;/a&gt; label, in which the R stands for reusability. Open Science thus turns software and datasets into industrial commodities.&lt;/p&gt;

&lt;h2 id="the-knowledge-gap"&gt;The knowledge gap&lt;/h2&gt;

&lt;p&gt;A characteristic feature of industrial products is that consumers know much less about them than producers. Consumers cannot ask for personalized explanations either, unlike in the case of a product tailor-made by a craftsman. For material goods, this has led to a wide range of professions, institutions, and regulations designed to help consumers choose suitable products and to protect them against producers&amp;rsquo; abuse of their superior knowledge. Examples are consumer protection agencies, independent experts, technical norms, quality labels, etc. For the industrial products in scientific research, we have no established equivalents yet, and it is not even clear if can ever have them. And that is, in my opinion, a major cause of the reproducibility crisis.&lt;/p&gt;

&lt;p&gt;One piece of evidence is the nature of the cases discussed in the context of the crisis. Reproducibility has been an issue with experiments since the dawn of science, and yet experimental non-reproducibility never shows up in the examples cited. This is not because it is unimportant, but because it is well understood. Experimentalists of all disciplines know what ought to be reproducible in their field, and to which degree, and even the most theoretically minded theoreticians understand that experiments necessarily come with uncertainties. The issues that do show up in the catalogs of non-reproducible results are related to two specific research tools: statistics and computers. Both are recent, and both are routinely used by scientists who do not fully understand them. In other words, their users are consumers of industrial products who lack guidance in their choice of tools and methods.&lt;/p&gt;

&lt;p&gt;Side note: I can almost hear some readers complain that statistics are nothing recent, going back to Arab mathematicians who lived 1000 years ago. You are right. What is recent is the widespread &lt;em&gt;use&lt;/em&gt; of statistics in science. Before computers, statistical methods had to be applied manually, keeping them simple and the datasets small. The kind of statistical inference whose results turn out to be non-reproducible, e.g. in psychology, would not have been possible without computers.&lt;/p&gt;

&lt;p&gt;As an illustration, consider the common use of &lt;em&gt;p&lt;/em&gt;-value thresholds for deciding on significance. Anyone who understands the statistical framework to which &lt;em&gt;p&lt;/em&gt;-values belong (hypothesis testing) agrees that most uses of such thresholds in the scientific literature make no sense. The fact that they are widely used nevertheless thus shows that most people who deal with them, as authors or as reviewers, do not understand the statistical hypothesis testing sufficiently well. And since the abuse of &lt;em&gt;p&lt;/em&gt;-values has been going on for a while, it has now become a de-facto accepted practice, to the point that the people who do understand its absurdity have a hard time being heard. The same can be said about the abuse of journal impact factors for judging the authors of scientific articles, which are a sign of CVs and publication lists becoming industrial products as well.&lt;/p&gt;

&lt;p&gt;The root cause of computational non-reproducibility is an even better illustration of software becoming an industrial product. I noticed that many scientists who have never experienced reproducibility issues themselves find it hard to imagine that they can exist. After all, 2 + 2 is 4, today and tomorrow. What happens when two people obtain different results from &amp;ldquo;the same&amp;rdquo; computation is that they performed in fact different computations (using different software) without being aware of the difference. Software has become ever more complex over the last decades, but software developers have also made an effort to hide this complexity from users - with great success. Most scientists are surprised to learn that when they run that little script sent by a colleague, they are really using hundreds of software packages written (and modified frequently) by hundreds of people over many years with only loose coordination. It&amp;rsquo;s not only those hundreds of packages that are industrial commodities, but even the assembly of all those pieces, for example a Linux distribution.&lt;/p&gt;

&lt;h2 id="what-can-we-do"&gt;What can we do?&lt;/h2&gt;

&lt;p&gt;We can look at the much better understood industrial production of material goods for inspiration for possible solutions. A complex industrial product, such as a car or a television set, comes with a user manual and perhaps an obligation for user training, such as obtaining a driver&amp;rsquo;s license. Moreover, technical norms impose precautions on producers to make their products safe to use by non-experts. Independent experts evaluate products and publish reports that guide consumers in their choice. These approaches can be adapted to scientific software and statistical methods, but that work remains to be done.&lt;/p&gt;

&lt;p&gt;I expect reproducibility to play a major role in this, as a quality label. A reproducible result can still be wrong, but nevertheless reproducibility guarantees the absence of some kinds of common problems. We need additional, complementary quality labels of course, and in fact we have a few, such as the presence of test suites for scientific software, or the existence of provenance metadata for datasets. But this is only the beginning. We do not yet know how to make data and code an industrial product that is safe to use by others, nor do we know how to prepare scientists for working in such an ecosystem. Best practices, even good enough practices, remain to be established.&lt;/p&gt;

&lt;p&gt;Experts will likely be another ingredient of a solution. I suspect that most statistics-related problems could be solved by requiring that every publication making a claim based on statistical significance be validated by a trained statistician. We will have to figure out how to organize this validation. One possibility is to create independent certification agencies, similar to &lt;a href="https://www.cascad.tech/"&gt;cascad&lt;/a&gt; for computational reproducibility, that employ qualified statisticians and deliver validation certificates that will figure prominently in a paper.&lt;/p&gt;

&lt;h2 id="its-not-just-software-and-data"&gt;It&amp;rsquo;s not just software and data&lt;/h2&gt;

&lt;p&gt;As I said above, I have focused on data and code because the computational aspects of science are what I am most familiar with. But industrialization isn&amp;rsquo;t limited to computing. Even the good old journal article is slowly turning into an industrial product. With approaches such as meta-analyses or content mining, scientific papers are being used by people who are not part of the community that their authors belong to, and may thus not have the tacit knowledge shared by that community which might well be necessary to fully appreciate the published results. Interdisciplinary research is also a source of potential misunderstandings due to unshared tacit knowledge.&lt;/p&gt;

&lt;p&gt;We can also see industrialization in the management of science. In fact, the term &amp;ldquo;management&amp;rdquo; in itself implies some form of industrialization. Unfortunately, management principles from the material goods and service industries are being applied uncritically to scientific research, leading to phenomena such as the abuse of the journal impact factor to measure an individual&amp;rsquo;s productivity, or the attribution of budgets based on multiple-year predictions of research outcomes (called &amp;ldquo;grant proposals&amp;rdquo;) that lack any credibility. This suggests that the people who design these management practices consider science itself a commodity, as an industry that can be run just like any other industry. There is, however, a crucial difference: whereas the production of material goods is by necessity based on well-known technologies and processes (otherwise their deployment at scale would be bound to fail), research is all about the unknown. Scientists can describe directions they want to take, but not promise to reach specific goals in the future. Science is intrinsically a bottom-up process, whereas management is about top-down organization.&lt;/p&gt;

&lt;h2 id="open-source-and-open-science"&gt;Open Source and Open Science&lt;/h2&gt;

&lt;p&gt;Back to software, there is one aspect that deserves further discussion: the role of the FOSS (free/open source software) approach that has been gaining traction in research over the last decade, and that has furthermore inspired much of the Open Science movement. The origin of the FOSS movement can be seen as a rebellion against the industrialization of software, which made it difficult to impossible for users to adapt it to their needs. The widely shared story of Richard Stallman&amp;rsquo;s fight against a proprietary printer driver (see &lt;a href="https://poynder.blogspot.com/2006/03/interview-with-richard-stallman.html"&gt;here&lt;/a&gt; for example) is a nice illustration. Initially, the FOSS movement focused on establishing legal means (licenses) to protect software from becoming proprietary. More slowly, and less explicitly, it worked towards a view of software development as something a community does for its own needs, with the ideal that anyone sufficiently motivated should be able to join such a community and participate in the development process. This was a reasonable proposal in the 1980s, when software was simpler and most computer users had by necessity some programming experience.&lt;/p&gt;

&lt;p&gt;Today&amp;rsquo;s situation is very different. Most software has the status of an industrial product for most of its users, whether it&amp;rsquo;s FOSS or not. In theory, anyone can learn anything about FOSS and participate in its evolution at all levels. In practice, the effort is prohibitive for most, and nobody today can envisage understanding all the software they depend on, let alone contributing to its development. As I explained above, it has even become close to impossible to just keep track of which software one depends on. From a user&amp;rsquo;s perspective, the development communities of FOSS projects are industrial software producers just like commercial companies. In a way, FOSS users even have less power because the developer communities have no legal or moral obligations toward their users at all. There are a few cases of institutions that permit users to influence and support the development of FOSS, for example the &lt;a href="http://consortium.pharo.org/"&gt;Pharo consortium&lt;/a&gt; or the &lt;a href="https://www.fondation-inria.fr/"&gt;Inria foundation&lt;/a&gt;, but they are the exception rather than the rule.&lt;/p&gt;

&lt;p&gt;In science, the FOSS ideal of communities producing software for their own use works very well for domain-specific software packages, whose developers are a representative subset of a well-defined scientific community. But infrastructure software that is used across many scientific disciplines will invariably end up being an industrial product for most of its users. This is true for most of the Scientific Python ecosystem, for example, and also for the statistical software universe that has grown around the R language. Note that I am not saying that the FOSS approach has no advantages there. Open source code is very important to ensure the transparency required for making science verifiable. What I am saying is that openness is not enough to ensure that software is a safe-to-use industrial product, nor does it provide a mechanism for keeping a product&amp;rsquo;s evolution in sync with the needs of its user base.&lt;/p&gt;

&lt;p&gt;Whereas the FOSS community has largely remained blind to this issue, the Open Science movement seems to be more aware of the pitfalls of &amp;ldquo;just&amp;rdquo; being open, at least for data. The I and R (interoperability, reusability) in FAIR are the best evidence for this. For now, they remain ideals for which practically usable implementations remain to be defined. Perhaps this will lead to a more careful consideration of reusability for software as well. As with the material goods industries, the key is to recognize users and educators as stakeholders and ensure that their needs are taken into account by producers. Open source communities working on widely used infrastructure software could, for example, adopt a governance model that includes representative non-developing users. Funders of such communities could make such a governance model a condition for funding. But the very first step is creating an awareness of the problem. Development communities should openly state their ambition. It&amp;rsquo;s OK to develop software for use inside a delimited community, but then don&amp;rsquo;t advertise it as easy to use for everyone. It&amp;rsquo;s also OK to aim high and work on general-purpose infrastructure software, but then explain how users can make themselves heard without having to become contributors themselves. Being &amp;ldquo;open&amp;rdquo; is not enough.&lt;/p&gt;</description></item>
  <item>
   <title>Data science in ancient Greece</title>
   <link>http://blog.khinsen.net/posts/2017/12/19/data-science-in-ancient-greece/?utm_source=science&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2017-12-19-data-science-in-ancient-greece</guid>
   <pubDate>Tue, 19 Dec 2017 11:58:13 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;Data science is usually considered a very recent invention, made possible by electronic computing and communication technologies. Some consider it the &lt;a href="https://www.microsoft.com/en-us/research/publication/fourth-paradigm-data-intensive-scientific-discovery/"&gt;fourth paradigm&lt;/a&gt; of science, suggesting that it came after three other paradigms, though the whole idea of distinct paradigms remains controversial. What I want to point out in this post is that the principles of data science are much older than most of today&amp;rsquo;s practitioners imagine. Let me introduce you to &lt;a href="https://en.wikipedia.org/wiki/Apollonius_of_Perga"&gt;Apollonius&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Hipparchus"&gt;Hipparchus&lt;/a&gt;, and &lt;a href="https://en.wikipedia.org/wiki/Ptolemy"&gt;Ptolemy&lt;/a&gt;, who applied these principles about 2000 years ago.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;The focus of interest of these early researchers was a topic that had kept humanity busy for quite a while already, all over the world: the motion of heavenly bodies. The main motivation was making predictions for the near future. The configuration of the stars and planets was widely believed to have an impact on human affairs (a belief we call astrology today), so knowing them in advance was of obvious interest. They had astronomical observations at their disposal, but numbers alone are not sufficient to make predictions. You also need a model for extrapolating the numbers to the future.&lt;/p&gt;

&lt;p&gt;The tool that Apollonius, Hipparchus, Ptolemy, and probably many others, developed and improved to near perfection was &lt;a href="https://en.wikipedia.org/wiki/Deferent_and_epicycle"&gt;epicycles&lt;/a&gt;: a model for the orbit of a heavenly body consisting of a superposition of circles, with each circle&amp;rsquo;s center moving along a bigger circle&amp;rsquo;s circumference. Epicycles are similar in spirit to Fourier series. Any periodic orbit can be described as a superposition of circular motions. Given enough data, one can fit an epicycle model and make predictions. But since the epicycle model does not contain any physics, it doesn&amp;rsquo;t come with any safeguards against mistakes. Epicycles can equally well describe real and completely unrealistic orbits, and therefore the quality of the data is very important.&lt;/p&gt;

&lt;p&gt;Today&amp;rsquo;s data science works much the same. Very general models, such as neural networks, are fitted to large datasets and then used to make predictions. Again the models contain very few assumptions about underlying laws of nature. They are by design very general (see e.g this &lt;a href="http://neuralnetworksanddeeplearning.com/chap4.html"&gt;visual proof&lt;/a&gt; that neural networks can compute any function) in order to capture any kind of regularity in the input datasets. As for epicycles, data quality is important, which is why data scientist invest a significant effort into cleaning up the raw data they work on.&lt;/p&gt;

&lt;p&gt;Aside from the obvious technological aspects and the associated change of scale in the size of datasets, the main improvement of today&amp;rsquo;s data science on epicyle models for orbits is even more generality. Early astronomers had periodicity baked into their models from the start. Neural networks (and other models used in data science) could predict the motion of heavenly bodies with even less theoretical input. However, it is important to realize that every model imposes &lt;em&gt;some&lt;/em&gt; a priori assumptions, even if, as in the case of neural networks, these assumptions are not fully understood and therefore not formalized. Seen in this light, the improvement of modern data science over epicycles is gradual rather than fundamental.&lt;/p&gt;

&lt;p&gt;Adopting an historical perspective, data science turns out to mark the &lt;em&gt;beginning&lt;/em&gt; of scientific disciplines rather than their refinement. It permits the very first step from raw observations to a description of regularities. Connecting these regularities to known more fundamental principles, or even discovering &lt;em&gt;new&lt;/em&gt; fundamental principles as in the case of Newton&amp;rsquo;s laws for celestial mechanics, can only happen afterwards.&lt;/p&gt;

&lt;p&gt;Perhaps a more fundamental distinction than the one between experiment and theory (plus, according to some, simulation and data science) is the one between &lt;em&gt;data-driven&lt;/em&gt; and &lt;em&gt;model-driven&lt;/em&gt; science. Data-driven science starts from observations and searches for regularities using generic models. Model-driven science takes more advanced problem-specific models and aims at evaluating and improving their quality on one hand, and explore their consequences on the other hand. In terms of day-to-day research activities, data-driven science collects observations that promise to be interesting and uses statistical methods to interpret them. Model-driven science has theoreticians exploring models and experimentalists asking Nature specific questions arising from this exploration. The oldest and best-known scientific disciplines, i.e. physics and chemistry, are primarily model-driven today, which may contribute to the impression that data-driven science is new. As the epicycle example shows, this is really just a lack of historical perspective.&lt;/p&gt;</description></item></channel></rss>