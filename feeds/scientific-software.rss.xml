<?xml version="1.0" encoding="utf-8"?> 
<rss version="2.0">
 <channel>
  <title>Konrad Hinsen's Blog: Posts tagged 'scientific software'</title>
  <description>Konrad Hinsen's Blog: Posts tagged 'scientific software'</description>
  <link>http://blog.khinsen.net/tags/scientific-software.html</link>
  <lastBuildDate>Mon, 18 May 2020 08:54:19 UT</lastBuildDate>
  <pubDate>Mon, 18 May 2020 08:54:19 UT</pubDate>
  <ttl>1800</ttl>
  <item>
   <title>An open letter to software engineers criticizing Neil Ferguson's epidemics simulation code</title>
   <link>http://blog.khinsen.net/posts/2020/05/18/an-open-letter-to-software-engineers-criticizing-neil-ferguson-s-epidemics-simulation-code/?utm_source=scientific-software&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2020-05-18-an-open-letter-to-software-engineers-criticizing-neil-ferguson-s-epidemics-simulation-code</guid>
   <pubDate>Mon, 18 May 2020 08:54:19 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;Dear software engineers,&lt;/p&gt;

&lt;p&gt;&lt;a href="https://lockdownsceptics.org/code-review-of-fergusons-model/"&gt;Many&lt;/a&gt; &lt;a href="https://www.telegraph.co.uk/technology/2020/05/16/coding-led-lockdown-totally-unreliable-buggy-mess-say-experts/"&gt;of you&lt;/a&gt; &lt;a href="https://chrisvoncsefalvay.com/2020/05/09/imperial-covid-model/"&gt;were&lt;/a&gt; &lt;a href="https://github.com/mrc-ide/covid-sim/issues"&gt;horrified&lt;/a&gt; at the sight of &lt;a href="https://github.com/mrc-ide/covid-sim"&gt;the C++ code that Neil Ferguson and his team wrote to simulate the spread of epidemics&lt;/a&gt;. I feel with you. The only reason why I am less horrified than you is that I have seen a lot of similar-looking code before. It is in fact quite common in scientific computing, in particular in research projects that have been running for many years. But like you, I don&amp;rsquo;t have much trust in that code being a faithful and trustworthy implementation of the epidemiological models that it is supposed to implement, and I don&amp;rsquo;t want to defend bad code in science.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;However, many of your specific criticisms show a lack of familiarity with today&amp;rsquo;s academic research. This code is not the sole result of 13 years of tax-payer-funded research. The core of that research is building and applying the model it implemented by the code, the code itself is merely a means to this end. The scientists who wrote this horrible code most probably had no training in software engineering, and no funding to hire software engineers. And the senior or former scientists who decided to give tax-payer money to this research group are probably even more ignorant of the importance of code for science. Otherwise they would surely have attributed money for software development, and verified the application of best practices.&lt;/p&gt;

&lt;p&gt;But the main message of this letter is something different: it&amp;rsquo;s about &lt;em&gt;your&lt;/em&gt; role in this story. That&amp;rsquo;s of course a collective you, not you the individual reading this letter. It&amp;rsquo;s you, the software engineering community, that is responsible for tools like C++ that look as if they were designed for shooting yourself in the foot. It&amp;rsquo;s also you, the software engineering community, that has made no effort to warn the non-expert public of the dangers of these tools. Sure, you have been discussing these dangers internally, even a lot. But to outsiders, such as computational scientists looking for implementation tools for their models, these discussions are hard to find and hard to understand. There are lots of tutorials teaching C++ to novices, but I have yet to see a single one that starts with a clear warning about the dangers. You know, the kind of warning that every instruction manual for a microwave oven starts with: don&amp;rsquo;t use this to dry your dog after a bath. A clear message saying &amp;ldquo;Unless you are willing to train for many years to become a software engineer yourself, this tool is &lt;em&gt;not&lt;/em&gt; for you.&amp;rdquo;&lt;/p&gt;

&lt;p&gt;As a famous member of your community famously said, &lt;a href="https://a16z.com/2011/08/20/why-software-is-eating-the-world/"&gt;software is eating the world&lt;/a&gt;. That gives you, dear software engineers, a lot of power in modern society. But power comes with responsibility. If you want scientists to construct reliable implementations of models that matter for public health decisions, the best you can do is make good tools for that task, but the very least you must do is put clear warning signs on tools that you do &lt;em&gt;not&lt;/em&gt; want scientists to use - always keeping in mind that scientists are not software engineers, and have neither the time nor the motivation to become software engineers.&lt;/p&gt;

&lt;p&gt;Consider what you, as a client, expect from engineers in other domains. You expect cars to be safe to use by anyone with a driver&amp;rsquo;s license. You expect household appliances to be safe to use for anyone after a cursory glance at the instruction manuals. It is reasonable then to expect &lt;em&gt;your&lt;/em&gt; clients to become proficient in &lt;em&gt;your&lt;/em&gt; work just to be able to use your products responsibly? Worse, is it reasonable to make that expectation tacitly?&lt;/p&gt;

&lt;p&gt;Some of you have helped with a first round of code cleanup, which I think is the most constructive attitude you can adopt in the short term. But this is not a sustainable approach for the future. We can&amp;rsquo;t ask software experts for a code review every time we do something important. We computational scientists need you software engineers to help us build a better future for computer-aided research. Which means pretty much all research, because software has been eating science as well for a while. Can we count on your help?&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;em&gt;PS added 2020&amp;ndash;05&amp;ndash;19T10:30:&lt;/em&gt; This post has provoked a lively discussion not only in the comments below but also &lt;a href="https://twitter.com/khinsen/status/1262307434632282112"&gt;on Twitter&lt;/a&gt;. There are way too many comments for me to reply to each one individually, so I decided to address recurrent topics in this follow-up.&lt;/p&gt;

&lt;p&gt;Many people seem to have read my post as putting the main responsibility for the problems related to the cited simulation code on software engineers. This was most certainly not my intention. Scientists, policy makers, and journalists have all contributed to a less than satisfactory outcome. My open letter is clearly addressed at a particular group of people (software engineers criticizing the Imperial College Covid&amp;ndash;19 simulations on the basis of code quality) and clearly states its focus on the role of software technology, which is what the target audience seems to overlook. A focus is always an arbitrary choice of an author for the sake of brevity or clarity. A glance at the rest of my blog should suffice to show that I do consider computational scientists responsible for their technological choices and their consequences. However, my main intention was not assigning blame for events in the past, but outline what needs to change to prevent similar events in the future.&lt;/p&gt;

&lt;p&gt;The car analogy was another frequent target of critical comments. Cars are a mature technology, in which many professions (engineers, workers, mechanics, driving instructors, drivers, etc.) have well-defined roles and everyone involved has a general understanding of the role of everyone else. Software is an immature technology in which roles remain fuzzy and everyone has an even fuzzier view of which other roles exist and who fills them. The discussion of my open letter has provided ample evidence for this all-encompassing fuzziness.  What we collectively need to work on is turning software into a mature technology. That requires all stakeholders to make their own role views explicit and then negotiate shared role definitions with everyone else. Several commenters have pointed out the emergence of research software engineers (RSEs) as a sign for progress, and I completely agree. But even the role of RSEs remains fuzzy at this time. Should they work a collaborators on research projects, with a particular specialization? Or as occasional consultants or service providers to researchers? Their interaction with the software engineering universe is even less clear. For now it is mostly one-way in that RSEs bring software technology from the outside into research labs. What my letter argues for is an action in the opposite direction: make software technology evolve to adapt to the specific needs of scientists. A big problem is culture clash. In academia, scientists are traditionally on top of the power pyramid and are used to everyone else working for them (even though the top position is now held by managers, but that&amp;rsquo;s a different story). In the tech world, it&amp;rsquo;s software engineers who are kings and used to everyone else, including their clients, obeying their directives. In the worst case, RSEs might find themselves trapped in the valley between two power pyramids. In the ideal case (from my point of view), they will be diplomats working towards a merger of the two kingdoms, with a simultaneous transformation into a democracy.&lt;/p&gt;</description></item>
  <item>
   <title>Wanted: a hierarchically modular software architecture</title>
   <link>http://blog.khinsen.net/posts/2020/05/05/wanted-a-hierarchically-modular-software-architecture/?utm_source=scientific-software&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2020-05-05-wanted-a-hierarchically-modular-software-architecture</guid>
   <pubDate>Tue, 05 May 2020 12:59:25 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;In his 1962 classic &lt;a href="https://www.jstor.org/stable/985254"&gt;&amp;ldquo;The Architecture of Complexity&amp;rdquo;&lt;/a&gt;, Herbert Simon described the hierarchical structure found in many complex systems, both natural and human-made. But even though complexity is recognized as a major issue in software development today, the architecture described by Simon is not common in software, and in fact seems unsupported by today&amp;rsquo;s software development and deployment tools.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;The prime characteristic that Simon identifies in most complex systems is a hierarchical structure. Systems consist of subsystems, which consist of sub-sub-systems, etc. Simon describes the subsystems at each level as &amp;ldquo;nearly decomposable&amp;rdquo;, meaning that the interactions between subsystems are much less important than the interactions between the parts inside a subsystem. I prefer the shorter term &amp;ldquo;modular&amp;rdquo; for this feature, and thus end up with &amp;ldquo;hierarchically modular&amp;rdquo; as my label for the architecture that Simon describes in much detail. I won&amp;rsquo;t repeat his arguments for the ubiquity of such systems, so please read the paper - it&amp;rsquo;s definitely worth it, and it&amp;rsquo;s very clearly written.&lt;/p&gt;

&lt;p&gt;It may seem as if many of today&amp;rsquo;s programming languages propose exactly this kind of architecture for designing software systems, but a critical inspection shows that they don&amp;rsquo;t. To explain where the problem is, I will use Python as an example because it is widely known, but the arguments apply with some modifications to most other languages as well.&lt;/p&gt;

&lt;p&gt;Python&amp;rsquo;s module system is basically a hierarchy of namespaces, with namespaces containing mainly function and class definitions, but also variables referring to arbitrary data objects. Since namespaces are independent, and can contain sub-namespaces, this looks like a perfect match for a hierarchically modular architecture.&lt;/p&gt;

&lt;p&gt;One obstacle is that there is no way to combine independently designed modules into a larger hierarchy. Suppose I want to create a software component called &lt;code&gt;ode_solver&lt;/code&gt; that uses the popular packages &lt;a href="http://numpy.org/"&gt;NumPy&lt;/a&gt; and &lt;a href="http://scipy.org/"&gt;SciPy&lt;/a&gt;. In a hierarchically modular architecture, implementation details of a component, such as the names of the packages it uses, would be hidden from outside view. The packages would become &lt;code&gt;ode_solver.numpy&lt;/code&gt; and &lt;code&gt;ode_solver.scipy&lt;/code&gt;. In real Python, they can only remain &lt;code&gt;numpy&lt;/code&gt; and &lt;code&gt;scipy&lt;/code&gt;, as their authors decided to call them. Independently written software components in Python always live in the globally shared top-level namespace. And since developers are free to modify their packages as they like, this makes the top-level namespace an instance of &lt;a href="https://www.qwant.com/?q=shared%20mutable%20state"&gt;shared mutable state&lt;/a&gt;, universally recognized as problematic in software engineering.&lt;/p&gt;

&lt;p&gt;The shared top-level namespace creates a strong interaction between all components at all levels. Suppose I have another component called &lt;code&gt;visualizer&lt;/code&gt; that also uses NumPy and SciPy, but requires different versions. That component becomes impossible to combine with my &lt;code&gt;ode_solver&lt;/code&gt; because of conflicting version requirements - the well known &lt;a href="https://en.wikipedia.org/wiki/Dependency_hell"&gt;dependency hell&lt;/a&gt;. Another way to look at this is to consider each package&amp;rsquo;s detailed dependency list, with version requirements, as part of its interface.&lt;/p&gt;

&lt;p&gt;The second obstacle is that the full specification of a module&amp;rsquo;s interface (something that&amp;rsquo;s never ever written down in Python) in general includes classes defined by its dependencies. My &lt;code&gt;ode_solver&lt;/code&gt; could, for example, return some value as a NumPy array. That would make NumPy not only a run-time dependency of the code, but also a specification dependency for the interface. If &lt;code&gt;visualizer&lt;/code&gt; expects a NumPy array as the input to one of its functions, I&amp;rsquo;d be in trouble again as the class definition in the two different versions of NumPy might not be the same. And that trouble would not go away if I could migrate NumPy and SciPy inside my component&amp;rsquo;s namespace as suggested above.&lt;/p&gt;

&lt;p&gt;Some readers&amp;rsquo; first reaction is likely to be &amp;ldquo;that&amp;rsquo;s a symptom of bad specifications&amp;rdquo; or &amp;ldquo;that&amp;rsquo;s the trouble you deserve for using a dynamically typed language&amp;rdquo;. However, static typing doesn&amp;rsquo;t solve the problem, it merely shifts it from run time to compile time. It&amp;rsquo;s the types introduced by dependencies that end up in the static interface of a component. The impact on component compatibility is the same. And if that&amp;rsquo;s a symptom of bad design, then good design is not only rare but also actively discouraged by today&amp;rsquo;s software development tools. The only way out I can see is to create wrapper types and wrapper functions in the component that hide the implementation in terms of dependencies. Hands up if you find that idea appealing!&lt;/p&gt;

&lt;p&gt;The only programming language I know of that does not suffer from this problem is &lt;a href="https://www.unisonweb.org/"&gt;Unison&lt;/a&gt;, which refers to functions and data types &lt;a href="https://www.unisonweb.org/2020/04/10/reducing-churn/"&gt;via hashes rather than names&lt;/a&gt;. It&amp;rsquo;s a very young language, so it&amp;rsquo;s too early to say how this feature will change software architecture on a larger scale.&lt;/p&gt;

&lt;p&gt;Programming languages are not the only realm in which we can try to construct hierarchically modular software. It would in fact be preferable to do so at a language-neutral level, to escape from the silos that languages tend to represent. I&amp;rsquo;d love to be able to combine a component written in Python with a component written in R! So maybe we should try to make hierarchically modular assemblies at the level of compiled binaries.&lt;/p&gt;

&lt;p&gt;One candidate would then be Linux&amp;rsquo; &lt;a href="https://en.wikipedia.org/wiki/Executable_and_Linkable_Format"&gt;Executable and Linkable Format&lt;/a&gt; (ELF), which covers several types of binary files: executables, object files, shared libraries, and more. But there is no kind of ELF file that could represent hierarchically composable modules, as far as I can see. There&amp;rsquo;s no way to combine two shared libraries into a bigger shared library, nor two executables into a larger executable, and moreover every executable has a global namespace that would create the same issues that I outlined above for Python. You can&amp;rsquo;t have an executable that includes or refers to two different versions of the &lt;a href="https://zlib.net/"&gt;zlib library&lt;/a&gt;, for example.&lt;/p&gt;

&lt;p&gt;The only approach that looks doable in the Unix world is working at the process level. A software component is then a process based on an executable, and data between processes is exchanged via files or sockets. Choosing a clever hash-based naming scheme (as done by &lt;a href="https://nixos.org/"&gt;Nix&lt;/a&gt; and &lt;a href="https://guix.gnu.org/"&gt;Guix&lt;/a&gt;) makes it possible to keep any combination of versions accessible in parallel. Several processes could be managed as child processes by a superprocess, which would thus represent a component one level up in the hierarchy. In the Web world, a very similar setup could be constructed by making each component a Web service. There isn&amp;rsquo;t much tool support for such techniques, but perhaps the most important obstacle is efficiency issues in the communication between components, which would require serialization and either file storage or network communication.&lt;/p&gt;

&lt;p&gt;The main merit of the two approaches I have outlined in the last paragraph is that they can accommodate legacy code and systems, unlike the starting-from-scratch approach of Unison. With a bit of luck, improved tooling and optimization could turn the process/service-based approach into a viable technique for some types of real-life application, while Unison and perhaps others introduce the same basic idea at the programming language end of the scale of software component technologies. And then, if the concept turns out to be successful for taming software complexity, it might become the norm after a few decades. So far for my daily dose of wishful thinking!&lt;/p&gt;

&lt;p&gt;Finally, let me reveal my motivation for writing this post: I hope that someone will prove me wrong. I&amp;rsquo;d love to see a comment pointing out that I am simply not aware of the right tools and techniques. And you get bonus points for references to actual hierarchically modular software systems that work!&lt;/p&gt;</description></item>
  <item>
   <title>Emacs as a malleable system</title>
   <link>http://blog.khinsen.net/posts/2020/04/03/emacs-as-a-malleable-system/?utm_source=scientific-software&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2020-04-03-emacs-as-a-malleable-system</guid>
   <pubDate>Fri, 03 Apr 2020 13:56:04 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;Malleable systems are software systems that are designed to be modified and extended by their users, eliminating the usually strict borderline between developers and users. Making scientific software more malleable is a goal that I have been pursuing for 25 years, starting with a shift from Fortran to Python as my main programming language, and a simultaneous shift from writing programs to writing toolkits, such as my &lt;a href="http://dirac.cnrs-orleans.fr/MMTK/"&gt;Molecular Modelling Toolkit&lt;/a&gt; first published in 1997. Therefore I was pleased to discover the &lt;a href="https://malleable.systems/"&gt;Malleable Systems Collective&lt;/a&gt;, which has just published a &lt;a href="https://malleable.systems/blog/2020/04/01/the-most-successful-malleable-system-in-history/"&gt;post&lt;/a&gt; in which I examine what is probably the most successful malleable system in the history of software: Emacs. If you care about users having more influence on their software, check out their site!&lt;/p&gt;</description></item>
  <item>
   <title>Knowledge distillation in computer-aided research</title>
   <link>http://blog.khinsen.net/posts/2018/10/21/knowledge-distillation-in-computer-aided-research/?utm_source=scientific-software&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2018-10-21-knowledge-distillation-in-computer-aided-research</guid>
   <pubDate>Sun, 21 Oct 2018 16:28:39 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;There is an important and ubiquitous process in scientific research that scientists never seem to talk about. There isn&amp;rsquo;t even a word for it, as far as I now, so I&amp;rsquo;ll introduce my own: I&amp;rsquo;ll call it &lt;em&gt;knowledge distillation&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In today&amp;rsquo;s scientific practice, there are two main variants of this process, one for individual research studies and one for managing the collective knowledge of a discipline. I&amp;rsquo;ll briefly present both of them, before coming to the main point of this post, which is the integration of &lt;em&gt;digital&lt;/em&gt; knowledge, and in particular software, into the knowledge distillation process.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;The first variant is performed by individual researchers or closely collaborating teams who, starting from the raw information of their lab notebooks, describing methods applied and results obtained, write a journal article summarizing all of this information into an illustrated narrative that is much easier to digest for their fellow scientists. This narrative contains what the authors consider the essence of their work, leaving out what they consider technical details. Moreover, the narrative places the work into its wider scientific context. In a second step, the authors condense the article into an even smaller abstract, supposed to tell readers at a glance if the article is of interest to them without going into any details. This process can be illustrated as a pyramid:&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="./knowledge-pyramid-1.svg" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;At the bottom we have all the gory details, one level up the distilled version for communication, and at the top the minimal summary for first contact with a potential reader. It is not uncommon to have an additional layer between the bottom two, often published as &amp;ldquo;supplementary material&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Whereas authors work from the bottom to the top of this pyramid, readers work down from the top, gaining a more detailed understanding at each step. Until not so long ago, this was a two-step process: after the abstract, they could move on to the paper, but after that they had to contact the authors for obtaining more details, and the authors might well not care to reply. The Open Science movement has made some progress in pushing for more transparency by making deeper information layers available for critical inspection, in particular raw datasets and the source code for the software used to process them. The situation is very much in flux as various scientific disciplines are working out which information can and should be shared, and how. The maximal level of openness is known as &lt;a href="https://en.wikipedia.org/wiki/Open-notebook_science"&gt;Open Notebook science&lt;/a&gt;, which basically means making the whole pyramid public. Note, however, that giving access to the base of pyramid does not make the knowledge distillation steps superfluous. Readers would succumb to information overload if exposed to all the details without a proper introduction in the form of distilled knowledge. In fact, &lt;em&gt;most&lt;/em&gt; readers don&amp;rsquo;t want to anything else than the distilled version.&lt;/p&gt;

&lt;p&gt;The second variant of knowledge distillation is performed collectively by domain experts who summarize the literature of their field into review articles and then into monographs or textbooks for students. The pyramid diagram is very similar to the first variant&amp;rsquo;s:&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="./knowledge-pyramid-2.svg" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;It&amp;rsquo;s really just the same process at another scale: knowledge transfer about a discipline, rather than about a specific study.&lt;/p&gt;

&lt;p&gt;So far for good old science - let&amp;rsquo;s move to the digital age. The base of our first pyramid now contains code and digital datasets. Some of the code was written by the authors of the study for this specific project and typically takes the form of scripts, workflows, or notebooks. This is complemented by the dependencies of this project-specific code - see my &lt;a href="http://blog.khinsen.net/posts/2017/01/13/sustainable-software-and-reproducible-research-dealing-with-software-collapse/"&gt;post on software collapse&lt;/a&gt; for an analysis of the full software stack. Full openness requires making all of this public, with computational reproducibility serving as a success indicator. If other researchers can re-run the software and get the same results, they possess all the information one could possibly ask for, from a computational point of view.&lt;/p&gt;

&lt;p&gt;But as with Open Notebook science, making all the details open is not sufficient. Readers will again succumb to information overload when exposed to a complex software stack and digital datasets whose precise role in the study is not clear. Information overload is even a much more serious problem with software because the amount of detail that software source code contains is orders of magnitude bigger than what can be written down in a lab notebook.&lt;/p&gt;

&lt;p&gt;So how do we distill the scientific knowledge embedded in software? The bad news is that we don&amp;rsquo;t yet have any good techniques. What we find in journal articles when it comes to describing computational methods is very brief summaries in plain English, closer to the abstract level than to the journal article level. As a consequence, computational methods remain impenetrable to the reader who does not have prior experience with the software that has been applied. There is no way to work down the pyramid, readers have to acquire the base level skills on their own. Worse, there is no way to stop at the middle level of the pyramid and yet have a clear understanding of what is going on.&lt;/p&gt;

&lt;p&gt;The recent years have seen a flurry of research and development concerning the publication of software and computations. One main focus has been the reproducibility of results, another the sustainability of scientific software development, and a third one the readability of computational analyses. This last focus has most notably led to the development of computational notebooks (such as Jupyter, Rmarkdown, Emacs/Org-mode and many more), which embed code and results in a narrative providing context and explanations. Notebooks are occasionally put forward as &amp;ldquo;the paper of the future&amp;rdquo;, but in view of the knowledge pyramid, that&amp;rsquo;s not what they are. They are closer to the digital age equivalent of lab notebooks, especially when combined with version control to capture the time evolution of their contents. The real paper of the future must contain a &lt;em&gt;distilled&lt;/em&gt; version of the source code.&lt;/p&gt;

&lt;p&gt;It is interesting to examine why notebooks have been so successful in some scientific domains. First of all, they are a much better human-readable presentation of source code than anything we had before, with the exception of the related idea of literate programming which I expect to see a come-back as well. Next, in domains where computational studies tend to be linear sequences of well-known standard operations, such as statistical analyses, the notebook is very similar to a distilled computational protocol, because the technical details are mostly hidden in libraries. These libraries also contain significant scientific knowledge, but because these methods are well-known, they have in a way been distilled in the form of textbooks.&lt;/p&gt;

&lt;p&gt;More generally, though, notebooks contain both too little and too much information to qualify as distilled descriptions of computational studies. Too little because much scientific knowledge is hidden in the notebook&amp;rsquo;s dependencies, which are not documented at the same level of readability (which is why I believe that literate programming has a future). Too much because they still expose technical details to the reader that is more a hindrance than a help for understanding.&lt;/p&gt;

&lt;p&gt;How, then, should the paper of the future present distilled computational knowledge? I see three main requirements:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;It must be possible to explain and discuss individual models, approximations, or algorithms without the constraints of an efficient working implementation.&lt;/li&gt;
 &lt;li&gt;These models, approximations, and algorithms must be presented in a sufficiently precise form that automatic verification procedures can ensure that the source code at the base level of the pyramid actually implements them.&lt;/li&gt;
 &lt;li&gt;Suitable user interfaces must allow a reader to explore these models, approximations, and algorithms through concrete examples.&lt;/li&gt;&lt;/ol&gt;

&lt;p&gt;The first requirement says that clarity of exposition must take absolute precedence over any technical considerations of software technology. The intrinsic complexity of computational methods makes understanding hard enough, so everything possible must be done to keep accidental complexity out of the way.&lt;/p&gt;

&lt;p&gt;The second requirement ensures that the conformity between the distilled and the detailed representations of a computational protocol can be verified by computers rather than by humans. Humans aren&amp;rsquo;t very good at checking that two complex artifacts are equivalent.&lt;/p&gt;

&lt;p&gt;The third requirement is motivated by the observation that a real understanding of a computational method, which is usually too lengthy to be actually performed manually, requires both reading code and observing how it processes simple test cases. Observation is not limited to the final outcome, it may well be necessary to provide access to intermediate results.&lt;/p&gt;

&lt;p&gt;To get an idea of what &amp;ldquo;suitable user interfaces&amp;rdquo; might look like, it&amp;rsquo;s worth looking at the &lt;a href="https://explorabl.es/"&gt;explorable explanations&lt;/a&gt; and the &lt;a href="http://www.complexity-explorables.org/"&gt;Complexity Explorables&lt;/a&gt; Web sites. Note, however, that none of these exploration user interfaces provide easy access to a precise formulation of the underlying models or algorithm. They exist in the form of JavaScript source code embedded in the Web site, but that&amp;rsquo;s not exactly a reader-friendly medium of expression. Another interesting line of development is happening in the &lt;a href="https://pharo.org/"&gt;Pharo&lt;/a&gt; community (Pharo being a modern descendent of Smalltalk), e.g. the idea of &lt;a href="http://scg.unibe.ch/research/moldableinspector"&gt;moldable inspectors&lt;/a&gt;, which are user interfaces specifically designed to explore a particular kind of object, which in the O-O tradition combines code and data.&lt;/p&gt;

&lt;p&gt;Back to requirements 1 and 2: we want a precise and easily inspectable description that can be embedded into an explanatory narrative. We also want to be sure that it actually corresponds to what the user interface lets us explore, and to what the software implementation applies efficiently to real-world problems. I am not aware of any existing technology that can fulfill this role, although there many that were designed with somewhat different goals in mind that can serve as guidelines, in particular the various &lt;a href="https://en.wikipedia.org/wiki/Modeling_language"&gt;modeling&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/Specification_language"&gt;specification languages&lt;/a&gt;.&lt;/p&gt;

&lt;div class="figure"&gt;&lt;img src="./knowledge-pyramid-3.svg" alt="" /&gt;
 &lt;p class="caption"&gt;&lt;/p&gt;&lt;/div&gt;

&lt;p&gt;My own research into this problem had led to the concept of &lt;a href="http://sjscience.org/article?id=527"&gt;digital scientific notations&lt;/a&gt;, and I am currently designing such a notation for physics and chemistry, called &lt;a href="https://github.com/khinsen/leibniz"&gt;Leibniz&lt;/a&gt;. A &lt;a href="https://peerj.com/articles/cs-158/"&gt;first report&lt;/a&gt; on this research has been published earlier this year. Leibniz is mainly inspired by traditional mathematical notation concerning the way it is embedded into a narrative, and from specification languages in terms of semantics. Some relevant features of Leibniz for expressing distilled knowledge are&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;
  &lt;p&gt;Its highly declarative nature. Leibniz code consists of short declarations that can be written down in (nearly) arbitrary order, making them easy to embed into a narrative, much like mathematical expressions and equations.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;Its foundation in term rewriting (the same foundation adopted by most computer algebra systems). Among other advantages, this allows Leibniz code to concentrate on one aspect of a model or algorithm while leaving other aspects unspecified.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;Its restriction to a single universal (but often inefficient) data structure.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;These features mainly address requirement 1. As for requirement 2, Leibniz uses XML for its syntax and has very simple semantics, making it easy to write libraries that read and execute Leibniz code which in turn make it easy to integrate Leibniz into scientific software of all kinds. Only Leibniz development environments have to deal with the more complex user-facing syntax requiring a specific parser.&lt;/p&gt;

&lt;p&gt;Leibniz does not try to address requirement 3, but since it meets requirement 2, it doesn&amp;rsquo;t get in the way of people wishing to build exploration and inspection user interfaces for Leibniz-based models and algorithms.&lt;/p&gt;

&lt;p&gt;Leibniz is still very much experimental, and I am not at all sure that it will turn out to be useful in its current form. In fact, I am almost certain that it will require modification to be of practical use. If that doesn&amp;rsquo;t scare you off, have a look at the &lt;a href="http://khinsen.net/leibniz-examples/"&gt;example collection&lt;/a&gt; to get an idea of what Leibniz can do and what it looks like. Feedback of any kind is more than welcome!&lt;/p&gt;</description></item>
  <item>
   <title>Literate computational science</title>
   <link>http://blog.khinsen.net/posts/2018/07/26/literate-computational-science/?utm_source=scientific-software&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2018-07-26-literate-computational-science</guid>
   <pubDate>Thu, 26 Jul 2018 14:44:31 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;Since the dawn of computer programming, software developers have been aware of the rapidly growing complexity of code as its size increases. Keeping in mind all the details in a few hundred lines of code is not trivial, and understanding someone else&amp;rsquo;s code is even more difficult because many higher-level decisions about algorithms and data structures are not visible unless the authors have carefully documented them and keep those comments up to date.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;The main angle of attack to keep software source code manageable has been the development of ever more sophisticated programming languages and development paradigms, but it is not the only one. Another approach was initiated by Donald Knuth&amp;rsquo;s invention of &lt;a href="http://literateprogramming.com/"&gt;literate programming&lt;/a&gt;. Its basic idea is to invert the roles of code and documentation. Rather than adding doxumentation as annotations to the code, literate programming puts an explanatory narrative about the software at the center of the software author&amp;rsquo;s attention. Code snippets are embedded into this narrative, much like mathematical formulas are embedded into scientific articles and textbooks.&lt;/p&gt;

&lt;p&gt;Literate programming never gained much popularity, for reasons that, to the best of my knowledge, have never been explored systematically. Insufficient tool support is often cited as an obstacle, but I suspect that the mismatch between the structure of the narrative and the language-imposed structure of the code is equally problematic. Programmers need to name code blocks and then assemble them into valid source code by hand. My own experience is that it&amp;rsquo;s usually easier to write and test the code first and then re-create it as a literate program, but this doesn&amp;rsquo;t lead to code that naturally fits the narrative.&lt;/p&gt;

&lt;p&gt;The main argument in support of this suspicion is the much higher popularity of a variant of literate programming that both adds and removes features compared to Knuth&amp;rsquo;s original system. Computational notebooks (implemented e.g. by &lt;a href="https://jupyter.org/"&gt;Jupyter&lt;/a&gt;) document a computation rather than a piece of software. In addition to code, they embed input data and results into the narrative, but they also restrict code to a linear assembly of code cells executed in sequence. This limitation removes the need to name and assemble code blocks.&lt;/p&gt;

&lt;p&gt;An idea I have been exploring recently is to take another step towards letting the explanatory narrative take center stage, by designing a formal language specifically for embedding into such a narrative. However, my language called &lt;a href="https://github.com/khinsen/leibniz"&gt;Leibniz&lt;/a&gt; is not a programming language. I call it a digital scientific notation to emphasize its intended use in the documentation of scientific models and methods, but in terms of computer science terminology it is a &lt;a href="https://en.wikipedia.org/wiki/Specification_language"&gt;specification language&lt;/a&gt; designed for models expressed in terms of equations and algorithms. Leibniz code &lt;em&gt;must&lt;/em&gt; be embedded into a narrative, although the Leibniz authoring environment also extracts a machine-readable version as an XML file for easy processing by scientific software.&lt;/p&gt;

&lt;p&gt;For getting an overview of Leibniz, I suggest to look first at a &lt;a href="http://khinsen.net/leibniz-examples/examples/leibniz-by-example.html"&gt;simple example&lt;/a&gt;, and then read my &lt;a href="https://peerj.com/articles/cs-158/"&gt;paper&lt;/a&gt; describing Leibniz and the problems it is designed to solve, which just appeared in PeerJ CompSci (Open Access like all of PeerJ). The explanations in the paper should prepare you for a look at the currently &lt;a href="http://khinsen.net/leibniz-examples/examples/mass-on-a-spring.html"&gt;most extensive example&lt;/a&gt;, which documents, for a toy problem, the full path of assumptions and approximations that lead from a theoretical framework (Newton&amp;rsquo;s equations of motion) to a numerical algorithm, with all models along the way being machine-readable.&lt;/p&gt;

&lt;p&gt;As the paper explains, Leibniz is best described as a research prototype at the current stage. It has known limitations that make its application to complex real-world problems a bit challenging. However, I am confident that these limitations can be overcome, and that Leibniz will be suitable for a wide range of scientific models and methods, starting with mathematical equations and ending with literate workflows. As Silicon Valley startups would say, make sure you won&amp;rsquo;t be left behind by the Leibniz revolution!&lt;/p&gt;</description></item>
  <item>
   <title>Scientific software is different from lab equipment</title>
   <link>http://blog.khinsen.net/posts/2018/05/07/scientific-software-is-different-from-lab-equipment/?utm_source=scientific-software&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2018-05-07-scientific-software-is-different-from-lab-equipment</guid>
   <pubDate>Mon, 07 May 2018 07:40:35 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;My most recent paper submission (&lt;a href="https://peerj.com/preprints/26633/"&gt;preprint&lt;/a&gt; available) is about improving the verifiability of computer-aided research, and contains many references to the related subject of reproducibility. A reviewer asked the same question about all these references: isn&amp;rsquo;t this the same as for experiments done with lab equipment? Is software worse? I think the answers are of general interest, so here they are.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;First of all, an inevitable remark about terminology, which is still far from standardized (see &lt;a href="https://arxiv.org/abs/1802.03311"&gt;this preprint&lt;/a&gt; and &lt;a href="https://doi.org/10.3389%2Ffninf.2017.00076"&gt;this article&lt;/a&gt; for two recent contributions to the controversy). I will use the term &amp;ldquo;computational reproducibility&amp;rdquo; in its historically first sense introduced by Claerbout in 1992, because it seems to me that this is currently the dominant usage. &lt;em&gt;Reproducing&lt;/em&gt; a computation thus means running the same software on the same data, though it&amp;rsquo;s usually done by a different person using a different computer. In contrast, &lt;em&gt;replication&lt;/em&gt; refers to solving the same problem using different software. This terminological subtlety matters for the following discussion, because experimental reproducibility is actually more similar to replicability, rather than reproducibility, in the computational case.&lt;/p&gt;

&lt;p&gt;There are two aspects in which I think scientific software differs significantly from lab equipment:&lt;/p&gt;

&lt;ol&gt;
 &lt;li&gt;Its characteristics as a human-made artifact&lt;/li&gt;
 &lt;li&gt;Its role in the process of doing science.&lt;/li&gt;&lt;/ol&gt;

&lt;h2 id="software-is-more-complex-and-less-robust-than-lab-equipment"&gt;Software is more complex and less robust than lab equipment&lt;/h2&gt;

&lt;p&gt;The first point I raised in my paper is the epistemic opacity of automated computation. Quote:&lt;/p&gt;

&lt;blockquote&gt;
 &lt;p&gt;The overarching issue is that performing a computation by hand, step by step, on concrete data, yields a level of understanding and awareness of potential pitfalls that cannot be achieved by reasoning more abstractly about algorithms. As one moves up the ladder of abstraction from manual computation via writing code from scratch, writing code that relies on libraries, and running code written by others, to having code run by a graduate student, more and more aspects of the computation fade from a researcher&amp;rsquo;s attention. While a certain level of epistemic opacity is inevitable if we want to delegate computations to a machine, there are also many sources of accidental epistemic opacity that can and should be eliminated in order to make scientific results as understandable as possible.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The reviewer asks: isn&amp;rsquo;t this the same as when doing experiments using lab equipment constructed by somebody else? My answer is no.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s do a little thought experiment, introducing Alice and Bob as virtual guinea pigs. Alice is an experienced microscopist, Bob is an experienced computational scientist. We give Alice a microscope she hasn&amp;rsquo;t seen before, and ask her to evaluate if it is suitable for her research. We give Bob a simulation program (with source code and documentation) that he hasn&amp;rsquo;t seen before, and ask him the same question.&lt;/p&gt;

&lt;p&gt;My expectation is that Alice will go off an do some tests with samples that she knows well, and perhaps do some measurements on the microscope. After that, she will tell us for which aspects of her work she can use this new microscope. Meanwhile, Bob will be scratching his head while trying to figure out how to deal with our question.&lt;/p&gt;

&lt;p&gt;One reason for the difference is that a microscope is a much simpler artifact than a simulation program. While it is certainly difficult to design and produce a good microscope, from a user&amp;rsquo;s perspective its characteristics can be described by a handful of parameters, and its quality can be evaluated by a series of test observations. Software, on the contrary, can do almost anything. A typical simulation program has lots of options, whose precise meaning isn&amp;rsquo;t always obvious from its documentation. More importantly, no two simulation programs have identical options. Even the most experienced user of simulation software A falls back to near-novice status when given simulation software B.&lt;/p&gt;

&lt;p&gt;A more subtle difference is that microscopes, and lab equipment in general, are designed to be robust against small production defects and small variations of environmental conditions. Such small variations cause only small changes in the generated images. With software, on the other hands, all bets are off. A one-character mistake in the source code can cause the program to crash, but also to produce arbitrarily different numbers. In fact, there is no notion of similarity and thus of small variations for software. For a more detailed discussion, see my &lt;a href="http://doi.ieeecomputersociety.org/10.1109/MCSE.2016.67"&gt;CiSE article&lt;/a&gt; on this topic. This is why you can evaluate the quality of a microscope using a few judiciously chosen samples, whereas no amount of test runs can assure you that a piece of software is free of bugs. Unless you can afford to test &lt;em&gt;all possible&lt;/em&gt; inputs, of course, but then you don&amp;rsquo;t really need the software.&lt;/p&gt;

&lt;p&gt;These two differences explain why Alice knows how to evaluate the microscope, whereas Bob doesn&amp;rsquo;t know where to start. He might look at the documentation and the test cases to see if the program is meant to be used for the kind of work he does. But the documentation almost certainly lacks some important details of the approximations that are made in the code and that matter for Bob&amp;rsquo;s work. Moreover, he would still have to check that the software has no serious bugs related to the functionality he plans to use. Without knowing the implemented algorithms in detail, he cannot even anticipate what bugs to watch out for.&lt;/p&gt;

&lt;p&gt;Bob could also choose a very different approach and judge the software by quality standards from software engineering. Is the code well structured? Does it have unit and integration tests? These are the criteria that software journal ask their reviewers to evaluate (e.g. the &lt;a href="http://dx.doi.org/10.6084/m9.figshare.795303"&gt;Journal of Open Research Software&lt;/a&gt; or the &lt;a href="http://joss.theoj.org/about#reviewer_guidelines"&gt;Journal of Open Source Software&lt;/a&gt;). Statistically, they are probably related to the risk of encountering bugs (if anyone knows about research into this question, please leave a comment!). But even the most meticulous developers make mistakes, and, more importantly, may have different applications in mind than those that Bob cares about.&lt;/p&gt;

&lt;p&gt;Finally, Bob could do what in my experience (and also according to &lt;a href="https://arxiv.org/abs/1605.02265v1"&gt;this study&lt;/a&gt; ) most scientists do in choosing research software: they use what their colleagues use. Bob would then send a few emails asking if anyone he knows uses this software and is happy with it. This is a reasonable approach if you can assume that your colleagues, or at least a sizable fraction of them, are in a better position to judge the suitability of a piece of software than yourself. But if everyone adopts this approach, it becomes a popularity contest with little intrinsic value (see &lt;a href="https://doi.org/10.1126%2Fscience.1231535"&gt;this paper&lt;/a&gt; for a detailed example). In any case, it is not a way to actually answer our question.&lt;/p&gt;

&lt;p&gt;In the end, if you really want to know if your software does what you expect it to do, you have to go through every line of the source code until you understand what it does. You are then at the minimal level of epistemic opacity that you can attain without actually doing the computations by hand. Unfortunately, in the case of complex wide-spectrum software, this is likely to be much more effort than writing your own special-purpose software.&lt;/p&gt;

&lt;p&gt;The solution I propose in my paper is to use human-readable formal specifications as a form of documentation that is rigorous and complete, and can be used as a reference to verify the software against. The idea is to have a statement of the implemented algorithms that is precise and complete but as simple as possible, without being encumbered by considerations such as performance. Note that I don&amp;rsquo;t know if this will turn out to be possible - my work is merely a first step into that direction that, to the best of my knowledge, has not been explored until now.&lt;/p&gt;

&lt;h2 id="software-is-about-models-lab-equipment-is-about-observations"&gt;Software is about models, lab equipment is about observations&lt;/h2&gt;

&lt;p&gt;A popular meme in explaining science describes it as founded on two pillars, experiment and theory. Some people propose to add computation and/or simulation as a third pillar, and data mining as a fourth, although these additions remain controversial. In my opinion, they are misguided by a bad identification of the initial pillars. They are not experiment and theory, but observations and models. We often speak of computational experiments when doing simulations, and there are good reasons for the analogy, but it is important to keep in mind that these are experiments on models, not on natural phenomena.&lt;/p&gt;

&lt;p&gt;Observations provide us with information about nature, and models allow us to organize and generalize this information. In this picture, computation has two roles: evaluating the consequences of a model, and comparing them to observations. Simulation is an example for the first role, data mining for the second. Both of these roles predate electronic computers, they simply received more modest labels such as &amp;ldquo;solving differential equations&amp;rdquo; or &amp;ldquo;fitting parameters&amp;rdquo; in the past.&lt;/p&gt;

&lt;p&gt;In the context of reproducibility and verifiability, it is important to realize that there is no symmetry between these two pillars. Nature is the big unknown that we probe through observations. To do this, we use lab equipment that can never be perfect, for two reasons: first, it is constructed on the basis of our imperfect understanding of nature, and second, our control of matter is limited, so we cannot produce equipment that behaves precisely as we imagine it. Models, on the other hand, are symbolic artifacts that are under our precise control. We can formulate and communicate them without any ambiguity, if only we are careful enough.&lt;/p&gt;

&lt;p&gt;Because of these very different roles of observations and models, computational reproducibility has no analogue in the universe of observations. It is almost exclusively a communication issue, the one exception being the non-determinism in parallel computing that we accept in exchange for getting results faster. Non-determinism aside, if Alice cannot reproduce Bob&amp;rsquo;s computations, that simply means that Bob has not been able or willing to describe his work in enough detail for Alice to re-do it identically. There is no fundamental obstacle to such a description, because models and software are symbolic artifacts. We actually know how to achieve computational reproducibility, but we still need to make it straightforward in practice.&lt;/p&gt;

&lt;p&gt;Similarly, if Alice cannot verify that Bob&amp;rsquo;s computation solves the problem he claims them to solve, this means that Bob has not succeeded in explaining his work clearly enough for Alice to understand what is going on. An unverifiable computation is thus very similar to a badly written article. The big difference in practice is that centuries of experience with writing have lead to accepted and documented standards of good writing style, whereas after a few decades of scientific computing, we still do not know how to expose complex algorithms to human readers in the most understandable way. My paper is a first small step towards developing appropriate techniques.&lt;/p&gt;

&lt;p&gt;Experimental reproducibility, on the other hand, is an ideal that can never be achieved perfectly, because no two setups are strictly the same. Verifiability is equally limited because observations can never be repeated identically, even when done with the same equipment. Reproducibility is a quality attribute much like accuracy, precision, or cost. Tradeoffs between these attributes are inevitable, and have to be made by each scientific discipline as a function of what its main obstacles to progress are.&lt;/p&gt;

&lt;p&gt;Science has been adjusting to the inevitable limits of observations since its beginnings, whereas the issue of incomplete model descriptions has come up only with the introduction of computers permitting to work with complex models. We don&amp;rsquo;t know how yet if non-verifiable models are a real problem or not. However, as a theoretician I am not comfortable with the current situation. Models can be simple or complex, good or bad, grounded in solid theory or ad-hoc, but they should not be fuzzy. In particular not for complex systems, where it is very hard to foresee the consequences of minor changes.&lt;/p&gt;</description></item>
  <item>
   <title>What can we do to check scientific computation more effectively?</title>
   <link>http://blog.khinsen.net/posts/2018/03/07/what-can-we-do-to-check-scientific-computation-more-effectively/?utm_source=scientific-software&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2018-03-07-what-can-we-do-to-check-scientific-computation-more-effectively</guid>
   <pubDate>Wed, 07 Mar 2018 17:15:50 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;It is widely recognized by now that software is an important ingredient to modern scientific research. If we want to check that published results are valid, and if we want to build on our colleagues&amp;rsquo; published work, we must have access to the software and data that were used in the computations. The latest high-impact statement along these lines is a &lt;a href="https://www.nature.com/articles/d41586-018-02741-4"&gt;Nature editorial&lt;/a&gt; that argues that with any manuscript submission, authors should also submit the data &lt;em&gt;and&lt;/em&gt; the software for review. I am all for that, and I hope that more journals will follow.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;However, we must also be aware of the inherent limitations of simply including software in peer review. With the exception of small and focused software, of the kind we typically have in replications submitted to &lt;a href="http://rescience.github.io/"&gt;ReScience&lt;/a&gt; (one of the very few scientific journals that actually does code review), the task of evaluating scientific software is so enormous that asking a single person to do it within two weeks is simply unreasonable. For that reason, journals specialized in software papers, such as the &lt;a href="https://openresearchsoftware.metajnl.com/"&gt;Journal of Open Research Software&lt;/a&gt; or the &lt;a href="https://joss.theoj.org/"&gt;Journal of Open Source Software&lt;/a&gt;, limit the reviewing process to more easily verifiable formal aspects, such as the presence of documentation and the use of appropriate software engineering techniques. Which is, of course, much better than nothing, but it isn&amp;rsquo;t enough.&lt;/p&gt;

&lt;p&gt;A few months ago I wrote about the &lt;a href="http://blog.khinsen.net/posts/2017/05/04/which-mistakes-do-we-actually-make-in-scientific-code/"&gt;kinds of mistakes that we tend to make in scientific computing&lt;/a&gt;. In my experience (I&amp;rsquo;d love to see a systematic study on this), most mistakes are due to discrepancies between what a paper describes and what is actually computed. This covers simple mistakes such as a wrong sign in a computed formula (such as in the widely publicized case of &lt;a href="http://doi.org/10.1126/science.314.5807.1856"&gt;protein structure retractions&lt;/a&gt;), or a typo in the input parameter file for a simulation program, but also more complex situations such as the &lt;a href="http://doi.org/10.1073/pnas.1602413113"&gt;inflated false-positive rates in fMRI studies&lt;/a&gt; that also made it into the headlines of science news. In this case, the fundamental issue was a mismatch between the methods implemented in the software and the methods that would have been appropriate for many typical use cases of the software. Put differently, the users of the software did not fully understand what exactly the software did. They trusted the software authors blindly to do &amp;ldquo;the right thing&amp;rdquo;, whatever that was. And they were probably reinforced in their blind trust by the fact that many of their colleagues used the same software. It&amp;rsquo;s the research version of &amp;ldquo;nobody ever got fired for buying IBM equipment&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Code review is an important step to a better verification of scientific computations, but in the cases I just described its utility is very limited. Neither the wrong sign in the protein crystallography code nor the not-quite-universally-applicable statistical analysis method used by the fMRI software would be detectable by software engineering methods. In the first case, the code would have to be compared to the set of mathematical formulas on which it was based, a task requiring expert knowledge in both crystallography and programming, plus a lot of time - much more than what a reviewer can typically invest. In the second case, code review cannot do anything at all. Only the reviewers of the application papers could have spotted the inappropriateness of the methods - but why should they be expected to be more knowledgeable about the pitfalls than the authors?&lt;/p&gt;

&lt;p&gt;An important but not yet widely recognized aspect of these situations is that today&amp;rsquo;s scientific software incorporates a significant amount of scientific knowledge that is very difficult to access and verify by users and reviewers. The translation of mathematical equations in a paper into efficient computer code is almost a form of encryption from the point of view of scientific knowledge transformation. Extracting equations from software source code is not much easier than extracting source code from compiled binaries.&lt;/p&gt;

&lt;p&gt;But can we do anything about this? I believe we can, but it will require a serious rethinking of the way we use computers to do research. My first explorations in this direction are described in a paper that is now available as a &lt;a href="https://peerj.com/preprints/26633/?td=bl"&gt;PeerJ preprint&lt;/a&gt;. Please have a look, and don&amp;rsquo;t hesitate to ask a question or leave other feedback of any kind!&lt;/p&gt;</description></item>
  <item>
   <title>There is no such thing as software development</title>
   <link>http://blog.khinsen.net/posts/2017/11/09/there-is-no-such-thing-as-software-development/?utm_source=scientific-software&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2017-11-09-there-is-no-such-thing-as-software-development</guid>
   <pubDate>Thu, 09 Nov 2017 11:37:07 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;It&amp;rsquo;s hard to find an aspect of modern life that is not influenced in some way by software. Some of it is very visible, for example the Web browser I start on my computer. Other software is completely invisible, such as the software controlling my car&amp;rsquo;s diesel engine. Some software is safety critical, for example flight control software in airplanes. Other software is used in a much more futile way, such as playing games. I could go on listing characteristics in which different software packages differ, but I will leave it at that - I don&amp;rsquo;t really expect anyone to disagree about the ubiquity and diversity of software in our increasingly digital world.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;Given this diversity, it is surprising how many seem to consider &amp;ldquo;software development&amp;rdquo;, and related terms such as &amp;ldquo;software engineering&amp;rdquo;, as general concepts requiring no further qualification. In particular, plenty of people are happy to discuss in an abstract way how software should best be developed, without any reference to a concrete application domain, project size, expected longevity, etc. Imagine we did the same for the world of atoms, lumping together activities as distinct as chemical synthesis, carpentry, and dental surgery under the label &amp;ldquo;matter manipulation&amp;rdquo;, and starting a discussion about best practices for matter manipulation. I doubt anyone would take such a debate seriously.&lt;/p&gt;

&lt;p&gt;A good example of such an overly abstract discussion is the one about the benefits of static typing. There is a large camp of static typing enthusiasts who claim that static typing is Right with a capital R. They argue that it&amp;rsquo;s always better to have correctness guarantees than not to have them. The implicit assumption is that static typing comes at no cost, which is manifestly false. The main contributions to this cost are 1) additional cognitive load, 2) the need to work around the limitations of a type checker, and 3) additional &lt;a href="http://blog.khinsen.net/posts/2016/03/04/composition-is-the-root-of-all-evil/"&gt;barriers&lt;/a&gt; to the combination of independently developed libraries. As soon as one admits the necessity of a cost-benefit analysis for static typing, it quickly becomes obvious that this can only be done for 1) some specific category of software and 2) a specific type system. The question then becomes: is type system A useful for improving the quality of software in application domain X? A nice example of this point of view is given by Rich Hickey in his &lt;a href="https://www.youtube.com/watch?v=2V1FtfBDsLU"&gt;keynote on &amp;ldquo;Effective Programs&amp;rdquo;&lt;/a&gt;, where he explains why none of the well-known type systems are useful for the kind of software he writes, leading to his decision to design &lt;a href="http://clojure.org/"&gt;Clojure&lt;/a&gt; as a dynamically typed language.&lt;/p&gt;

&lt;p&gt;Focusing software development questions on specific software categories has many potential benefits. Perhaps most importantly, it permits formulating questions in a precise enough way to make them amenable to empirical verification (aka &amp;ldquo;the scientific method&amp;rdquo;), acting at the same time as a safeguard against overly generalizing the conclusions from empirical studies. Moreover, the study of specific use cases is likely to lead to improvements in the methodology. In my example of static typing, it can be expected that once type system designers adopt the habit of thinking about specific software categories, they will design and evaluate type systems for various important application domains, taking into account both the kind of data being processed and the kinds of mistakes one would like to protect oneself against. Even better, once type system designers recognize that there is no single type system to rule them all, they might start to think about how to combine pieces of software written using different type systems. In the end, the three cost factors I mentioned might all end up heavily reduced.&lt;/p&gt;

&lt;p&gt;Since there is a chance that some type system designers are reading this, I&amp;rsquo;ll profit from having their attention and suggest developing a type system for numerical computations, which by some strange coincidence is what I do in my own work. In this application domain, most data represents physical quantities and its low-level representation is &amp;ldquo;float&amp;rdquo; or &amp;ldquo;array of floats&amp;rdquo;. Properties that one could usefully monitor in the course of type checking are dimensions and units, but also positivity or non-zeroness. For array operations, the compatibility of array dimensions is worth a check as well. A static proof of complete absence of such mistakes is probably not doable, but detecting as many mistakes as possible while inserting run-time checks for the rest is probably a very useful compromise. It is also worth considering some important sub-categories of numerical software, in particular the different layers of the scientific software stack that I have &lt;a href="http://blog.khinsen.net/posts/2017/01/13/sustainable-software-and-reproducible-research-dealing-with-software-collapse/"&gt;described before&lt;/a&gt;. The required guarantees are much higher for infrastructure software (layer 2) than for scripts and workflows (layer 4), and infrastructure developers can be expected to invest more effort to ensure correctness. However, this does raise the question of type-checking at the interface between layers, a possible solution being &lt;a href="https://en.wikipedia.org/wiki/Gradual_typing"&gt;gradual typing&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Static typing is merely one example for the importance of looking at specific software application domains, there are many others. The utility of paradigms such as object-oriented or functional programming is also mostly discussed in the abstract, as are the relative merits of development strategies like test-driven or agile development. Finally, some less discussed but practically important questions could get more limelight exposure if formulated more concretely in the context of specific applications. I am thinking for example of the choice between using external libraries and writing one&amp;rsquo;s own code, involving the trade-off between development effort and the long-term risk of uncontrollable dependencies.&lt;/p&gt;</description></item>
  <item>
   <title>Which mistakes do we actually make in scientific code?</title>
   <link>http://blog.khinsen.net/posts/2017/05/04/which-mistakes-do-we-actually-make-in-scientific-code/?utm_source=scientific-software&amp;utm_medium=RSS</link>
   <guid isPermaLink="false">urn:http-blog-khinsen-net:-posts-2017-05-04-which-mistakes-do-we-actually-make-in-scientific-code</guid>
   <pubDate>Thu, 04 May 2017 10:00:16 UT</pubDate>
   <author>Konrad Hinsen</author>
   <description>
&lt;p&gt;Over the last few years, I have repeated a little experiment: Have two scientists, or two teams of scientists, write code for the same task, described in plain English as it would appear in a paper, and then compare the results produced by the two programs. Each person/team was asked to do a maximum amount of verification and testing before comparing to the other person&amp;rsquo;s/team&amp;rsquo;s work.&lt;/p&gt;
&lt;!-- more--&gt;

&lt;p&gt;Let me state the most disturbing outcome of this experiment first: we never found complete agreement between the two programs. Not once. And when we explored to find the cause of the discrepancies, we most often found bugs in &lt;em&gt;both&lt;/em&gt; programs, plus missing details in the description written initially for human readers.&lt;/p&gt;

&lt;p&gt;The two most practically significant experiments of this kind were actual research projects that have since been published:&lt;/p&gt;

&lt;ul&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;a href="http://dx.doi.org/10.1063/1.4821598"&gt;A comparison of reduced coordinate sets for describing protein structure&lt;/a&gt;. For this work, Shuangwei Hu wrote Matlab code, and I wrote the Python code that was &lt;a href="https://doi.org/10.6084/m9.figshare.798825.v1"&gt;ultimately published&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;
 &lt;li&gt;
  &lt;p&gt;&lt;a href="http://dx.doi.org/10.1063/1.4823996"&gt;Model-free simulation approach to molecular diffusion tensors&lt;/a&gt;. In this case, Gerald Kneller wrote Mathematica code, and I wrote the &lt;a href="https://doi.org/10.6084/m9.figshare.808594.v1"&gt;Python version&lt;/a&gt; again.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;

&lt;p&gt;Later on, I did a series of similar experiments with PhD students participating in what can be summarized as advanced Python programming courses. PhD students with limited programming experience are exactly the kind of scientists who write much of the software for research projects. But the setting was &amp;ldquo;exercises in a course&amp;rdquo;, with programming tasks being much simpler, and much better specified, than what the typical research project requires.&lt;/p&gt;

&lt;p&gt;The results of these experiments that I will summarize here are no more than anecdotal evidence. In fact, the initial goal was not to perform an experiment in scientific computing, but to perform better checks on the code for a research project. It would be interesting to do a larger-scale proper study, but that&amp;rsquo;s beyond my means and competence.&lt;/p&gt;

&lt;p&gt;As I already mentioned, there was never complete agreement between the two programs supposed to solve the same problem. In many cases the differences were small, and I suspect many would have brushed them away as caused by uncontrollable round-off, given that all problems were numerical in nature. But upon closer scrutiny, we always found different issues, and got much better agreement after fixing them. This is why I still believe that &lt;a href="https://khinsen.wordpress.com/2015/01/07/why-bitwise-reproducibility-matters/"&gt;bitwise reproducibility matters&lt;/a&gt;. When small numerical differences are inevitable, as they are with today&amp;rsquo;s scientific programming languages, it becomes much more difficult to search for and eliminate mistakes.&lt;/p&gt;

&lt;p&gt;So which are the mistakes that were uncovered by comparing two independent implementations of the same method?&lt;/p&gt;

&lt;p&gt;Number one, by far, is discrepancies between the informal description for human readers and the executable implementation. Put simply, the programs did not compute what the informal description said they should compute, or the informal description was incomplete, admitting more than one interpretation.&lt;/p&gt;

&lt;p&gt;Number two is typos in numerical constants and in variable names. Since I can almost hear proponents of static typing saying &amp;ldquo;that&amp;rsquo;s what you deserve for using Python&amp;rdquo;, let me add that most typos in variable names would &lt;em&gt;not&lt;/em&gt; have been caught by static type checking. If you have two integer loop indices &lt;code&gt;i&lt;/code&gt; and &lt;code&gt;j&lt;/code&gt;, no type checker will complain when you interchange them by mistake.&lt;/p&gt;

&lt;p&gt;Number three is off-by-one-or-two errors in loops and in array indices. If you have a complex formula involving lots of x[i], x[i+1], and x[i&amp;ndash;1], it&amp;rsquo;s hard to avoid getting an index wrong occasionally. Unfortunately, array bounds checking does not catch all of these mistakes. Another interesting observation is that this type of mistake is just as likely in the informal description as in the code. Humans are apparently not very good at handling this kind of &amp;ldquo;detail&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Is there anything we can do to reduce the risk of these types of mistakes? I&amp;rsquo;d say yes, but it&amp;rsquo;s not going to be easy.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s start with what software engineering techniques could do to improve the situation. The main opportunity I see is for mistakes of the third kind. Index arithmetic could be eliminated altogether by abstracting it away. Most situations correspond to one of a handful of patterns, often called &lt;a href="https://en.wikipedia.org/wiki/Stencil_(numerical_analysis)"&gt;stencils&lt;/a&gt;, which could become functions or macros in a suitable domain-specific language. Another idea, applicable to legacy code, is to have code checking tools recognize stencils and small deviations from common stencils and point out potential mistakes - see &lt;a href="https://camfort.github.io/tvcs2017/#contrastin"&gt;this presentation&lt;/a&gt; at the recent &lt;a href="https://camfort.github.io/tvcs2017/"&gt;2nd Meeting on Testing and Verification for Computational Science&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Similar heuristic searches for potential mistakes could be applied to typos in variable names, though it is not sure that such reports would ultimately be useful. The real issue is the widespread use of short and similar variable names. A radical approach would be to ban them as part of a programming style guide, and have source code checkers flag violations of such a rule.&lt;/p&gt;

&lt;p&gt;For the main source of mistakes, discrepancies between informal specification and implementation, software engineering approaches are totally hopeless in my opinion. After all, the programs are perfectly reasonable and consistent, they merely solve a problem that is different from the one they were written to solve. Given the current state of technology, the comparison between the two problem decriptions can only be done by human proofreading, as long as at least one problem description is informal. I suspect the best approach we have today is exactly what I described above - develop two independent implementations and compare.&lt;/p&gt;

&lt;p&gt;In the long run, we can work on reducing the gap between informal descriptions (papers, software documentations) and executable implementations. I vaguely remember hearing about people exploring the possibility of turning informal descriptions into formal specifications by natural language processing - if anyone has a reference, please leave a comment! But I am rather skeptical of this approach, and therefore I prefer to let humans make the move towards formal specifications. The human-computer interface for such specifications is what I call &lt;a href="http://sjscience.org/article?id=527"&gt;digital scientific notations&lt;/a&gt;, and I am currently working on &lt;a href="https://github.com/khinsen/leibniz"&gt;developing such a notation&lt;/a&gt; for my corner of science, which is computational physics and chemistry.&lt;/p&gt;

&lt;p&gt;Finally, let me point out that my experiments and their conclusion apply only to research code in the strict sense, i.e. code that was written to compute a result that is &lt;em&gt;a priori&lt;/em&gt; unknown. Referring to my &lt;a href="http://blog.khinsen.net/posts/2017/01/13/sustainable-software-and-reproducible-research-dealing-with-software-collapse/"&gt;earlier post on software collapse&lt;/a&gt;, this is the fourth and project-specific layer of scientific software. When writing libraries and software tools that implement established methods for wider use, the situation is different because testing can be used much more effectively.&lt;/p&gt;</description></item></channel></rss>